{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb13b8b8",
   "metadata": {},
   "source": [
    "<h1> HOW TO: </h1>\n",
    "\n",
    "1. Edit the configs list in Cell 3 (CONFIG DEFINITIONS) with all experiments you want to try\n",
    "2. Run Cell 2 (RESULT STORAGE) to initialize results storage\n",
    "3. Run Cell 4 (TRAINING FUNCTION) to define the training function\n",
    "4. Run Cell 5 (MAIN EXECUTION LOOP) to start training \n",
    "5. When done, run Cells 6-8 to analyze and save results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e0415",
   "metadata": {},
   "source": [
    "<h2> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d7fbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from precomputed_dataset import precomputedDataset\n",
    "from modules import MPNNLayer, MPNNTokenizer, SelfAttentionEncoder, PredictionHead\n",
    "from model import MPNNTransformerModel\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d302f",
   "metadata": {},
   "source": [
    "<h2> Result Storage & Utility </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1937e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Results for all configurations ---\n",
    "all_results = []\n",
    "\n",
    "# --- Utility function to check gradient flow ---\n",
    "def check_gradients(model):\n",
    "    \"\"\"Check if gradients are flowing properly\"\"\"\n",
    "    total_norm = 0.0\n",
    "    param_count = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "            param_count += 1\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm, param_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9db971",
   "metadata": {},
   "source": [
    "<h2> Config definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe01ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define each configuration as a dictionary ---\n",
    "configs = [\n",
    "    {\n",
    "        'name': 'mpnn_layer_1', #Find MPNN layer count then\n",
    "        'num_epochs': 1000,\n",
    "        # --- Architecture ---\n",
    "        'mpnn_hidden_dim': 128,\n",
    "        'mpnn_num_layers': 1, # Set this \n",
    "        'mp_layer_norm': False,\n",
    "        'token_dim': 128,\n",
    "        'attn_num_heads': 4, \n",
    "        'attn_num_layers': 4, # Set this later\n",
    "        'head_mlp_hidden_dim': 256,\n",
    "        'mpnn_dropout': 0.1,\n",
    "        'attn_dropout': 0.1,\n",
    "        'head_dropout': 0.1,\n",
    "        'pooling_strategy': 'cls_token',\n",
    "        # --- Hyperparameters ---\n",
    "        'lr': 1e-4,\n",
    "        'weight_decay': 0.0,\n",
    "        'batch_size': 32,\n",
    "        # --- Scheduler params --- \n",
    "        'scheduler': False,\n",
    "        'mode': 'min',\n",
    "        'factor': 0.5,\n",
    "        'patience': 10,\n",
    "        'threshold': 1e-5, \n",
    "        'threshold_mode': \"rel\",\n",
    "        'cooldown': 5,\n",
    "        'min_lr': 1e-7,\n",
    "    },\n",
    "    {\n",
    "        'name': 'mpnn_layer_2', \n",
    "        'num_epochs': 1000,\n",
    "        # --- Architecture ---\n",
    "        'mpnn_hidden_dim': 128,\n",
    "        'mpnn_num_layers': 2, # Set this \n",
    "        'mp_layer_norm': False,\n",
    "        'token_dim': 128,\n",
    "        'attn_num_heads': 4, \n",
    "        'attn_num_layers': 4, # Set this later \n",
    "        'head_mlp_hidden_dim': 256,\n",
    "        'mpnn_dropout': 0.1,\n",
    "        'attn_dropout': 0.1,\n",
    "        'head_dropout': 0.1,\n",
    "        'pooling_strategy': 'cls_token',\n",
    "        # --- Hyperparameters ---\n",
    "        'lr': 1e-4,\n",
    "        'weight_decay': 0.0,\n",
    "        'batch_size': 32,\n",
    "        # --- Scheduler params --- \n",
    "        'scheduler': False,\n",
    "        'mode': 'min',\n",
    "        'factor': 0.5,\n",
    "        'patience': 10,\n",
    "        'threshold': 1e-5, \n",
    "        'threshold_mode': \"rel\",\n",
    "        'cooldown': 5,\n",
    "        'min_lr': 1e-7,\n",
    "    },\n",
    "    {\n",
    "        'name': 'mpnn_layer_3', #Try to set learning rates so it converges\n",
    "        'num_epochs': 1000,\n",
    "        # --- Architecture ---\n",
    "        'mpnn_hidden_dim': 128,\n",
    "        'mpnn_num_layers': 3, # Set this \n",
    "        'mp_layer_norm': False,\n",
    "        'token_dim': 128,\n",
    "        'attn_num_heads': 4, \n",
    "        'attn_num_layers': 4, # Set this later \n",
    "        'head_mlp_hidden_dim': 256,\n",
    "        'mpnn_dropout': 0.1,\n",
    "        'attn_dropout': 0.1,\n",
    "        'head_dropout': 0.1,\n",
    "        'pooling_strategy': 'cls_token',\n",
    "        # --- Hyperparameters ---\n",
    "        'lr': 1e-4,\n",
    "        'weight_decay': 0.0,\n",
    "        'batch_size': 32,\n",
    "        # --- Scheduler params --- \n",
    "        'scheduler': False,\n",
    "        'mode': 'min',\n",
    "        'factor': 0.5,\n",
    "        'patience': 10,\n",
    "        'threshold': 1e-5, \n",
    "        'threshold_mode': \"rel\",\n",
    "        'cooldown': 5,\n",
    "        'min_lr': 1e-7,\n",
    "    },\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4490d4b",
   "metadata": {},
   "source": [
    "<h2> Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_config(config, device, h5_path, test_path, seed=0):\n",
    "    \"\"\"\n",
    "    Train a model with given configuration and return results\n",
    "    \n",
    "    Returns:\n",
    "        dict with training metrics and test performance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING CONFIG: {config['name']}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # --- Dataset setup ---\n",
    "    ds = precomputedDataset(h5_path)\n",
    "    loader = PyGDataLoader(\n",
    "        ds, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,  \n",
    "        num_workers=4, \n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    test_ds = precomputedDataset(test_path)\n",
    "    \n",
    "    # --- Model setup ---\n",
    "    sample0 = ds[0]\n",
    "    node_in_dim = sample0.x.shape[-1]\n",
    "    edge_in_dim = sample0.edge_attr.shape[-1]\n",
    "    \n",
    "    model = MPNNTransformerModel(\n",
    "        node_in_dim=node_in_dim,\n",
    "        edge_in_dim=edge_in_dim,\n",
    "        num_output_sources=1,\n",
    "        mpnn_hidden_dim=config['mpnn_hidden_dim'],\n",
    "        mpnn_num_layers=config['mpnn_num_layers'],\n",
    "        mp_layer_norm=config['mp_layer_norm'],\n",
    "        token_dim=config['token_dim'],\n",
    "        attn_num_heads=config['attn_num_heads'],\n",
    "        attn_num_layers=config['attn_num_layers'],\n",
    "        head_mlp_hidden_dim=config['head_mlp_hidden_dim'],\n",
    "        mpnn_dropout=config['mpnn_dropout'],\n",
    "        attn_dropout=config['attn_dropout'],\n",
    "        head_dropout=config['head_dropout'],\n",
    "        pooling_strategy=config['pooling_strategy']\n",
    "    ).to(device)\n",
    "    \n",
    "    # --- Optimizer and Scheduler ---\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        betas=(0.9, 0.999),\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=config['mode'],\n",
    "        factor=config['factor'],          \n",
    "        patience=config['patience'],       \n",
    "        threshold=config['threshold'],    \n",
    "        threshold_mode=config['threshold_mode'],\n",
    "        cooldown=config['cooldown'],        \n",
    "        min_lr=config['min_lr'],\n",
    "    )\n",
    "    \n",
    "    # --- General Tracking ---\n",
    "    loss_history = []\n",
    "    pred_std_history = []\n",
    "    grad_norm_history = []\n",
    "\n",
    "    # --- Early stopping tracking and settings ---\n",
    "    best_test_loss = float('inf')\n",
    "    corresponding_best_train_loss = float('inf') #Train loss that corresponds to best test loss\n",
    "    patience_counter = 0\n",
    "    patience = 50 # Set here \n",
    "    min_delta = 1e-5 # Set here\n",
    "    test_loss_history = []\n",
    "    \n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Training loop ---\n",
    "    for epoch in range(1, config['num_epochs'] + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_preds = []\n",
    "        \n",
    "        for batch_idx, data in enumerate(loader):\n",
    "            data = data.to(device)\n",
    "            pred = model.forward_from_data(data)  \n",
    "            target = data.y  \n",
    "\n",
    "            assert pred.shape == target.shape\n",
    "            \n",
    "            loss = F.mse_loss(pred, target)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_preds.append(pred.detach().cpu().numpy())\n",
    "        \n",
    "        # --- Epoch statistics ---\n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        if config['scheduler'] == True:\n",
    "            scheduler.step(avg_loss)\n",
    "\n",
    "        grad_norm, _ = check_gradients(model)\n",
    "        grad_norm_history.append(grad_norm)\n",
    "        \n",
    "        all_preds = np.concatenate(epoch_preds, axis=0)\n",
    "        pred_std = all_preds.std(axis=0).mean()\n",
    "        pred_std_history.append(pred_std)\n",
    "        \n",
    "\n",
    "        \n",
    "        # --- Validate for early stopping ---\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in test_ds:\n",
    "                data = data.to(device)\n",
    "                pred = model.forward_from_data(data)\n",
    "                target = data.y.squeeze(0)\n",
    "                test_loss += F.mse_loss(pred, target).item()\n",
    "        \n",
    "        avg_test_loss = test_loss / len(test_ds)\n",
    "        test_loss_history.append(avg_test_loss)\n",
    "        \n",
    "        # --- Early stopping ---\n",
    "        if avg_test_loss < best_test_loss - min_delta:\n",
    "            best_test_loss = avg_test_loss\n",
    "            corresponding_best_train_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"  Early stopping at epoch {epoch} (test_loss={avg_test_loss:.6f})\")\n",
    "                break\n",
    "\n",
    "        # Logging\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:4d} | Train Loss: {avg_loss:.6f} | Test Loss: {avg_test_loss:.6f} | Best Test: {best_test_loss:.6f} | Patience: {patience_counter}/{patience} | LR: {optimizer.param_groups[0]['lr']:.2e} | Pred_std: {pred_std:.4f}\")\n",
    "                 #Grad_norm: {grad_norm:.4f} |\n",
    "\n",
    "        ###############\n",
    "        \n",
    "        # Check for collapse\n",
    "        if pred_std < 0.004:\n",
    "            print(f\"  WARNING: Predictions collapsed at epoch {epoch}!\")\n",
    "            break\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_loss < 5e-5:\n",
    "            print(f\"  Converged at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    final_train_loss = loss_history[-1]\n",
    "    \n",
    "    # --- Evaluation ---\n",
    "    print(f\"\\nEvaluating on test set...\")\n",
    "    #test_ds was already loaded for early stopping\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in test_ds:\n",
    "            data = data.to(device)\n",
    "            pred = model.forward_from_data(data)\n",
    "            target = data.y.squeeze(0)\n",
    "            test_loss += F.mse_loss(pred, target).item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_ds)\n",
    "    \n",
    "    print(f\"Final train loss: {final_train_loss:.6f}\")\n",
    "    print(f\"Test loss: {avg_test_loss:.6f}\")\n",
    "    print(f\"Generalization gap: {avg_test_loss - final_train_loss:.6f}\")\n",
    "    \n",
    "    # Return results\n",
    "    results = {\n",
    "        'config_name': config['name'],\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_test_loss': avg_test_loss,\n",
    "        'final_generalization_gap': avg_test_loss - final_train_loss,\n",
    "        'best_test_loss': best_test_loss,\n",
    "        'best_corresponding_train_loss': corresponding_best_train_loss,\n",
    "        'best_generalization_gap': best_test_loss - corresponding_best_train_loss,\n",
    "        'epochs_trained': len(loss_history),\n",
    "        'loss_history': loss_history,\n",
    "        'pred_std_history': pred_std_history,\n",
    "        'config': config.copy()\n",
    "    }\n",
    "    \n",
    "    # Clean up\n",
    "    del model, optimizer, loader, ds, test_ds\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c064f",
   "metadata": {},
   "source": [
    "<h2> Main Execution Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d450820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n",
      "Total configs to run: 3\n",
      "\n",
      "\n",
      "############################################################\n",
      "# Running config 1/3: mpnn_layer_1\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "TRAINING CONFIG: mpnn_layer_1\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/zaid/projects/geometry-aware-ssl/training/../src/modules.py:130: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  self.transformer_encoder = nn.TransformerEncoder(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Epoch    1 | Train Loss: 0.024061 | Test Loss: 0.012239 | Best Test: 0.012239 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.0680\n",
      "Epoch    2 | Train Loss: 0.013770 | Test Loss: 0.012046 | Best Test: 0.012046 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1204\n",
      "Epoch    3 | Train Loss: 0.012457 | Test Loss: 0.011563 | Best Test: 0.011563 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1250\n",
      "Epoch    4 | Train Loss: 0.011884 | Test Loss: 0.013499 | Best Test: 0.011563 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1265\n",
      "Epoch    5 | Train Loss: 0.011809 | Test Loss: 0.011071 | Best Test: 0.011071 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1272\n",
      "Epoch    6 | Train Loss: 0.011230 | Test Loss: 0.011443 | Best Test: 0.011071 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1288\n",
      "Epoch    7 | Train Loss: 0.011163 | Test Loss: 0.010945 | Best Test: 0.010945 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1296\n",
      "Epoch    8 | Train Loss: 0.010920 | Test Loss: 0.010733 | Best Test: 0.010733 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1297\n",
      "Epoch    9 | Train Loss: 0.010770 | Test Loss: 0.011501 | Best Test: 0.010733 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1306\n",
      "Epoch   10 | Train Loss: 0.010547 | Test Loss: 0.011670 | Best Test: 0.010733 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1314\n",
      "Epoch   11 | Train Loss: 0.010623 | Test Loss: 0.009912 | Best Test: 0.009912 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1309\n",
      "Epoch   12 | Train Loss: 0.010383 | Test Loss: 0.009514 | Best Test: 0.009514 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1321\n",
      "Epoch   13 | Train Loss: 0.010218 | Test Loss: 0.009505 | Best Test: 0.009514 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1326\n",
      "Epoch   14 | Train Loss: 0.010075 | Test Loss: 0.010885 | Best Test: 0.009514 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1330\n",
      "Epoch   15 | Train Loss: 0.010006 | Test Loss: 0.009564 | Best Test: 0.009514 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1335\n",
      "Epoch   16 | Train Loss: 0.009664 | Test Loss: 0.009881 | Best Test: 0.009514 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1341\n",
      "Epoch   17 | Train Loss: 0.009595 | Test Loss: 0.009406 | Best Test: 0.009406 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1348\n",
      "Epoch   18 | Train Loss: 0.009446 | Test Loss: 0.009065 | Best Test: 0.009065 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1355\n",
      "Epoch   19 | Train Loss: 0.009351 | Test Loss: 0.009163 | Best Test: 0.009065 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1359\n",
      "Epoch   20 | Train Loss: 0.009283 | Test Loss: 0.009259 | Best Test: 0.009065 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1356\n",
      "Epoch   21 | Train Loss: 0.009134 | Test Loss: 0.008477 | Best Test: 0.008477 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1362\n",
      "Epoch   22 | Train Loss: 0.009085 | Test Loss: 0.008446 | Best Test: 0.008446 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1370\n",
      "Epoch   23 | Train Loss: 0.008992 | Test Loss: 0.008904 | Best Test: 0.008446 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1367\n",
      "Epoch   24 | Train Loss: 0.008890 | Test Loss: 0.008359 | Best Test: 0.008359 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1374\n",
      "Epoch   25 | Train Loss: 0.009003 | Test Loss: 0.008753 | Best Test: 0.008359 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1366\n",
      "Epoch   26 | Train Loss: 0.008764 | Test Loss: 0.008377 | Best Test: 0.008359 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1375\n",
      "Epoch   27 | Train Loss: 0.008676 | Test Loss: 0.008245 | Best Test: 0.008245 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1381\n",
      "Epoch   28 | Train Loss: 0.008818 | Test Loss: 0.008164 | Best Test: 0.008164 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1373\n",
      "Epoch   29 | Train Loss: 0.008637 | Test Loss: 0.008568 | Best Test: 0.008164 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1378\n",
      "Epoch   30 | Train Loss: 0.008595 | Test Loss: 0.008507 | Best Test: 0.008164 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1385\n",
      "Epoch   31 | Train Loss: 0.008501 | Test Loss: 0.008340 | Best Test: 0.008164 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1383\n",
      "Epoch   32 | Train Loss: 0.008651 | Test Loss: 0.008111 | Best Test: 0.008111 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1379\n",
      "Epoch   33 | Train Loss: 0.008451 | Test Loss: 0.008817 | Best Test: 0.008111 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1386\n",
      "Epoch   34 | Train Loss: 0.008516 | Test Loss: 0.008193 | Best Test: 0.008111 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1387\n",
      "Epoch   35 | Train Loss: 0.008339 | Test Loss: 0.007973 | Best Test: 0.007973 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1388\n",
      "Epoch   36 | Train Loss: 0.008427 | Test Loss: 0.007887 | Best Test: 0.007887 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1387\n",
      "Epoch   37 | Train Loss: 0.008325 | Test Loss: 0.007919 | Best Test: 0.007887 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1393\n",
      "Epoch   38 | Train Loss: 0.008263 | Test Loss: 0.008628 | Best Test: 0.007887 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1390\n",
      "Epoch   39 | Train Loss: 0.008249 | Test Loss: 0.008630 | Best Test: 0.007887 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1394\n",
      "Epoch   40 | Train Loss: 0.008154 | Test Loss: 0.008469 | Best Test: 0.007887 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1393\n",
      "Epoch   41 | Train Loss: 0.008188 | Test Loss: 0.007788 | Best Test: 0.007788 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1396\n",
      "Epoch   42 | Train Loss: 0.008169 | Test Loss: 0.008418 | Best Test: 0.007788 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1396\n",
      "Epoch   43 | Train Loss: 0.008152 | Test Loss: 0.008137 | Best Test: 0.007788 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1396\n",
      "Epoch   44 | Train Loss: 0.008107 | Test Loss: 0.008141 | Best Test: 0.007788 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1395\n",
      "Epoch   45 | Train Loss: 0.008119 | Test Loss: 0.008053 | Best Test: 0.007788 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1397\n",
      "Epoch   46 | Train Loss: 0.008103 | Test Loss: 0.008997 | Best Test: 0.007788 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1398\n",
      "Epoch   47 | Train Loss: 0.007876 | Test Loss: 0.007647 | Best Test: 0.007647 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1404\n",
      "Epoch   48 | Train Loss: 0.007929 | Test Loss: 0.007794 | Best Test: 0.007647 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1401\n",
      "Epoch   49 | Train Loss: 0.007850 | Test Loss: 0.007658 | Best Test: 0.007647 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1407\n",
      "Epoch   50 | Train Loss: 0.007905 | Test Loss: 0.008068 | Best Test: 0.007647 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1404\n",
      "Epoch   51 | Train Loss: 0.007880 | Test Loss: 0.007506 | Best Test: 0.007506 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1405\n",
      "Epoch   52 | Train Loss: 0.007661 | Test Loss: 0.007200 | Best Test: 0.007200 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1410\n",
      "Epoch   53 | Train Loss: 0.007691 | Test Loss: 0.007673 | Best Test: 0.007200 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1410\n",
      "Epoch   54 | Train Loss: 0.007939 | Test Loss: 0.008503 | Best Test: 0.007200 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1402\n",
      "Epoch   55 | Train Loss: 0.007597 | Test Loss: 0.007769 | Best Test: 0.007200 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1414\n",
      "Epoch   56 | Train Loss: 0.007706 | Test Loss: 0.008034 | Best Test: 0.007200 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1408\n",
      "Epoch   57 | Train Loss: 0.007589 | Test Loss: 0.007556 | Best Test: 0.007200 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1414\n",
      "Epoch   58 | Train Loss: 0.007528 | Test Loss: 0.007463 | Best Test: 0.007200 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1418\n",
      "Epoch   59 | Train Loss: 0.007707 | Test Loss: 0.008654 | Best Test: 0.007200 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1408\n",
      "Epoch   60 | Train Loss: 0.007659 | Test Loss: 0.007216 | Best Test: 0.007200 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1410\n",
      "Epoch   61 | Train Loss: 0.007506 | Test Loss: 0.007459 | Best Test: 0.007200 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1415\n",
      "Epoch   62 | Train Loss: 0.007471 | Test Loss: 0.007236 | Best Test: 0.007200 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1415\n",
      "Epoch   63 | Train Loss: 0.007742 | Test Loss: 0.007116 | Best Test: 0.007116 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1412\n",
      "Epoch   64 | Train Loss: 0.007315 | Test Loss: 0.008242 | Best Test: 0.007116 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1422\n",
      "Epoch   65 | Train Loss: 0.007348 | Test Loss: 0.007212 | Best Test: 0.007116 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1422\n",
      "Epoch   66 | Train Loss: 0.007303 | Test Loss: 0.007226 | Best Test: 0.007116 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1424\n",
      "Epoch   67 | Train Loss: 0.007281 | Test Loss: 0.006979 | Best Test: 0.006979 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1423\n",
      "Epoch   68 | Train Loss: 0.007271 | Test Loss: 0.007624 | Best Test: 0.006979 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1424\n",
      "Epoch   69 | Train Loss: 0.007323 | Test Loss: 0.006906 | Best Test: 0.006906 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1422\n",
      "Epoch   70 | Train Loss: 0.007267 | Test Loss: 0.006978 | Best Test: 0.006906 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1425\n",
      "Epoch   71 | Train Loss: 0.007185 | Test Loss: 0.007819 | Best Test: 0.006906 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1427\n",
      "Epoch   72 | Train Loss: 0.007253 | Test Loss: 0.007470 | Best Test: 0.006906 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1424\n",
      "Epoch   73 | Train Loss: 0.007054 | Test Loss: 0.008044 | Best Test: 0.006906 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1433\n",
      "Epoch   74 | Train Loss: 0.007223 | Test Loss: 0.007002 | Best Test: 0.006906 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1423\n",
      "Epoch   75 | Train Loss: 0.007081 | Test Loss: 0.007295 | Best Test: 0.006906 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1431\n",
      "Epoch   76 | Train Loss: 0.007060 | Test Loss: 0.007599 | Best Test: 0.006906 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1433\n",
      "Epoch   77 | Train Loss: 0.006964 | Test Loss: 0.006874 | Best Test: 0.006874 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1434\n",
      "Epoch   78 | Train Loss: 0.006954 | Test Loss: 0.006536 | Best Test: 0.006536 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1433\n",
      "Epoch   79 | Train Loss: 0.006995 | Test Loss: 0.007975 | Best Test: 0.006536 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1432\n",
      "Epoch   80 | Train Loss: 0.007006 | Test Loss: 0.007442 | Best Test: 0.006536 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1433\n",
      "Epoch   81 | Train Loss: 0.006814 | Test Loss: 0.007691 | Best Test: 0.006536 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1439\n",
      "Epoch   82 | Train Loss: 0.006877 | Test Loss: 0.006521 | Best Test: 0.006521 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1438\n",
      "Epoch   83 | Train Loss: 0.006934 | Test Loss: 0.006527 | Best Test: 0.006521 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1435\n",
      "Epoch   84 | Train Loss: 0.006712 | Test Loss: 0.006983 | Best Test: 0.006521 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1443\n",
      "Epoch   85 | Train Loss: 0.006712 | Test Loss: 0.006927 | Best Test: 0.006521 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1442\n",
      "Epoch   86 | Train Loss: 0.006817 | Test Loss: 0.007068 | Best Test: 0.006521 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1442\n",
      "Epoch   87 | Train Loss: 0.006632 | Test Loss: 0.006732 | Best Test: 0.006521 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1444\n",
      "Epoch   88 | Train Loss: 0.006680 | Test Loss: 0.006837 | Best Test: 0.006521 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1443\n",
      "Epoch   89 | Train Loss: 0.006877 | Test Loss: 0.006717 | Best Test: 0.006521 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1439\n",
      "Epoch   90 | Train Loss: 0.006505 | Test Loss: 0.006843 | Best Test: 0.006521 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1448\n",
      "Epoch   91 | Train Loss: 0.006567 | Test Loss: 0.006581 | Best Test: 0.006521 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1449\n",
      "Epoch   92 | Train Loss: 0.006612 | Test Loss: 0.006532 | Best Test: 0.006521 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1445\n",
      "Epoch   93 | Train Loss: 0.006760 | Test Loss: 0.006580 | Best Test: 0.006521 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1441\n",
      "Epoch   94 | Train Loss: 0.006375 | Test Loss: 0.006211 | Best Test: 0.006211 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1454\n",
      "Epoch   95 | Train Loss: 0.006438 | Test Loss: 0.006922 | Best Test: 0.006211 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1453\n",
      "Epoch   96 | Train Loss: 0.006542 | Test Loss: 0.007094 | Best Test: 0.006211 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1448\n",
      "Epoch   97 | Train Loss: 0.006387 | Test Loss: 0.006247 | Best Test: 0.006211 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1452\n",
      "Epoch   98 | Train Loss: 0.006425 | Test Loss: 0.006970 | Best Test: 0.006211 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1457\n",
      "Epoch   99 | Train Loss: 0.006616 | Test Loss: 0.006929 | Best Test: 0.006211 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1444\n",
      "Epoch  100 | Train Loss: 0.006469 | Test Loss: 0.006255 | Best Test: 0.006211 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1452\n",
      "Epoch  101 | Train Loss: 0.006416 | Test Loss: 0.006347 | Best Test: 0.006211 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1453\n",
      "Epoch  102 | Train Loss: 0.006299 | Test Loss: 0.006717 | Best Test: 0.006211 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1455\n",
      "Epoch  103 | Train Loss: 0.006248 | Test Loss: 0.006362 | Best Test: 0.006211 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1461\n",
      "Epoch  104 | Train Loss: 0.006048 | Test Loss: 0.006392 | Best Test: 0.006211 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1462\n",
      "Epoch  105 | Train Loss: 0.006280 | Test Loss: 0.006617 | Best Test: 0.006211 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1461\n",
      "Epoch  106 | Train Loss: 0.006096 | Test Loss: 0.006860 | Best Test: 0.006211 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1462\n",
      "Epoch  107 | Train Loss: 0.006158 | Test Loss: 0.007318 | Best Test: 0.006211 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1463\n",
      "Epoch  108 | Train Loss: 0.006209 | Test Loss: 0.006881 | Best Test: 0.006211 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1460\n",
      "Epoch  109 | Train Loss: 0.006145 | Test Loss: 0.006319 | Best Test: 0.006211 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1464\n",
      "Epoch  110 | Train Loss: 0.005815 | Test Loss: 0.006224 | Best Test: 0.006211 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1468\n",
      "Epoch  111 | Train Loss: 0.005982 | Test Loss: 0.006354 | Best Test: 0.006211 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1473\n",
      "Epoch  112 | Train Loss: 0.005918 | Test Loss: 0.006411 | Best Test: 0.006211 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1465\n",
      "Epoch  113 | Train Loss: 0.005904 | Test Loss: 0.006202 | Best Test: 0.006211 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1469\n",
      "Epoch  114 | Train Loss: 0.005980 | Test Loss: 0.006261 | Best Test: 0.006211 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1468\n",
      "Epoch  115 | Train Loss: 0.005782 | Test Loss: 0.006062 | Best Test: 0.006062 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1473\n",
      "Epoch  116 | Train Loss: 0.006089 | Test Loss: 0.006180 | Best Test: 0.006062 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1466\n",
      "Epoch  117 | Train Loss: 0.005841 | Test Loss: 0.006395 | Best Test: 0.006062 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1470\n",
      "Epoch  118 | Train Loss: 0.005688 | Test Loss: 0.006200 | Best Test: 0.006062 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1480\n",
      "Epoch  119 | Train Loss: 0.005700 | Test Loss: 0.006484 | Best Test: 0.006062 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1478\n",
      "Epoch  120 | Train Loss: 0.005645 | Test Loss: 0.006174 | Best Test: 0.006062 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1480\n",
      "Epoch  121 | Train Loss: 0.005567 | Test Loss: 0.007116 | Best Test: 0.006062 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1479\n",
      "Epoch  122 | Train Loss: 0.005761 | Test Loss: 0.006061 | Best Test: 0.006062 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1474\n",
      "Epoch  123 | Train Loss: 0.005774 | Test Loss: 0.006541 | Best Test: 0.006062 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1474\n",
      "Epoch  124 | Train Loss: 0.005689 | Test Loss: 0.006624 | Best Test: 0.006062 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1477\n",
      "Epoch  125 | Train Loss: 0.005709 | Test Loss: 0.006960 | Best Test: 0.006062 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1479\n",
      "Epoch  126 | Train Loss: 0.005512 | Test Loss: 0.006491 | Best Test: 0.006062 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1485\n",
      "Epoch  127 | Train Loss: 0.005521 | Test Loss: 0.005849 | Best Test: 0.005849 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1481\n",
      "Epoch  128 | Train Loss: 0.005531 | Test Loss: 0.006442 | Best Test: 0.005849 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1483\n",
      "Epoch  129 | Train Loss: 0.005763 | Test Loss: 0.006204 | Best Test: 0.005849 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1477\n",
      "Epoch  130 | Train Loss: 0.005493 | Test Loss: 0.006359 | Best Test: 0.005849 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1482\n",
      "Epoch  131 | Train Loss: 0.005345 | Test Loss: 0.006207 | Best Test: 0.005849 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1490\n",
      "Epoch  132 | Train Loss: 0.005334 | Test Loss: 0.006565 | Best Test: 0.005849 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1488\n",
      "Epoch  133 | Train Loss: 0.005458 | Test Loss: 0.005977 | Best Test: 0.005849 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1483\n",
      "Epoch  134 | Train Loss: 0.005384 | Test Loss: 0.006788 | Best Test: 0.005849 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1490\n",
      "Epoch  135 | Train Loss: 0.005452 | Test Loss: 0.006174 | Best Test: 0.005849 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1488\n",
      "Epoch  136 | Train Loss: 0.005294 | Test Loss: 0.005981 | Best Test: 0.005849 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1489\n",
      "Epoch  137 | Train Loss: 0.005232 | Test Loss: 0.005799 | Best Test: 0.005799 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1495\n",
      "Epoch  138 | Train Loss: 0.005374 | Test Loss: 0.006007 | Best Test: 0.005799 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1487\n",
      "Epoch  139 | Train Loss: 0.004973 | Test Loss: 0.006752 | Best Test: 0.005799 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1500\n",
      "Epoch  140 | Train Loss: 0.005137 | Test Loss: 0.006709 | Best Test: 0.005799 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1496\n",
      "Epoch  141 | Train Loss: 0.005107 | Test Loss: 0.006049 | Best Test: 0.005799 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1499\n",
      "Epoch  142 | Train Loss: 0.005090 | Test Loss: 0.005791 | Best Test: 0.005799 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1497\n",
      "Epoch  143 | Train Loss: 0.005097 | Test Loss: 0.006012 | Best Test: 0.005799 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1496\n",
      "Epoch  144 | Train Loss: 0.005024 | Test Loss: 0.006117 | Best Test: 0.005799 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1502\n",
      "Epoch  145 | Train Loss: 0.005325 | Test Loss: 0.005526 | Best Test: 0.005526 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1488\n",
      "Epoch  146 | Train Loss: 0.005103 | Test Loss: 0.005891 | Best Test: 0.005526 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1497\n",
      "Epoch  147 | Train Loss: 0.005038 | Test Loss: 0.006752 | Best Test: 0.005526 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1499\n",
      "Epoch  148 | Train Loss: 0.004960 | Test Loss: 0.006071 | Best Test: 0.005526 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1503\n",
      "Epoch  149 | Train Loss: 0.004902 | Test Loss: 0.005693 | Best Test: 0.005526 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1503\n",
      "Epoch  150 | Train Loss: 0.005114 | Test Loss: 0.006435 | Best Test: 0.005526 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1498\n",
      "Epoch  151 | Train Loss: 0.005059 | Test Loss: 0.005905 | Best Test: 0.005526 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1498\n",
      "Epoch  152 | Train Loss: 0.004998 | Test Loss: 0.006879 | Best Test: 0.005526 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1502\n",
      "Epoch  153 | Train Loss: 0.004926 | Test Loss: 0.005987 | Best Test: 0.005526 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1503\n",
      "Epoch  154 | Train Loss: 0.004939 | Test Loss: 0.005991 | Best Test: 0.005526 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1502\n",
      "Epoch  155 | Train Loss: 0.004778 | Test Loss: 0.006359 | Best Test: 0.005526 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1505\n",
      "Epoch  156 | Train Loss: 0.004789 | Test Loss: 0.005874 | Best Test: 0.005526 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1509\n",
      "Epoch  157 | Train Loss: 0.004672 | Test Loss: 0.005956 | Best Test: 0.005526 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1511\n",
      "Epoch  158 | Train Loss: 0.004881 | Test Loss: 0.006368 | Best Test: 0.005526 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1504\n",
      "Epoch  159 | Train Loss: 0.004768 | Test Loss: 0.005811 | Best Test: 0.005526 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1507\n",
      "Epoch  160 | Train Loss: 0.004658 | Test Loss: 0.005771 | Best Test: 0.005526 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1513\n",
      "Epoch  161 | Train Loss: 0.004700 | Test Loss: 0.005715 | Best Test: 0.005526 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1511\n",
      "Epoch  162 | Train Loss: 0.004800 | Test Loss: 0.005357 | Best Test: 0.005357 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1505\n",
      "Epoch  163 | Train Loss: 0.004716 | Test Loss: 0.006588 | Best Test: 0.005357 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1507\n",
      "Epoch  164 | Train Loss: 0.004596 | Test Loss: 0.005519 | Best Test: 0.005357 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1517\n",
      "Epoch  165 | Train Loss: 0.004639 | Test Loss: 0.006265 | Best Test: 0.005357 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1512\n",
      "Epoch  166 | Train Loss: 0.004816 | Test Loss: 0.006039 | Best Test: 0.005357 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1507\n",
      "Epoch  167 | Train Loss: 0.004563 | Test Loss: 0.006401 | Best Test: 0.005357 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1515\n",
      "Epoch  168 | Train Loss: 0.004349 | Test Loss: 0.006327 | Best Test: 0.005357 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1521\n",
      "Epoch  169 | Train Loss: 0.004544 | Test Loss: 0.005603 | Best Test: 0.005357 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1515\n",
      "Epoch  170 | Train Loss: 0.004492 | Test Loss: 0.005950 | Best Test: 0.005357 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1518\n",
      "Epoch  171 | Train Loss: 0.004584 | Test Loss: 0.005778 | Best Test: 0.005357 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1515\n",
      "Epoch  172 | Train Loss: 0.004450 | Test Loss: 0.006401 | Best Test: 0.005357 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1515\n",
      "Epoch  173 | Train Loss: 0.004478 | Test Loss: 0.005609 | Best Test: 0.005357 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1517\n",
      "Epoch  174 | Train Loss: 0.004270 | Test Loss: 0.005357 | Best Test: 0.005357 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1526\n",
      "Epoch  175 | Train Loss: 0.004471 | Test Loss: 0.005600 | Best Test: 0.005357 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1520\n",
      "Epoch  176 | Train Loss: 0.004373 | Test Loss: 0.005545 | Best Test: 0.005357 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1521\n",
      "Epoch  177 | Train Loss: 0.004429 | Test Loss: 0.005714 | Best Test: 0.005357 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1518\n",
      "Epoch  178 | Train Loss: 0.003975 | Test Loss: 0.005691 | Best Test: 0.005357 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1534\n",
      "Epoch  179 | Train Loss: 0.004374 | Test Loss: 0.005999 | Best Test: 0.005357 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1521\n",
      "Epoch  180 | Train Loss: 0.004464 | Test Loss: 0.005650 | Best Test: 0.005357 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1518\n",
      "Epoch  181 | Train Loss: 0.004288 | Test Loss: 0.005779 | Best Test: 0.005357 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1524\n",
      "Epoch  182 | Train Loss: 0.004115 | Test Loss: 0.006167 | Best Test: 0.005357 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1528\n",
      "Epoch  183 | Train Loss: 0.004277 | Test Loss: 0.005751 | Best Test: 0.005357 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1525\n",
      "Epoch  184 | Train Loss: 0.004173 | Test Loss: 0.005688 | Best Test: 0.005357 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1527\n",
      "Epoch  185 | Train Loss: 0.004287 | Test Loss: 0.005918 | Best Test: 0.005357 | Patience: 23/50 | LR: 1.00e-04 | Pred_std: 0.1526\n",
      "Epoch  186 | Train Loss: 0.004223 | Test Loss: 0.005970 | Best Test: 0.005357 | Patience: 24/50 | LR: 1.00e-04 | Pred_std: 0.1523\n",
      "Epoch  187 | Train Loss: 0.004324 | Test Loss: 0.006226 | Best Test: 0.005357 | Patience: 25/50 | LR: 1.00e-04 | Pred_std: 0.1526\n",
      "Epoch  188 | Train Loss: 0.004051 | Test Loss: 0.004976 | Best Test: 0.004976 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1531\n",
      "Epoch  189 | Train Loss: 0.003928 | Test Loss: 0.005814 | Best Test: 0.004976 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1535\n",
      "Epoch  190 | Train Loss: 0.003994 | Test Loss: 0.005514 | Best Test: 0.004976 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1534\n",
      "Epoch  191 | Train Loss: 0.003873 | Test Loss: 0.005681 | Best Test: 0.004976 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1539\n",
      "Epoch  192 | Train Loss: 0.004233 | Test Loss: 0.005993 | Best Test: 0.004976 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1527\n",
      "Epoch  193 | Train Loss: 0.004188 | Test Loss: 0.005860 | Best Test: 0.004976 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1526\n",
      "Epoch  194 | Train Loss: 0.004178 | Test Loss: 0.005339 | Best Test: 0.004976 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1528\n",
      "Epoch  195 | Train Loss: 0.003865 | Test Loss: 0.005361 | Best Test: 0.004976 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1537\n",
      "Epoch  196 | Train Loss: 0.004036 | Test Loss: 0.005202 | Best Test: 0.004976 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1533\n",
      "Epoch  197 | Train Loss: 0.004075 | Test Loss: 0.006034 | Best Test: 0.004976 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1529\n",
      "Epoch  198 | Train Loss: 0.003965 | Test Loss: 0.005321 | Best Test: 0.004976 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1536\n",
      "Epoch  199 | Train Loss: 0.003857 | Test Loss: 0.005231 | Best Test: 0.004976 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1537\n",
      "Epoch  200 | Train Loss: 0.003852 | Test Loss: 0.005076 | Best Test: 0.004976 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1539\n",
      "Epoch  201 | Train Loss: 0.003944 | Test Loss: 0.005665 | Best Test: 0.004976 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1536\n",
      "Epoch  202 | Train Loss: 0.003918 | Test Loss: 0.005776 | Best Test: 0.004976 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1533\n",
      "Epoch  203 | Train Loss: 0.003918 | Test Loss: 0.005108 | Best Test: 0.004976 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1537\n",
      "Epoch  204 | Train Loss: 0.003667 | Test Loss: 0.005684 | Best Test: 0.004976 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1545\n",
      "Epoch  205 | Train Loss: 0.003739 | Test Loss: 0.005237 | Best Test: 0.004976 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1540\n",
      "Epoch  206 | Train Loss: 0.003842 | Test Loss: 0.005533 | Best Test: 0.004976 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1538\n",
      "Epoch  207 | Train Loss: 0.003591 | Test Loss: 0.005315 | Best Test: 0.004976 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1547\n",
      "Epoch  208 | Train Loss: 0.003696 | Test Loss: 0.005558 | Best Test: 0.004976 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1542\n",
      "Epoch  209 | Train Loss: 0.004125 | Test Loss: 0.005629 | Best Test: 0.004976 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1529\n",
      "Epoch  210 | Train Loss: 0.003650 | Test Loss: 0.005347 | Best Test: 0.004976 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1546\n",
      "Epoch  211 | Train Loss: 0.003649 | Test Loss: 0.005787 | Best Test: 0.004976 | Patience: 23/50 | LR: 1.00e-04 | Pred_std: 0.1543\n",
      "Epoch  212 | Train Loss: 0.003696 | Test Loss: 0.006206 | Best Test: 0.004976 | Patience: 24/50 | LR: 1.00e-04 | Pred_std: 0.1545\n",
      "Epoch  213 | Train Loss: 0.003478 | Test Loss: 0.005087 | Best Test: 0.004976 | Patience: 25/50 | LR: 1.00e-04 | Pred_std: 0.1549\n",
      "Epoch  214 | Train Loss: 0.003398 | Test Loss: 0.004957 | Best Test: 0.004957 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1551\n",
      "Epoch  215 | Train Loss: 0.003769 | Test Loss: 0.004908 | Best Test: 0.004908 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1541\n",
      "Epoch  216 | Train Loss: 0.003833 | Test Loss: 0.005240 | Best Test: 0.004908 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1539\n",
      "Epoch  217 | Train Loss: 0.003698 | Test Loss: 0.005214 | Best Test: 0.004908 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1544\n",
      "Epoch  218 | Train Loss: 0.003399 | Test Loss: 0.005282 | Best Test: 0.004908 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1552\n",
      "Epoch  219 | Train Loss: 0.003486 | Test Loss: 0.005580 | Best Test: 0.004908 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1551\n",
      "Epoch  220 | Train Loss: 0.003423 | Test Loss: 0.005183 | Best Test: 0.004908 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1549\n",
      "Epoch  221 | Train Loss: 0.003503 | Test Loss: 0.005125 | Best Test: 0.004908 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1550\n",
      "Epoch  222 | Train Loss: 0.003482 | Test Loss: 0.005747 | Best Test: 0.004908 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1549\n",
      "Epoch  223 | Train Loss: 0.003596 | Test Loss: 0.005184 | Best Test: 0.004908 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1546\n",
      "Epoch  224 | Train Loss: 0.003558 | Test Loss: 0.005443 | Best Test: 0.004908 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1550\n",
      "Epoch  225 | Train Loss: 0.003451 | Test Loss: 0.005952 | Best Test: 0.004908 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1552\n",
      "Epoch  226 | Train Loss: 0.003393 | Test Loss: 0.005772 | Best Test: 0.004908 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1551\n",
      "Epoch  227 | Train Loss: 0.003463 | Test Loss: 0.006378 | Best Test: 0.004908 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1551\n",
      "Epoch  228 | Train Loss: 0.003677 | Test Loss: 0.005254 | Best Test: 0.004908 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1542\n",
      "Epoch  229 | Train Loss: 0.003396 | Test Loss: 0.005206 | Best Test: 0.004908 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1554\n",
      "Epoch  230 | Train Loss: 0.003300 | Test Loss: 0.006047 | Best Test: 0.004908 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1555\n",
      "Epoch  231 | Train Loss: 0.003397 | Test Loss: 0.005385 | Best Test: 0.004908 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  232 | Train Loss: 0.003713 | Test Loss: 0.005133 | Best Test: 0.004908 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1544\n",
      "Epoch  233 | Train Loss: 0.003412 | Test Loss: 0.005514 | Best Test: 0.004908 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  234 | Train Loss: 0.003216 | Test Loss: 0.004924 | Best Test: 0.004908 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1557\n",
      "Epoch  235 | Train Loss: 0.003280 | Test Loss: 0.005075 | Best Test: 0.004908 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1555\n",
      "Epoch  236 | Train Loss: 0.003188 | Test Loss: 0.005392 | Best Test: 0.004908 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1559\n",
      "Epoch  237 | Train Loss: 0.003126 | Test Loss: 0.005215 | Best Test: 0.004908 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1562\n",
      "Epoch  238 | Train Loss: 0.003362 | Test Loss: 0.004724 | Best Test: 0.004724 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1556\n",
      "Epoch  239 | Train Loss: 0.003273 | Test Loss: 0.004722 | Best Test: 0.004724 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1555\n",
      "Epoch  240 | Train Loss: 0.003246 | Test Loss: 0.004999 | Best Test: 0.004724 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1557\n",
      "Epoch  241 | Train Loss: 0.003028 | Test Loss: 0.005346 | Best Test: 0.004724 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1565\n",
      "Epoch  242 | Train Loss: 0.003420 | Test Loss: 0.006291 | Best Test: 0.004724 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1551\n",
      "Epoch  243 | Train Loss: 0.003406 | Test Loss: 0.005351 | Best Test: 0.004724 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1554\n",
      "Epoch  244 | Train Loss: 0.003069 | Test Loss: 0.005103 | Best Test: 0.004724 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1561\n",
      "Epoch  245 | Train Loss: 0.002902 | Test Loss: 0.006557 | Best Test: 0.004724 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1569\n",
      "Epoch  246 | Train Loss: 0.002913 | Test Loss: 0.004694 | Best Test: 0.004694 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1567\n",
      "Epoch  247 | Train Loss: 0.003051 | Test Loss: 0.005603 | Best Test: 0.004694 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1566\n",
      "Epoch  248 | Train Loss: 0.003093 | Test Loss: 0.004741 | Best Test: 0.004694 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1563\n",
      "Epoch  249 | Train Loss: 0.003055 | Test Loss: 0.005036 | Best Test: 0.004694 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1563\n",
      "Epoch  250 | Train Loss: 0.003211 | Test Loss: 0.004990 | Best Test: 0.004694 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1559\n",
      "Epoch  251 | Train Loss: 0.003130 | Test Loss: 0.004905 | Best Test: 0.004694 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1560\n",
      "Epoch  252 | Train Loss: 0.003176 | Test Loss: 0.004874 | Best Test: 0.004694 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1561\n",
      "Epoch  253 | Train Loss: 0.003111 | Test Loss: 0.004914 | Best Test: 0.004694 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1560\n",
      "Epoch  254 | Train Loss: 0.003340 | Test Loss: 0.005222 | Best Test: 0.004694 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1556\n",
      "Epoch  255 | Train Loss: 0.003078 | Test Loss: 0.004770 | Best Test: 0.004694 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1564\n",
      "Epoch  256 | Train Loss: 0.003471 | Test Loss: 0.005798 | Best Test: 0.004694 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1549\n",
      "Epoch  257 | Train Loss: 0.003101 | Test Loss: 0.005395 | Best Test: 0.004694 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1562\n",
      "Epoch  258 | Train Loss: 0.002827 | Test Loss: 0.004820 | Best Test: 0.004694 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1570\n",
      "Epoch  259 | Train Loss: 0.002989 | Test Loss: 0.005443 | Best Test: 0.004694 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1564\n",
      "Epoch  260 | Train Loss: 0.002928 | Test Loss: 0.005054 | Best Test: 0.004694 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1570\n",
      "Epoch  261 | Train Loss: 0.003121 | Test Loss: 0.005416 | Best Test: 0.004694 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1561\n",
      "Epoch  262 | Train Loss: 0.002816 | Test Loss: 0.004863 | Best Test: 0.004694 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1571\n",
      "Epoch  263 | Train Loss: 0.002842 | Test Loss: 0.004811 | Best Test: 0.004694 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1569\n",
      "Epoch  264 | Train Loss: 0.002891 | Test Loss: 0.004972 | Best Test: 0.004694 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1569\n",
      "Epoch  265 | Train Loss: 0.003064 | Test Loss: 0.005149 | Best Test: 0.004694 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1563\n",
      "Epoch  266 | Train Loss: 0.002690 | Test Loss: 0.005132 | Best Test: 0.004694 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1575\n",
      "Epoch  267 | Train Loss: 0.002816 | Test Loss: 0.005135 | Best Test: 0.004694 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1572\n",
      "Epoch  268 | Train Loss: 0.003396 | Test Loss: 0.005280 | Best Test: 0.004694 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1554\n",
      "Epoch  269 | Train Loss: 0.002938 | Test Loss: 0.005471 | Best Test: 0.004694 | Patience: 23/50 | LR: 1.00e-04 | Pred_std: 0.1566\n",
      "Epoch  270 | Train Loss: 0.002638 | Test Loss: 0.004980 | Best Test: 0.004694 | Patience: 24/50 | LR: 1.00e-04 | Pred_std: 0.1576\n",
      "Epoch  271 | Train Loss: 0.003012 | Test Loss: 0.004916 | Best Test: 0.004694 | Patience: 25/50 | LR: 1.00e-04 | Pred_std: 0.1565\n",
      "Epoch  272 | Train Loss: 0.003023 | Test Loss: 0.005580 | Best Test: 0.004694 | Patience: 26/50 | LR: 1.00e-04 | Pred_std: 0.1565\n",
      "Epoch  273 | Train Loss: 0.003093 | Test Loss: 0.004773 | Best Test: 0.004694 | Patience: 27/50 | LR: 1.00e-04 | Pred_std: 0.1563\n",
      "Epoch  274 | Train Loss: 0.002733 | Test Loss: 0.004402 | Best Test: 0.004402 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1575\n",
      "Epoch  275 | Train Loss: 0.002722 | Test Loss: 0.004615 | Best Test: 0.004402 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1574\n",
      "Epoch  276 | Train Loss: 0.002597 | Test Loss: 0.005832 | Best Test: 0.004402 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1577\n",
      "Epoch  277 | Train Loss: 0.003068 | Test Loss: 0.005133 | Best Test: 0.004402 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1565\n",
      "Epoch  278 | Train Loss: 0.002816 | Test Loss: 0.005963 | Best Test: 0.004402 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1572\n",
      "Epoch  279 | Train Loss: 0.002910 | Test Loss: 0.005058 | Best Test: 0.004402 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1566\n",
      "Epoch  280 | Train Loss: 0.002548 | Test Loss: 0.004942 | Best Test: 0.004402 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  281 | Train Loss: 0.002760 | Test Loss: 0.004944 | Best Test: 0.004402 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1573\n",
      "Epoch  282 | Train Loss: 0.002635 | Test Loss: 0.005344 | Best Test: 0.004402 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1576\n",
      "Epoch  283 | Train Loss: 0.002620 | Test Loss: 0.005387 | Best Test: 0.004402 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1579\n",
      "Epoch  284 | Train Loss: 0.002839 | Test Loss: 0.005178 | Best Test: 0.004402 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1568\n",
      "Epoch  285 | Train Loss: 0.002448 | Test Loss: 0.004716 | Best Test: 0.004402 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1584\n",
      "Epoch  286 | Train Loss: 0.002513 | Test Loss: 0.005191 | Best Test: 0.004402 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  287 | Train Loss: 0.002775 | Test Loss: 0.005012 | Best Test: 0.004402 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1573\n",
      "Epoch  288 | Train Loss: 0.002927 | Test Loss: 0.004590 | Best Test: 0.004402 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1568\n",
      "Epoch  289 | Train Loss: 0.002625 | Test Loss: 0.004970 | Best Test: 0.004402 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1577\n",
      "Epoch  290 | Train Loss: 0.002516 | Test Loss: 0.004957 | Best Test: 0.004402 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  291 | Train Loss: 0.002512 | Test Loss: 0.004737 | Best Test: 0.004402 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  292 | Train Loss: 0.002534 | Test Loss: 0.004718 | Best Test: 0.004402 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  293 | Train Loss: 0.002469 | Test Loss: 0.005059 | Best Test: 0.004402 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  294 | Train Loss: 0.002652 | Test Loss: 0.005610 | Best Test: 0.004402 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1576\n",
      "Epoch  295 | Train Loss: 0.002603 | Test Loss: 0.005634 | Best Test: 0.004402 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1579\n",
      "Epoch  296 | Train Loss: 0.002688 | Test Loss: 0.005351 | Best Test: 0.004402 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1576\n",
      "Epoch  297 | Train Loss: 0.002701 | Test Loss: 0.004426 | Best Test: 0.004402 | Patience: 23/50 | LR: 1.00e-04 | Pred_std: 0.1575\n",
      "Epoch  298 | Train Loss: 0.002467 | Test Loss: 0.004840 | Best Test: 0.004402 | Patience: 24/50 | LR: 1.00e-04 | Pred_std: 0.1582\n",
      "Epoch  299 | Train Loss: 0.002321 | Test Loss: 0.005730 | Best Test: 0.004402 | Patience: 25/50 | LR: 1.00e-04 | Pred_std: 0.1584\n",
      "Epoch  300 | Train Loss: 0.002581 | Test Loss: 0.005416 | Best Test: 0.004402 | Patience: 26/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  301 | Train Loss: 0.002578 | Test Loss: 0.004543 | Best Test: 0.004402 | Patience: 27/50 | LR: 1.00e-04 | Pred_std: 0.1579\n",
      "Epoch  302 | Train Loss: 0.002327 | Test Loss: 0.004888 | Best Test: 0.004402 | Patience: 28/50 | LR: 1.00e-04 | Pred_std: 0.1584\n",
      "Epoch  303 | Train Loss: 0.002494 | Test Loss: 0.004595 | Best Test: 0.004402 | Patience: 29/50 | LR: 1.00e-04 | Pred_std: 0.1582\n",
      "Epoch  304 | Train Loss: 0.002513 | Test Loss: 0.004907 | Best Test: 0.004402 | Patience: 30/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  305 | Train Loss: 0.002287 | Test Loss: 0.004809 | Best Test: 0.004402 | Patience: 31/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  306 | Train Loss: 0.002337 | Test Loss: 0.005139 | Best Test: 0.004402 | Patience: 32/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  307 | Train Loss: 0.002553 | Test Loss: 0.004532 | Best Test: 0.004402 | Patience: 33/50 | LR: 1.00e-04 | Pred_std: 0.1579\n",
      "Epoch  308 | Train Loss: 0.002353 | Test Loss: 0.005325 | Best Test: 0.004402 | Patience: 34/50 | LR: 1.00e-04 | Pred_std: 0.1586\n",
      "Epoch  309 | Train Loss: 0.002366 | Test Loss: 0.005632 | Best Test: 0.004402 | Patience: 35/50 | LR: 1.00e-04 | Pred_std: 0.1584\n",
      "Epoch  310 | Train Loss: 0.002172 | Test Loss: 0.005265 | Best Test: 0.004402 | Patience: 36/50 | LR: 1.00e-04 | Pred_std: 0.1591\n",
      "Epoch  311 | Train Loss: 0.002873 | Test Loss: 0.004948 | Best Test: 0.004402 | Patience: 37/50 | LR: 1.00e-04 | Pred_std: 0.1573\n",
      "Epoch  312 | Train Loss: 0.002529 | Test Loss: 0.004877 | Best Test: 0.004402 | Patience: 38/50 | LR: 1.00e-04 | Pred_std: 0.1578\n",
      "Epoch  313 | Train Loss: 0.002463 | Test Loss: 0.005361 | Best Test: 0.004402 | Patience: 39/50 | LR: 1.00e-04 | Pred_std: 0.1582\n",
      "Epoch  314 | Train Loss: 0.002412 | Test Loss: 0.005159 | Best Test: 0.004402 | Patience: 40/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  315 | Train Loss: 0.002514 | Test Loss: 0.005010 | Best Test: 0.004402 | Patience: 41/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  316 | Train Loss: 0.002411 | Test Loss: 0.004756 | Best Test: 0.004402 | Patience: 42/50 | LR: 1.00e-04 | Pred_std: 0.1583\n",
      "Epoch  317 | Train Loss: 0.002247 | Test Loss: 0.005509 | Best Test: 0.004402 | Patience: 43/50 | LR: 1.00e-04 | Pred_std: 0.1589\n",
      "Epoch  318 | Train Loss: 0.002242 | Test Loss: 0.004239 | Best Test: 0.004239 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1590\n",
      "Epoch  319 | Train Loss: 0.002379 | Test Loss: 0.005067 | Best Test: 0.004239 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1584\n",
      "Epoch  320 | Train Loss: 0.002314 | Test Loss: 0.004948 | Best Test: 0.004239 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1587\n",
      "Epoch  321 | Train Loss: 0.002167 | Test Loss: 0.005001 | Best Test: 0.004239 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1591\n",
      "Epoch  322 | Train Loss: 0.002351 | Test Loss: 0.004892 | Best Test: 0.004239 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  323 | Train Loss: 0.002359 | Test Loss: 0.004703 | Best Test: 0.004239 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1586\n",
      "Epoch  324 | Train Loss: 0.002074 | Test Loss: 0.004678 | Best Test: 0.004239 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1593\n",
      "Epoch  325 | Train Loss: 0.002347 | Test Loss: 0.004733 | Best Test: 0.004239 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1587\n",
      "Epoch  326 | Train Loss: 0.002237 | Test Loss: 0.004698 | Best Test: 0.004239 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  327 | Train Loss: 0.002222 | Test Loss: 0.004914 | Best Test: 0.004239 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1591\n",
      "Epoch  328 | Train Loss: 0.002541 | Test Loss: 0.005407 | Best Test: 0.004239 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  329 | Train Loss: 0.002290 | Test Loss: 0.004376 | Best Test: 0.004239 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  330 | Train Loss: 0.002514 | Test Loss: 0.005361 | Best Test: 0.004239 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  331 | Train Loss: 0.002265 | Test Loss: 0.004779 | Best Test: 0.004239 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  332 | Train Loss: 0.002148 | Test Loss: 0.004968 | Best Test: 0.004239 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1593\n",
      "Epoch  333 | Train Loss: 0.002305 | Test Loss: 0.004872 | Best Test: 0.004239 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1586\n",
      "Epoch  334 | Train Loss: 0.002204 | Test Loss: 0.005230 | Best Test: 0.004239 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1590\n",
      "Epoch  335 | Train Loss: 0.002476 | Test Loss: 0.005370 | Best Test: 0.004239 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1582\n",
      "Epoch  336 | Train Loss: 0.002224 | Test Loss: 0.005065 | Best Test: 0.004239 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1589\n",
      "Epoch  337 | Train Loss: 0.002098 | Test Loss: 0.004810 | Best Test: 0.004239 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1594\n",
      "Epoch  338 | Train Loss: 0.002242 | Test Loss: 0.004909 | Best Test: 0.004239 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  339 | Train Loss: 0.001954 | Test Loss: 0.004942 | Best Test: 0.004239 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  340 | Train Loss: 0.002203 | Test Loss: 0.005600 | Best Test: 0.004239 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1592\n",
      "Epoch  341 | Train Loss: 0.002445 | Test Loss: 0.005170 | Best Test: 0.004239 | Patience: 23/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  342 | Train Loss: 0.002244 | Test Loss: 0.005177 | Best Test: 0.004239 | Patience: 24/50 | LR: 1.00e-04 | Pred_std: 0.1590\n",
      "Epoch  343 | Train Loss: 0.002031 | Test Loss: 0.004391 | Best Test: 0.004239 | Patience: 25/50 | LR: 1.00e-04 | Pred_std: 0.1596\n",
      "Epoch  344 | Train Loss: 0.002012 | Test Loss: 0.004845 | Best Test: 0.004239 | Patience: 26/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  345 | Train Loss: 0.002083 | Test Loss: 0.004937 | Best Test: 0.004239 | Patience: 27/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  346 | Train Loss: 0.002514 | Test Loss: 0.005800 | Best Test: 0.004239 | Patience: 28/50 | LR: 1.00e-04 | Pred_std: 0.1583\n",
      "Epoch  347 | Train Loss: 0.002116 | Test Loss: 0.004533 | Best Test: 0.004239 | Patience: 29/50 | LR: 1.00e-04 | Pred_std: 0.1592\n",
      "Epoch  348 | Train Loss: 0.002229 | Test Loss: 0.004635 | Best Test: 0.004239 | Patience: 30/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  349 | Train Loss: 0.001875 | Test Loss: 0.004531 | Best Test: 0.004239 | Patience: 31/50 | LR: 1.00e-04 | Pred_std: 0.1601\n",
      "Epoch  350 | Train Loss: 0.002186 | Test Loss: 0.004827 | Best Test: 0.004239 | Patience: 32/50 | LR: 1.00e-04 | Pred_std: 0.1591\n",
      "Epoch  351 | Train Loss: 0.002276 | Test Loss: 0.004726 | Best Test: 0.004239 | Patience: 33/50 | LR: 1.00e-04 | Pred_std: 0.1589\n",
      "Epoch  352 | Train Loss: 0.002264 | Test Loss: 0.004746 | Best Test: 0.004239 | Patience: 34/50 | LR: 1.00e-04 | Pred_std: 0.1590\n",
      "Epoch  353 | Train Loss: 0.002083 | Test Loss: 0.005277 | Best Test: 0.004239 | Patience: 35/50 | LR: 1.00e-04 | Pred_std: 0.1592\n",
      "Epoch  354 | Train Loss: 0.002170 | Test Loss: 0.005429 | Best Test: 0.004239 | Patience: 36/50 | LR: 1.00e-04 | Pred_std: 0.1592\n",
      "Epoch  355 | Train Loss: 0.001925 | Test Loss: 0.005953 | Best Test: 0.004239 | Patience: 37/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  356 | Train Loss: 0.002300 | Test Loss: 0.005203 | Best Test: 0.004239 | Patience: 38/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  357 | Train Loss: 0.002025 | Test Loss: 0.004637 | Best Test: 0.004239 | Patience: 39/50 | LR: 1.00e-04 | Pred_std: 0.1596\n",
      "Epoch  358 | Train Loss: 0.001809 | Test Loss: 0.004848 | Best Test: 0.004239 | Patience: 40/50 | LR: 1.00e-04 | Pred_std: 0.1603\n",
      "Epoch  359 | Train Loss: 0.001912 | Test Loss: 0.004966 | Best Test: 0.004239 | Patience: 41/50 | LR: 1.00e-04 | Pred_std: 0.1600\n",
      "Epoch  360 | Train Loss: 0.001954 | Test Loss: 0.005129 | Best Test: 0.004239 | Patience: 42/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  361 | Train Loss: 0.001925 | Test Loss: 0.005162 | Best Test: 0.004239 | Patience: 43/50 | LR: 1.00e-04 | Pred_std: 0.1599\n",
      "Epoch  362 | Train Loss: 0.002157 | Test Loss: 0.004793 | Best Test: 0.004239 | Patience: 44/50 | LR: 1.00e-04 | Pred_std: 0.1592\n",
      "Epoch  363 | Train Loss: 0.002355 | Test Loss: 0.005149 | Best Test: 0.004239 | Patience: 45/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  364 | Train Loss: 0.002121 | Test Loss: 0.004612 | Best Test: 0.004239 | Patience: 46/50 | LR: 1.00e-04 | Pred_std: 0.1594\n",
      "Epoch  365 | Train Loss: 0.002021 | Test Loss: 0.005229 | Best Test: 0.004239 | Patience: 47/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  366 | Train Loss: 0.002318 | Test Loss: 0.005109 | Best Test: 0.004239 | Patience: 48/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  367 | Train Loss: 0.001830 | Test Loss: 0.004525 | Best Test: 0.004239 | Patience: 49/50 | LR: 1.00e-04 | Pred_std: 0.1602\n",
      "  Early stopping at epoch 368 (test_loss=0.005467)\n",
      "\n",
      "Evaluating on test set...\n",
      "Final train loss: 0.002130\n",
      "Test loss: 0.005467\n",
      "Generalization gap: 0.003338\n",
      "\n",
      " Config 'mpnn_layer_1' completed in 8513.5s\n",
      "\n",
      "############################################################\n",
      "# Running config 2/3: mpnn_layer_2\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "TRAINING CONFIG: mpnn_layer_2\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Epoch    1 | Train Loss: 0.025264 | Test Loss: 0.015125 | Best Test: 0.015125 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.0618\n",
      "Epoch    2 | Train Loss: 0.014239 | Test Loss: 0.012896 | Best Test: 0.012896 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1184\n",
      "Epoch    3 | Train Loss: 0.012719 | Test Loss: 0.011458 | Best Test: 0.011458 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1238\n",
      "Epoch    4 | Train Loss: 0.011885 | Test Loss: 0.011836 | Best Test: 0.011458 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1267\n",
      "Epoch    5 | Train Loss: 0.011839 | Test Loss: 0.011824 | Best Test: 0.011458 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1268\n",
      "Epoch    6 | Train Loss: 0.011727 | Test Loss: 0.010425 | Best Test: 0.010425 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1275\n",
      "Epoch    7 | Train Loss: 0.011286 | Test Loss: 0.010157 | Best Test: 0.010157 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1286\n",
      "Epoch    8 | Train Loss: 0.011048 | Test Loss: 0.010368 | Best Test: 0.010157 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1294\n",
      "Epoch    9 | Train Loss: 0.011013 | Test Loss: 0.010684 | Best Test: 0.010157 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1298\n",
      "Epoch   10 | Train Loss: 0.010775 | Test Loss: 0.010133 | Best Test: 0.010133 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1304\n",
      "Epoch   11 | Train Loss: 0.010641 | Test Loss: 0.010006 | Best Test: 0.010006 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1310\n",
      "Epoch   12 | Train Loss: 0.010649 | Test Loss: 0.010038 | Best Test: 0.010006 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1310\n",
      "Epoch   13 | Train Loss: 0.010269 | Test Loss: 0.009770 | Best Test: 0.009770 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1321\n",
      "Epoch   14 | Train Loss: 0.010108 | Test Loss: 0.009342 | Best Test: 0.009342 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1330\n",
      "Epoch   15 | Train Loss: 0.010099 | Test Loss: 0.009448 | Best Test: 0.009342 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1332\n",
      "Epoch   16 | Train Loss: 0.009858 | Test Loss: 0.009634 | Best Test: 0.009342 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1339\n",
      "Epoch   17 | Train Loss: 0.009881 | Test Loss: 0.009514 | Best Test: 0.009342 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1341\n",
      "Epoch   18 | Train Loss: 0.009569 | Test Loss: 0.008764 | Best Test: 0.008764 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1348\n",
      "Epoch   19 | Train Loss: 0.009567 | Test Loss: 0.009199 | Best Test: 0.008764 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1347\n",
      "Epoch   20 | Train Loss: 0.009471 | Test Loss: 0.008793 | Best Test: 0.008764 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1354\n",
      "Epoch   21 | Train Loss: 0.009254 | Test Loss: 0.009428 | Best Test: 0.008764 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1360\n",
      "Epoch   22 | Train Loss: 0.009175 | Test Loss: 0.008896 | Best Test: 0.008764 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1362\n",
      "Epoch   23 | Train Loss: 0.008996 | Test Loss: 0.008915 | Best Test: 0.008764 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1369\n",
      "Epoch   24 | Train Loss: 0.009055 | Test Loss: 0.008635 | Best Test: 0.008635 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1366\n",
      "Epoch   25 | Train Loss: 0.008954 | Test Loss: 0.009188 | Best Test: 0.008635 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1369\n",
      "Epoch   26 | Train Loss: 0.008931 | Test Loss: 0.008989 | Best Test: 0.008635 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1373\n",
      "Epoch   27 | Train Loss: 0.008738 | Test Loss: 0.008721 | Best Test: 0.008635 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1375\n",
      "Epoch   28 | Train Loss: 0.008684 | Test Loss: 0.008308 | Best Test: 0.008308 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1379\n",
      "Epoch   29 | Train Loss: 0.008710 | Test Loss: 0.008883 | Best Test: 0.008308 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1377\n",
      "Epoch   30 | Train Loss: 0.008668 | Test Loss: 0.008270 | Best Test: 0.008270 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1382\n",
      "Epoch   31 | Train Loss: 0.008520 | Test Loss: 0.008358 | Best Test: 0.008270 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1383\n",
      "Epoch   32 | Train Loss: 0.008677 | Test Loss: 0.008413 | Best Test: 0.008270 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1378\n",
      "Epoch   33 | Train Loss: 0.008447 | Test Loss: 0.007879 | Best Test: 0.007879 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1390\n",
      "Epoch   34 | Train Loss: 0.008491 | Test Loss: 0.008170 | Best Test: 0.007879 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1384\n",
      "Epoch   35 | Train Loss: 0.008420 | Test Loss: 0.008326 | Best Test: 0.007879 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1386\n",
      "Epoch   36 | Train Loss: 0.008314 | Test Loss: 0.008049 | Best Test: 0.007879 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1393\n",
      "Epoch   37 | Train Loss: 0.008415 | Test Loss: 0.008085 | Best Test: 0.007879 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1389\n",
      "Epoch   38 | Train Loss: 0.008210 | Test Loss: 0.008137 | Best Test: 0.007879 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1391\n",
      "Epoch   39 | Train Loss: 0.008169 | Test Loss: 0.008245 | Best Test: 0.007879 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1397\n",
      "Epoch   40 | Train Loss: 0.008327 | Test Loss: 0.007474 | Best Test: 0.007474 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1391\n",
      "Epoch   41 | Train Loss: 0.008081 | Test Loss: 0.008472 | Best Test: 0.007474 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1397\n",
      "Epoch   42 | Train Loss: 0.008114 | Test Loss: 0.008333 | Best Test: 0.007474 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1402\n",
      "Epoch   43 | Train Loss: 0.007938 | Test Loss: 0.007828 | Best Test: 0.007474 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1399\n",
      "Epoch   44 | Train Loss: 0.007976 | Test Loss: 0.008739 | Best Test: 0.007474 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1402\n",
      "Epoch   45 | Train Loss: 0.008122 | Test Loss: 0.007304 | Best Test: 0.007304 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1396\n",
      "Epoch   46 | Train Loss: 0.007897 | Test Loss: 0.008255 | Best Test: 0.007304 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1404\n",
      "Epoch   47 | Train Loss: 0.008054 | Test Loss: 0.007755 | Best Test: 0.007304 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1400\n",
      "Epoch   48 | Train Loss: 0.007840 | Test Loss: 0.008923 | Best Test: 0.007304 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1405\n",
      "Epoch   49 | Train Loss: 0.007891 | Test Loss: 0.007790 | Best Test: 0.007304 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1404\n",
      "Epoch   50 | Train Loss: 0.007864 | Test Loss: 0.009086 | Best Test: 0.007304 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1405\n",
      "Epoch   51 | Train Loss: 0.007785 | Test Loss: 0.008464 | Best Test: 0.007304 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1406\n",
      "Epoch   52 | Train Loss: 0.007837 | Test Loss: 0.007543 | Best Test: 0.007304 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1407\n",
      "Epoch   53 | Train Loss: 0.007746 | Test Loss: 0.007213 | Best Test: 0.007213 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1409\n",
      "Epoch   54 | Train Loss: 0.007701 | Test Loss: 0.007826 | Best Test: 0.007213 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1407\n",
      "Epoch   55 | Train Loss: 0.007724 | Test Loss: 0.008351 | Best Test: 0.007213 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1412\n",
      "Epoch   56 | Train Loss: 0.007709 | Test Loss: 0.007149 | Best Test: 0.007149 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1408\n",
      "Epoch   57 | Train Loss: 0.007627 | Test Loss: 0.007301 | Best Test: 0.007149 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1413\n",
      "Epoch   58 | Train Loss: 0.007546 | Test Loss: 0.007432 | Best Test: 0.007149 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1415\n",
      "Epoch   59 | Train Loss: 0.007522 | Test Loss: 0.007240 | Best Test: 0.007149 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1415\n",
      "Epoch   60 | Train Loss: 0.007549 | Test Loss: 0.007172 | Best Test: 0.007149 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1414\n",
      "Epoch   61 | Train Loss: 0.007528 | Test Loss: 0.007432 | Best Test: 0.007149 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1417\n",
      "Epoch   62 | Train Loss: 0.007427 | Test Loss: 0.007005 | Best Test: 0.007005 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1418\n",
      "Epoch   63 | Train Loss: 0.007461 | Test Loss: 0.007587 | Best Test: 0.007005 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1417\n",
      "Epoch   64 | Train Loss: 0.007380 | Test Loss: 0.007772 | Best Test: 0.007005 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1422\n",
      "Epoch   65 | Train Loss: 0.007263 | Test Loss: 0.007873 | Best Test: 0.007005 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1423\n",
      "Epoch   66 | Train Loss: 0.007249 | Test Loss: 0.009642 | Best Test: 0.007005 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1429\n",
      "Epoch   67 | Train Loss: 0.007514 | Test Loss: 0.007576 | Best Test: 0.007005 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1414\n",
      "Epoch   68 | Train Loss: 0.007231 | Test Loss: 0.007032 | Best Test: 0.007005 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1426\n",
      "Epoch   69 | Train Loss: 0.007192 | Test Loss: 0.007720 | Best Test: 0.007005 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1429\n",
      "Epoch   70 | Train Loss: 0.007117 | Test Loss: 0.007299 | Best Test: 0.007005 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1429\n",
      "Epoch   71 | Train Loss: 0.007177 | Test Loss: 0.006826 | Best Test: 0.006826 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1423\n",
      "Epoch   72 | Train Loss: 0.006948 | Test Loss: 0.007156 | Best Test: 0.006826 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1435\n",
      "Epoch   73 | Train Loss: 0.007136 | Test Loss: 0.007858 | Best Test: 0.006826 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1429\n",
      "Epoch   74 | Train Loss: 0.007069 | Test Loss: 0.007317 | Best Test: 0.006826 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1432\n",
      "Epoch   75 | Train Loss: 0.006948 | Test Loss: 0.007449 | Best Test: 0.006826 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1436\n",
      "Epoch   76 | Train Loss: 0.007069 | Test Loss: 0.006800 | Best Test: 0.006800 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1429\n",
      "Epoch   77 | Train Loss: 0.006831 | Test Loss: 0.007027 | Best Test: 0.006800 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1438\n",
      "Epoch   78 | Train Loss: 0.006770 | Test Loss: 0.007162 | Best Test: 0.006800 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1448\n",
      "Epoch   79 | Train Loss: 0.007055 | Test Loss: 0.006938 | Best Test: 0.006800 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1426\n",
      "Epoch   80 | Train Loss: 0.006709 | Test Loss: 0.006666 | Best Test: 0.006666 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1446\n",
      "Epoch   81 | Train Loss: 0.006943 | Test Loss: 0.006952 | Best Test: 0.006666 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1435\n",
      "Epoch   82 | Train Loss: 0.006793 | Test Loss: 0.006606 | Best Test: 0.006606 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1438\n",
      "Epoch   83 | Train Loss: 0.006828 | Test Loss: 0.006705 | Best Test: 0.006606 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1441\n",
      "Epoch   84 | Train Loss: 0.006825 | Test Loss: 0.006510 | Best Test: 0.006510 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1438\n",
      "Epoch   85 | Train Loss: 0.006589 | Test Loss: 0.006550 | Best Test: 0.006510 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1446\n",
      "Epoch   86 | Train Loss: 0.006677 | Test Loss: 0.007295 | Best Test: 0.006510 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1447\n",
      "Epoch   87 | Train Loss: 0.006730 | Test Loss: 0.007610 | Best Test: 0.006510 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1440\n",
      "Epoch   88 | Train Loss: 0.006544 | Test Loss: 0.006735 | Best Test: 0.006510 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1449\n",
      "Epoch   89 | Train Loss: 0.006544 | Test Loss: 0.007430 | Best Test: 0.006510 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1450\n",
      "Epoch   90 | Train Loss: 0.006612 | Test Loss: 0.006836 | Best Test: 0.006510 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1447\n",
      "Epoch   91 | Train Loss: 0.006652 | Test Loss: 0.006173 | Best Test: 0.006173 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1444\n",
      "Epoch   92 | Train Loss: 0.006458 | Test Loss: 0.007385 | Best Test: 0.006173 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1455\n",
      "Epoch   93 | Train Loss: 0.006426 | Test Loss: 0.006861 | Best Test: 0.006173 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1450\n",
      "Epoch   94 | Train Loss: 0.006627 | Test Loss: 0.006583 | Best Test: 0.006173 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1445\n",
      "Epoch   95 | Train Loss: 0.006521 | Test Loss: 0.006508 | Best Test: 0.006173 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1452\n",
      "Epoch   96 | Train Loss: 0.006417 | Test Loss: 0.006723 | Best Test: 0.006173 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1451\n",
      "Epoch   97 | Train Loss: 0.006329 | Test Loss: 0.007245 | Best Test: 0.006173 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1457\n",
      "Epoch   98 | Train Loss: 0.006383 | Test Loss: 0.006335 | Best Test: 0.006173 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1454\n",
      "Epoch   99 | Train Loss: 0.006325 | Test Loss: 0.006411 | Best Test: 0.006173 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1457\n",
      "Epoch  100 | Train Loss: 0.006031 | Test Loss: 0.006212 | Best Test: 0.006173 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1462\n",
      "Epoch  101 | Train Loss: 0.006009 | Test Loss: 0.006338 | Best Test: 0.006173 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1470\n",
      "Epoch  102 | Train Loss: 0.006418 | Test Loss: 0.006286 | Best Test: 0.006173 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1451\n",
      "Epoch  103 | Train Loss: 0.006145 | Test Loss: 0.006216 | Best Test: 0.006173 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1461\n",
      "Epoch  104 | Train Loss: 0.006053 | Test Loss: 0.006972 | Best Test: 0.006173 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1465\n",
      "Epoch  105 | Train Loss: 0.006104 | Test Loss: 0.006768 | Best Test: 0.006173 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1464\n",
      "Epoch  106 | Train Loss: 0.005958 | Test Loss: 0.006419 | Best Test: 0.006173 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1470\n",
      "Epoch  107 | Train Loss: 0.006005 | Test Loss: 0.006083 | Best Test: 0.006083 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1466\n",
      "Epoch  108 | Train Loss: 0.005808 | Test Loss: 0.006207 | Best Test: 0.006083 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1473\n",
      "Epoch  109 | Train Loss: 0.006052 | Test Loss: 0.006499 | Best Test: 0.006083 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1466\n",
      "Epoch  110 | Train Loss: 0.006019 | Test Loss: 0.006544 | Best Test: 0.006083 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1468\n",
      "Epoch  111 | Train Loss: 0.005901 | Test Loss: 0.006711 | Best Test: 0.006083 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1469\n",
      "Epoch  112 | Train Loss: 0.005883 | Test Loss: 0.007339 | Best Test: 0.006083 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1472\n",
      "Epoch  113 | Train Loss: 0.005814 | Test Loss: 0.006191 | Best Test: 0.006083 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1475\n",
      "Epoch  114 | Train Loss: 0.005654 | Test Loss: 0.006857 | Best Test: 0.006083 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1476\n",
      "Epoch  115 | Train Loss: 0.005700 | Test Loss: 0.006348 | Best Test: 0.006083 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1476\n",
      "Epoch  116 | Train Loss: 0.005648 | Test Loss: 0.006052 | Best Test: 0.006052 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1477\n",
      "Epoch  117 | Train Loss: 0.005554 | Test Loss: 0.006167 | Best Test: 0.006052 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1485\n",
      "Epoch  118 | Train Loss: 0.005501 | Test Loss: 0.006231 | Best Test: 0.006052 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1483\n",
      "Epoch  119 | Train Loss: 0.005589 | Test Loss: 0.006280 | Best Test: 0.006052 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1479\n",
      "Epoch  120 | Train Loss: 0.005617 | Test Loss: 0.006106 | Best Test: 0.006052 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1481\n",
      "Epoch  121 | Train Loss: 0.005529 | Test Loss: 0.007029 | Best Test: 0.006052 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1482\n",
      "Epoch  122 | Train Loss: 0.005684 | Test Loss: 0.006635 | Best Test: 0.006052 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1479\n",
      "Epoch  123 | Train Loss: 0.005543 | Test Loss: 0.005980 | Best Test: 0.005980 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1480\n",
      "Epoch  124 | Train Loss: 0.005424 | Test Loss: 0.005942 | Best Test: 0.005942 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1488\n",
      "Epoch  125 | Train Loss: 0.005284 | Test Loss: 0.006797 | Best Test: 0.005942 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1488\n",
      "Epoch  126 | Train Loss: 0.005253 | Test Loss: 0.006987 | Best Test: 0.005942 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1492\n",
      "Epoch  127 | Train Loss: 0.005297 | Test Loss: 0.006179 | Best Test: 0.005942 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1495\n",
      "Epoch  128 | Train Loss: 0.005394 | Test Loss: 0.006301 | Best Test: 0.005942 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1486\n",
      "Epoch  129 | Train Loss: 0.005460 | Test Loss: 0.006309 | Best Test: 0.005942 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1485\n",
      "Epoch  130 | Train Loss: 0.005033 | Test Loss: 0.005876 | Best Test: 0.005876 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1499\n",
      "Epoch  131 | Train Loss: 0.005273 | Test Loss: 0.006949 | Best Test: 0.005876 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1493\n",
      "Epoch  132 | Train Loss: 0.005115 | Test Loss: 0.006039 | Best Test: 0.005876 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1497\n",
      "Epoch  133 | Train Loss: 0.005262 | Test Loss: 0.005841 | Best Test: 0.005841 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1493\n",
      "Epoch  134 | Train Loss: 0.005205 | Test Loss: 0.006218 | Best Test: 0.005841 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1491\n",
      "Epoch  135 | Train Loss: 0.005237 | Test Loss: 0.005849 | Best Test: 0.005841 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1495\n",
      "Epoch  136 | Train Loss: 0.005082 | Test Loss: 0.005980 | Best Test: 0.005841 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1500\n",
      "Epoch  137 | Train Loss: 0.005099 | Test Loss: 0.006019 | Best Test: 0.005841 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1496\n",
      "Epoch  138 | Train Loss: 0.005181 | Test Loss: 0.005924 | Best Test: 0.005841 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1492\n",
      "Epoch  139 | Train Loss: 0.004999 | Test Loss: 0.006047 | Best Test: 0.005841 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1502\n",
      "Epoch  140 | Train Loss: 0.004995 | Test Loss: 0.006036 | Best Test: 0.005841 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1500\n",
      "Epoch  141 | Train Loss: 0.004854 | Test Loss: 0.006096 | Best Test: 0.005841 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1504\n",
      "Epoch  142 | Train Loss: 0.004966 | Test Loss: 0.006103 | Best Test: 0.005841 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1502\n",
      "Epoch  143 | Train Loss: 0.004714 | Test Loss: 0.005673 | Best Test: 0.005673 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1510\n",
      "Epoch  144 | Train Loss: 0.004752 | Test Loss: 0.006911 | Best Test: 0.005673 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1506\n",
      "Epoch  145 | Train Loss: 0.004771 | Test Loss: 0.006383 | Best Test: 0.005673 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1509\n",
      "Epoch  146 | Train Loss: 0.004914 | Test Loss: 0.006030 | Best Test: 0.005673 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1501\n",
      "Epoch  147 | Train Loss: 0.004934 | Test Loss: 0.006746 | Best Test: 0.005673 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1505\n",
      "Epoch  148 | Train Loss: 0.004723 | Test Loss: 0.006233 | Best Test: 0.005673 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1510\n",
      "Epoch  149 | Train Loss: 0.005089 | Test Loss: 0.006384 | Best Test: 0.005673 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1497\n",
      "Epoch  150 | Train Loss: 0.004715 | Test Loss: 0.005889 | Best Test: 0.005673 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1510\n",
      "Epoch  151 | Train Loss: 0.004622 | Test Loss: 0.006239 | Best Test: 0.005673 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1516\n",
      "Epoch  152 | Train Loss: 0.004626 | Test Loss: 0.005698 | Best Test: 0.005673 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1509\n",
      "Epoch  153 | Train Loss: 0.004434 | Test Loss: 0.005659 | Best Test: 0.005659 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1521\n",
      "Epoch  154 | Train Loss: 0.004895 | Test Loss: 0.006235 | Best Test: 0.005659 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1504\n",
      "Epoch  155 | Train Loss: 0.004466 | Test Loss: 0.005928 | Best Test: 0.005659 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1518\n",
      "Epoch  156 | Train Loss: 0.004521 | Test Loss: 0.005668 | Best Test: 0.005659 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1514\n",
      "Epoch  157 | Train Loss: 0.004414 | Test Loss: 0.005725 | Best Test: 0.005659 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1521\n",
      "Epoch  158 | Train Loss: 0.004386 | Test Loss: 0.007131 | Best Test: 0.005659 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1521\n",
      "Epoch  159 | Train Loss: 0.004699 | Test Loss: 0.005554 | Best Test: 0.005554 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1510\n",
      "Epoch  160 | Train Loss: 0.004408 | Test Loss: 0.005669 | Best Test: 0.005554 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1520\n",
      "Epoch  161 | Train Loss: 0.004428 | Test Loss: 0.005618 | Best Test: 0.005554 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1518\n",
      "Epoch  162 | Train Loss: 0.004319 | Test Loss: 0.006038 | Best Test: 0.005554 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1522\n",
      "Epoch  163 | Train Loss: 0.004354 | Test Loss: 0.005592 | Best Test: 0.005554 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1523\n",
      "Epoch  164 | Train Loss: 0.004588 | Test Loss: 0.005850 | Best Test: 0.005554 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1515\n",
      "Epoch  165 | Train Loss: 0.004369 | Test Loss: 0.005499 | Best Test: 0.005499 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1521\n",
      "Epoch  166 | Train Loss: 0.004211 | Test Loss: 0.006366 | Best Test: 0.005499 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1524\n",
      "Epoch  167 | Train Loss: 0.004402 | Test Loss: 0.006168 | Best Test: 0.005499 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1521\n",
      "Epoch  168 | Train Loss: 0.004241 | Test Loss: 0.005416 | Best Test: 0.005416 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1527\n",
      "Epoch  169 | Train Loss: 0.004342 | Test Loss: 0.005702 | Best Test: 0.005416 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1520\n",
      "Epoch  170 | Train Loss: 0.004147 | Test Loss: 0.005432 | Best Test: 0.005416 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1531\n",
      "Epoch  171 | Train Loss: 0.004077 | Test Loss: 0.005501 | Best Test: 0.005416 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1530\n",
      "Epoch  172 | Train Loss: 0.004058 | Test Loss: 0.005605 | Best Test: 0.005416 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1531\n",
      "Epoch  173 | Train Loss: 0.004272 | Test Loss: 0.006027 | Best Test: 0.005416 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1526\n",
      "Epoch  174 | Train Loss: 0.004204 | Test Loss: 0.005942 | Best Test: 0.005416 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1525\n",
      "Epoch  175 | Train Loss: 0.004264 | Test Loss: 0.005242 | Best Test: 0.005242 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1525\n",
      "Epoch  176 | Train Loss: 0.003904 | Test Loss: 0.005795 | Best Test: 0.005242 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1534\n",
      "Epoch  177 | Train Loss: 0.004000 | Test Loss: 0.006347 | Best Test: 0.005242 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1535\n",
      "Epoch  178 | Train Loss: 0.003885 | Test Loss: 0.005505 | Best Test: 0.005242 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1537\n",
      "Epoch  179 | Train Loss: 0.003921 | Test Loss: 0.005419 | Best Test: 0.005242 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1535\n",
      "Epoch  180 | Train Loss: 0.004069 | Test Loss: 0.005538 | Best Test: 0.005242 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1532\n",
      "Epoch  181 | Train Loss: 0.003765 | Test Loss: 0.006788 | Best Test: 0.005242 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1541\n",
      "Epoch  182 | Train Loss: 0.004046 | Test Loss: 0.005572 | Best Test: 0.005242 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1531\n",
      "Epoch  183 | Train Loss: 0.003971 | Test Loss: 0.005361 | Best Test: 0.005242 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1533\n",
      "Epoch  184 | Train Loss: 0.003880 | Test Loss: 0.005178 | Best Test: 0.005178 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1539\n",
      "Epoch  185 | Train Loss: 0.003789 | Test Loss: 0.005303 | Best Test: 0.005178 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1540\n",
      "Epoch  186 | Train Loss: 0.003822 | Test Loss: 0.005102 | Best Test: 0.005102 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1540\n",
      "Epoch  187 | Train Loss: 0.004177 | Test Loss: 0.005636 | Best Test: 0.005102 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1527\n",
      "Epoch  188 | Train Loss: 0.003622 | Test Loss: 0.005743 | Best Test: 0.005102 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1545\n",
      "Epoch  189 | Train Loss: 0.003759 | Test Loss: 0.005633 | Best Test: 0.005102 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1542\n",
      "Epoch  190 | Train Loss: 0.003825 | Test Loss: 0.006372 | Best Test: 0.005102 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1538\n",
      "Epoch  191 | Train Loss: 0.003860 | Test Loss: 0.005192 | Best Test: 0.005102 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1539\n",
      "Epoch  192 | Train Loss: 0.004176 | Test Loss: 0.005608 | Best Test: 0.005102 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1528\n",
      "Epoch  193 | Train Loss: 0.003619 | Test Loss: 0.005568 | Best Test: 0.005102 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1543\n",
      "Epoch  194 | Train Loss: 0.003410 | Test Loss: 0.005409 | Best Test: 0.005102 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  195 | Train Loss: 0.003620 | Test Loss: 0.005231 | Best Test: 0.005102 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1545\n",
      "Epoch  196 | Train Loss: 0.003569 | Test Loss: 0.005056 | Best Test: 0.005056 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1546\n",
      "Epoch  197 | Train Loss: 0.003608 | Test Loss: 0.005368 | Best Test: 0.005056 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1546\n",
      "Epoch  198 | Train Loss: 0.003541 | Test Loss: 0.005793 | Best Test: 0.005056 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1546\n",
      "Epoch  199 | Train Loss: 0.003449 | Test Loss: 0.005437 | Best Test: 0.005056 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1552\n",
      "Epoch  200 | Train Loss: 0.003332 | Test Loss: 0.005760 | Best Test: 0.005056 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1555\n",
      "Epoch  201 | Train Loss: 0.004221 | Test Loss: 0.006219 | Best Test: 0.005056 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1528\n",
      "Epoch  202 | Train Loss: 0.003553 | Test Loss: 0.005228 | Best Test: 0.005056 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1548\n",
      "Epoch  203 | Train Loss: 0.003547 | Test Loss: 0.005022 | Best Test: 0.005022 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1548\n",
      "Epoch  204 | Train Loss: 0.003555 | Test Loss: 0.005572 | Best Test: 0.005022 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1547\n",
      "Epoch  205 | Train Loss: 0.003374 | Test Loss: 0.005063 | Best Test: 0.005022 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1554\n",
      "Epoch  206 | Train Loss: 0.003391 | Test Loss: 0.005465 | Best Test: 0.005022 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1552\n",
      "Epoch  207 | Train Loss: 0.003478 | Test Loss: 0.005249 | Best Test: 0.005022 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1547\n",
      "Epoch  208 | Train Loss: 0.003405 | Test Loss: 0.005116 | Best Test: 0.005022 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1555\n",
      "Epoch  209 | Train Loss: 0.003331 | Test Loss: 0.005207 | Best Test: 0.005022 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  210 | Train Loss: 0.003176 | Test Loss: 0.005251 | Best Test: 0.005022 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1558\n",
      "Epoch  211 | Train Loss: 0.003450 | Test Loss: 0.005186 | Best Test: 0.005022 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  212 | Train Loss: 0.003363 | Test Loss: 0.005449 | Best Test: 0.005022 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1554\n",
      "Epoch  213 | Train Loss: 0.003267 | Test Loss: 0.005055 | Best Test: 0.005022 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1556\n",
      "Epoch  214 | Train Loss: 0.003411 | Test Loss: 0.005098 | Best Test: 0.005022 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  215 | Train Loss: 0.003144 | Test Loss: 0.004878 | Best Test: 0.004878 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1559\n",
      "Epoch  216 | Train Loss: 0.003225 | Test Loss: 0.005293 | Best Test: 0.004878 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1557\n",
      "Epoch  217 | Train Loss: 0.003191 | Test Loss: 0.004957 | Best Test: 0.004878 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1560\n",
      "Epoch  218 | Train Loss: 0.003295 | Test Loss: 0.005418 | Best Test: 0.004878 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1557\n",
      "Epoch  219 | Train Loss: 0.003340 | Test Loss: 0.004883 | Best Test: 0.004878 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  220 | Train Loss: 0.003277 | Test Loss: 0.004987 | Best Test: 0.004878 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1557\n",
      "Epoch  221 | Train Loss: 0.003424 | Test Loss: 0.004986 | Best Test: 0.004878 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  222 | Train Loss: 0.003101 | Test Loss: 0.005426 | Best Test: 0.004878 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1559\n",
      "Epoch  223 | Train Loss: 0.002958 | Test Loss: 0.005408 | Best Test: 0.004878 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1567\n",
      "Epoch  224 | Train Loss: 0.003204 | Test Loss: 0.004935 | Best Test: 0.004878 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1559\n",
      "Epoch  225 | Train Loss: 0.002890 | Test Loss: 0.005475 | Best Test: 0.004878 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1571\n",
      "Epoch  226 | Train Loss: 0.003086 | Test Loss: 0.005185 | Best Test: 0.004878 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1562\n",
      "Epoch  227 | Train Loss: 0.003629 | Test Loss: 0.005084 | Best Test: 0.004878 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1546\n",
      "Epoch  228 | Train Loss: 0.003346 | Test Loss: 0.005460 | Best Test: 0.004878 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1554\n",
      "Epoch  229 | Train Loss: 0.002933 | Test Loss: 0.005272 | Best Test: 0.004878 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1565\n",
      "Epoch  230 | Train Loss: 0.002976 | Test Loss: 0.005784 | Best Test: 0.004878 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1566\n",
      "Epoch  231 | Train Loss: 0.002914 | Test Loss: 0.005309 | Best Test: 0.004878 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1570\n",
      "Epoch  232 | Train Loss: 0.003057 | Test Loss: 0.005140 | Best Test: 0.004878 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1563\n",
      "Epoch  233 | Train Loss: 0.002823 | Test Loss: 0.005206 | Best Test: 0.004878 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1572\n",
      "Epoch  234 | Train Loss: 0.002816 | Test Loss: 0.004932 | Best Test: 0.004878 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1570\n",
      "Epoch  235 | Train Loss: 0.003019 | Test Loss: 0.004784 | Best Test: 0.004784 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1564\n",
      "Epoch  236 | Train Loss: 0.003219 | Test Loss: 0.005852 | Best Test: 0.004784 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1559\n",
      "Epoch  237 | Train Loss: 0.003155 | Test Loss: 0.004847 | Best Test: 0.004784 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1561\n",
      "Epoch  238 | Train Loss: 0.003030 | Test Loss: 0.004891 | Best Test: 0.004784 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1565\n",
      "Epoch  239 | Train Loss: 0.002876 | Test Loss: 0.004860 | Best Test: 0.004784 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1566\n",
      "Epoch  240 | Train Loss: 0.002927 | Test Loss: 0.004921 | Best Test: 0.004784 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1569\n",
      "Epoch  241 | Train Loss: 0.003207 | Test Loss: 0.005250 | Best Test: 0.004784 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1558\n",
      "Epoch  242 | Train Loss: 0.002894 | Test Loss: 0.005306 | Best Test: 0.004784 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1568\n",
      "Epoch  243 | Train Loss: 0.002752 | Test Loss: 0.004601 | Best Test: 0.004601 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1574\n",
      "Epoch  244 | Train Loss: 0.002826 | Test Loss: 0.004760 | Best Test: 0.004601 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1572\n",
      "Epoch  245 | Train Loss: 0.002879 | Test Loss: 0.004863 | Best Test: 0.004601 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1569\n",
      "Epoch  246 | Train Loss: 0.002922 | Test Loss: 0.004946 | Best Test: 0.004601 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1568\n",
      "Epoch  247 | Train Loss: 0.002704 | Test Loss: 0.005255 | Best Test: 0.004601 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1573\n",
      "Epoch  248 | Train Loss: 0.003143 | Test Loss: 0.005096 | Best Test: 0.004601 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1561\n",
      "Epoch  249 | Train Loss: 0.002661 | Test Loss: 0.004570 | Best Test: 0.004570 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1577\n",
      "Epoch  250 | Train Loss: 0.002651 | Test Loss: 0.004930 | Best Test: 0.004570 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1575\n",
      "Epoch  251 | Train Loss: 0.002726 | Test Loss: 0.005009 | Best Test: 0.004570 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1575\n",
      "Epoch  252 | Train Loss: 0.002600 | Test Loss: 0.005142 | Best Test: 0.004570 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1578\n",
      "Epoch  253 | Train Loss: 0.002798 | Test Loss: 0.004919 | Best Test: 0.004570 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1571\n",
      "Epoch  254 | Train Loss: 0.002746 | Test Loss: 0.004806 | Best Test: 0.004570 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1575\n",
      "Epoch  255 | Train Loss: 0.002692 | Test Loss: 0.004993 | Best Test: 0.004570 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1573\n",
      "Epoch  256 | Train Loss: 0.002766 | Test Loss: 0.004651 | Best Test: 0.004570 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1572\n",
      "Epoch  257 | Train Loss: 0.002734 | Test Loss: 0.004983 | Best Test: 0.004570 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1575\n",
      "Epoch  258 | Train Loss: 0.002646 | Test Loss: 0.004735 | Best Test: 0.004570 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1576\n",
      "Epoch  259 | Train Loss: 0.002627 | Test Loss: 0.004372 | Best Test: 0.004372 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1577\n",
      "Epoch  260 | Train Loss: 0.002428 | Test Loss: 0.005011 | Best Test: 0.004372 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1582\n",
      "Epoch  261 | Train Loss: 0.002447 | Test Loss: 0.004753 | Best Test: 0.004372 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1583\n",
      "Epoch  262 | Train Loss: 0.002683 | Test Loss: 0.005135 | Best Test: 0.004372 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1575\n",
      "Epoch  263 | Train Loss: 0.002639 | Test Loss: 0.004520 | Best Test: 0.004372 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1578\n",
      "Epoch  264 | Train Loss: 0.002810 | Test Loss: 0.005011 | Best Test: 0.004372 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1572\n",
      "Epoch  265 | Train Loss: 0.002508 | Test Loss: 0.004542 | Best Test: 0.004372 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  266 | Train Loss: 0.002535 | Test Loss: 0.004805 | Best Test: 0.004372 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  267 | Train Loss: 0.002524 | Test Loss: 0.004902 | Best Test: 0.004372 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  268 | Train Loss: 0.002668 | Test Loss: 0.004596 | Best Test: 0.004372 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1576\n",
      "Epoch  269 | Train Loss: 0.002639 | Test Loss: 0.004438 | Best Test: 0.004372 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1576\n",
      "Epoch  270 | Train Loss: 0.002503 | Test Loss: 0.004610 | Best Test: 0.004372 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  271 | Train Loss: 0.002519 | Test Loss: 0.004739 | Best Test: 0.004372 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1582\n",
      "Epoch  272 | Train Loss: 0.002279 | Test Loss: 0.005088 | Best Test: 0.004372 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1586\n",
      "Epoch  273 | Train Loss: 0.002668 | Test Loss: 0.005583 | Best Test: 0.004372 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1578\n",
      "Epoch  274 | Train Loss: 0.002558 | Test Loss: 0.004714 | Best Test: 0.004372 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1579\n",
      "Epoch  275 | Train Loss: 0.002956 | Test Loss: 0.004612 | Best Test: 0.004372 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1565\n",
      "Epoch  276 | Train Loss: 0.002288 | Test Loss: 0.004790 | Best Test: 0.004372 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1589\n",
      "Epoch  277 | Train Loss: 0.002421 | Test Loss: 0.004557 | Best Test: 0.004372 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1582\n",
      "Epoch  278 | Train Loss: 0.002363 | Test Loss: 0.004699 | Best Test: 0.004372 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1586\n",
      "Epoch  279 | Train Loss: 0.002151 | Test Loss: 0.004888 | Best Test: 0.004372 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1593\n",
      "Epoch  280 | Train Loss: 0.002352 | Test Loss: 0.005055 | Best Test: 0.004372 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  281 | Train Loss: 0.002297 | Test Loss: 0.004705 | Best Test: 0.004372 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  282 | Train Loss: 0.002183 | Test Loss: 0.005109 | Best Test: 0.004372 | Patience: 23/50 | LR: 1.00e-04 | Pred_std: 0.1592\n",
      "Epoch  283 | Train Loss: 0.002464 | Test Loss: 0.005950 | Best Test: 0.004372 | Patience: 24/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  284 | Train Loss: 0.002549 | Test Loss: 0.005024 | Best Test: 0.004372 | Patience: 25/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  285 | Train Loss: 0.002360 | Test Loss: 0.004728 | Best Test: 0.004372 | Patience: 26/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  286 | Train Loss: 0.002281 | Test Loss: 0.005089 | Best Test: 0.004372 | Patience: 27/50 | LR: 1.00e-04 | Pred_std: 0.1589\n",
      "Epoch  287 | Train Loss: 0.002270 | Test Loss: 0.004550 | Best Test: 0.004372 | Patience: 28/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  288 | Train Loss: 0.002102 | Test Loss: 0.004423 | Best Test: 0.004372 | Patience: 29/50 | LR: 1.00e-04 | Pred_std: 0.1593\n",
      "Epoch  289 | Train Loss: 0.002700 | Test Loss: 0.005119 | Best Test: 0.004372 | Patience: 30/50 | LR: 1.00e-04 | Pred_std: 0.1578\n",
      "Epoch  290 | Train Loss: 0.002362 | Test Loss: 0.004790 | Best Test: 0.004372 | Patience: 31/50 | LR: 1.00e-04 | Pred_std: 0.1584\n",
      "Epoch  291 | Train Loss: 0.002234 | Test Loss: 0.004560 | Best Test: 0.004372 | Patience: 32/50 | LR: 1.00e-04 | Pred_std: 0.1590\n",
      "Epoch  292 | Train Loss: 0.002024 | Test Loss: 0.004776 | Best Test: 0.004372 | Patience: 33/50 | LR: 1.00e-04 | Pred_std: 0.1596\n",
      "Epoch  293 | Train Loss: 0.002236 | Test Loss: 0.005323 | Best Test: 0.004372 | Patience: 34/50 | LR: 1.00e-04 | Pred_std: 0.1591\n",
      "Epoch  294 | Train Loss: 0.002396 | Test Loss: 0.005006 | Best Test: 0.004372 | Patience: 35/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  295 | Train Loss: 0.002219 | Test Loss: 0.004597 | Best Test: 0.004372 | Patience: 36/50 | LR: 1.00e-04 | Pred_std: 0.1591\n",
      "Epoch  296 | Train Loss: 0.002257 | Test Loss: 0.004413 | Best Test: 0.004372 | Patience: 37/50 | LR: 1.00e-04 | Pred_std: 0.1589\n",
      "Epoch  297 | Train Loss: 0.001941 | Test Loss: 0.004441 | Best Test: 0.004372 | Patience: 38/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  298 | Train Loss: 0.002228 | Test Loss: 0.004688 | Best Test: 0.004372 | Patience: 39/50 | LR: 1.00e-04 | Pred_std: 0.1590\n",
      "Epoch  299 | Train Loss: 0.002737 | Test Loss: 0.004916 | Best Test: 0.004372 | Patience: 40/50 | LR: 1.00e-04 | Pred_std: 0.1573\n",
      "Epoch  300 | Train Loss: 0.002436 | Test Loss: 0.004619 | Best Test: 0.004372 | Patience: 41/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  301 | Train Loss: 0.002078 | Test Loss: 0.004621 | Best Test: 0.004372 | Patience: 42/50 | LR: 1.00e-04 | Pred_std: 0.1592\n",
      "Epoch  302 | Train Loss: 0.001986 | Test Loss: 0.004303 | Best Test: 0.004303 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  303 | Train Loss: 0.001863 | Test Loss: 0.004567 | Best Test: 0.004303 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1602\n",
      "Epoch  304 | Train Loss: 0.002006 | Test Loss: 0.005180 | Best Test: 0.004303 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  305 | Train Loss: 0.002158 | Test Loss: 0.004341 | Best Test: 0.004303 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1593\n",
      "Epoch  306 | Train Loss: 0.001995 | Test Loss: 0.004729 | Best Test: 0.004303 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  307 | Train Loss: 0.002419 | Test Loss: 0.004616 | Best Test: 0.004303 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1586\n",
      "Epoch  308 | Train Loss: 0.002386 | Test Loss: 0.004240 | Best Test: 0.004240 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1583\n",
      "Epoch  309 | Train Loss: 0.002004 | Test Loss: 0.004921 | Best Test: 0.004240 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  310 | Train Loss: 0.001986 | Test Loss: 0.004695 | Best Test: 0.004240 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  311 | Train Loss: 0.002160 | Test Loss: 0.004674 | Best Test: 0.004240 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1591\n",
      "Epoch  312 | Train Loss: 0.001975 | Test Loss: 0.004653 | Best Test: 0.004240 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1599\n",
      "Epoch  313 | Train Loss: 0.001951 | Test Loss: 0.004791 | Best Test: 0.004240 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  314 | Train Loss: 0.002280 | Test Loss: 0.004690 | Best Test: 0.004240 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1590\n",
      "Epoch  315 | Train Loss: 0.002152 | Test Loss: 0.004803 | Best Test: 0.004240 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1590\n",
      "Epoch  316 | Train Loss: 0.002087 | Test Loss: 0.005385 | Best Test: 0.004240 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  317 | Train Loss: 0.002426 | Test Loss: 0.004765 | Best Test: 0.004240 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1583\n",
      "Epoch  318 | Train Loss: 0.002082 | Test Loss: 0.004430 | Best Test: 0.004240 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1594\n",
      "Epoch  319 | Train Loss: 0.001907 | Test Loss: 0.004313 | Best Test: 0.004240 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1599\n",
      "Epoch  320 | Train Loss: 0.002251 | Test Loss: 0.004490 | Best Test: 0.004240 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1589\n",
      "Epoch  321 | Train Loss: 0.002104 | Test Loss: 0.004585 | Best Test: 0.004240 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1594\n",
      "Epoch  322 | Train Loss: 0.001876 | Test Loss: 0.004059 | Best Test: 0.004059 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1600\n",
      "Epoch  323 | Train Loss: 0.001919 | Test Loss: 0.004613 | Best Test: 0.004059 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1600\n",
      "Epoch  324 | Train Loss: 0.001947 | Test Loss: 0.004682 | Best Test: 0.004059 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  325 | Train Loss: 0.001869 | Test Loss: 0.006447 | Best Test: 0.004059 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1600\n",
      "Epoch  326 | Train Loss: 0.002374 | Test Loss: 0.004951 | Best Test: 0.004059 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1587\n",
      "Epoch  327 | Train Loss: 0.001859 | Test Loss: 0.004263 | Best Test: 0.004059 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1602\n",
      "Epoch  328 | Train Loss: 0.001954 | Test Loss: 0.004524 | Best Test: 0.004059 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  329 | Train Loss: 0.002045 | Test Loss: 0.004624 | Best Test: 0.004059 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  330 | Train Loss: 0.001908 | Test Loss: 0.004778 | Best Test: 0.004059 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1599\n",
      "Epoch  331 | Train Loss: 0.002086 | Test Loss: 0.005088 | Best Test: 0.004059 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1594\n",
      "Epoch  332 | Train Loss: 0.001933 | Test Loss: 0.004650 | Best Test: 0.004059 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  333 | Train Loss: 0.002102 | Test Loss: 0.005742 | Best Test: 0.004059 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  334 | Train Loss: 0.002016 | Test Loss: 0.004609 | Best Test: 0.004059 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  335 | Train Loss: 0.001926 | Test Loss: 0.004483 | Best Test: 0.004059 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1600\n",
      "Epoch  336 | Train Loss: 0.002213 | Test Loss: 0.004213 | Best Test: 0.004059 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1590\n",
      "Epoch  337 | Train Loss: 0.001747 | Test Loss: 0.004402 | Best Test: 0.004059 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1603\n",
      "Epoch  338 | Train Loss: 0.001738 | Test Loss: 0.004364 | Best Test: 0.004059 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1605\n",
      "Epoch  339 | Train Loss: 0.001706 | Test Loss: 0.004612 | Best Test: 0.004059 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1605\n",
      "Epoch  340 | Train Loss: 0.001795 | Test Loss: 0.004227 | Best Test: 0.004059 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1604\n",
      "Epoch  341 | Train Loss: 0.001713 | Test Loss: 0.004378 | Best Test: 0.004059 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1605\n",
      "Epoch  342 | Train Loss: 0.001778 | Test Loss: 0.004863 | Best Test: 0.004059 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1604\n",
      "Epoch  343 | Train Loss: 0.002162 | Test Loss: 0.004497 | Best Test: 0.004059 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1591\n",
      "Epoch  344 | Train Loss: 0.002048 | Test Loss: 0.004423 | Best Test: 0.004059 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  345 | Train Loss: 0.002026 | Test Loss: 0.006055 | Best Test: 0.004059 | Patience: 23/50 | LR: 1.00e-04 | Pred_std: 0.1596\n",
      "Epoch  346 | Train Loss: 0.001789 | Test Loss: 0.004024 | Best Test: 0.004024 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1604\n",
      "Epoch  347 | Train Loss: 0.001587 | Test Loss: 0.005379 | Best Test: 0.004024 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1608\n",
      "Epoch  348 | Train Loss: 0.001868 | Test Loss: 0.005033 | Best Test: 0.004024 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1601\n",
      "Epoch  349 | Train Loss: 0.001794 | Test Loss: 0.003901 | Best Test: 0.003901 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1602\n",
      "Epoch  350 | Train Loss: 0.001679 | Test Loss: 0.004225 | Best Test: 0.003901 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1608\n",
      "Epoch  351 | Train Loss: 0.001810 | Test Loss: 0.005127 | Best Test: 0.003901 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1602\n",
      "Epoch  352 | Train Loss: 0.001780 | Test Loss: 0.004726 | Best Test: 0.003901 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1605\n",
      "Epoch  353 | Train Loss: 0.001820 | Test Loss: 0.004314 | Best Test: 0.003901 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1601\n",
      "Epoch  354 | Train Loss: 0.001896 | Test Loss: 0.005001 | Best Test: 0.003901 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1600\n",
      "Epoch  355 | Train Loss: 0.001995 | Test Loss: 0.005002 | Best Test: 0.003901 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  356 | Train Loss: 0.001784 | Test Loss: 0.004591 | Best Test: 0.003901 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1604\n",
      "Epoch  357 | Train Loss: 0.001724 | Test Loss: 0.004463 | Best Test: 0.003901 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1604\n",
      "Epoch  358 | Train Loss: 0.002065 | Test Loss: 0.005002 | Best Test: 0.003901 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1596\n",
      "Epoch  359 | Train Loss: 0.001895 | Test Loss: 0.004770 | Best Test: 0.003901 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1599\n",
      "Epoch  360 | Train Loss: 0.001661 | Test Loss: 0.004670 | Best Test: 0.003901 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1607\n",
      "Epoch  361 | Train Loss: 0.001698 | Test Loss: 0.004842 | Best Test: 0.003901 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1606\n",
      "Epoch  362 | Train Loss: 0.001880 | Test Loss: 0.004424 | Best Test: 0.003901 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1601\n",
      "Epoch  363 | Train Loss: 0.001649 | Test Loss: 0.004627 | Best Test: 0.003901 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1607\n",
      "Epoch  364 | Train Loss: 0.001632 | Test Loss: 0.004587 | Best Test: 0.003901 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1609\n",
      "Epoch  365 | Train Loss: 0.002211 | Test Loss: 0.004573 | Best Test: 0.003901 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1592\n",
      "Epoch  366 | Train Loss: 0.001766 | Test Loss: 0.004665 | Best Test: 0.003901 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1602\n",
      "Epoch  367 | Train Loss: 0.001815 | Test Loss: 0.004811 | Best Test: 0.003901 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1602\n",
      "Epoch  368 | Train Loss: 0.001867 | Test Loss: 0.005111 | Best Test: 0.003901 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1600\n",
      "Epoch  369 | Train Loss: 0.001993 | Test Loss: 0.004687 | Best Test: 0.003901 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  370 | Train Loss: 0.001628 | Test Loss: 0.004468 | Best Test: 0.003901 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1609\n",
      "Epoch  371 | Train Loss: 0.001679 | Test Loss: 0.004982 | Best Test: 0.003901 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1607\n",
      "Epoch  372 | Train Loss: 0.001571 | Test Loss: 0.004659 | Best Test: 0.003901 | Patience: 23/50 | LR: 1.00e-04 | Pred_std: 0.1608\n",
      "Epoch  373 | Train Loss: 0.001510 | Test Loss: 0.004330 | Best Test: 0.003901 | Patience: 24/50 | LR: 1.00e-04 | Pred_std: 0.1613\n",
      "Epoch  374 | Train Loss: 0.001757 | Test Loss: 0.004123 | Best Test: 0.003901 | Patience: 25/50 | LR: 1.00e-04 | Pred_std: 0.1604\n",
      "Epoch  375 | Train Loss: 0.001755 | Test Loss: 0.004344 | Best Test: 0.003901 | Patience: 26/50 | LR: 1.00e-04 | Pred_std: 0.1604\n",
      "Epoch  376 | Train Loss: 0.001588 | Test Loss: 0.004325 | Best Test: 0.003901 | Patience: 27/50 | LR: 1.00e-04 | Pred_std: 0.1609\n",
      "Epoch  377 | Train Loss: 0.001604 | Test Loss: 0.004270 | Best Test: 0.003901 | Patience: 28/50 | LR: 1.00e-04 | Pred_std: 0.1609\n",
      "Epoch  378 | Train Loss: 0.001666 | Test Loss: 0.004696 | Best Test: 0.003901 | Patience: 29/50 | LR: 1.00e-04 | Pred_std: 0.1607\n",
      "Epoch  379 | Train Loss: 0.001565 | Test Loss: 0.004756 | Best Test: 0.003901 | Patience: 30/50 | LR: 1.00e-04 | Pred_std: 0.1612\n",
      "Epoch  380 | Train Loss: 0.001828 | Test Loss: 0.004571 | Best Test: 0.003901 | Patience: 31/50 | LR: 1.00e-04 | Pred_std: 0.1601\n",
      "Epoch  381 | Train Loss: 0.001676 | Test Loss: 0.004636 | Best Test: 0.003901 | Patience: 32/50 | LR: 1.00e-04 | Pred_std: 0.1607\n",
      "Epoch  382 | Train Loss: 0.001525 | Test Loss: 0.004694 | Best Test: 0.003901 | Patience: 33/50 | LR: 1.00e-04 | Pred_std: 0.1612\n",
      "Epoch  383 | Train Loss: 0.001718 | Test Loss: 0.004504 | Best Test: 0.003901 | Patience: 34/50 | LR: 1.00e-04 | Pred_std: 0.1605\n",
      "Epoch  384 | Train Loss: 0.001594 | Test Loss: 0.004487 | Best Test: 0.003901 | Patience: 35/50 | LR: 1.00e-04 | Pred_std: 0.1610\n",
      "Epoch  385 | Train Loss: 0.001467 | Test Loss: 0.004443 | Best Test: 0.003901 | Patience: 36/50 | LR: 1.00e-04 | Pred_std: 0.1612\n",
      "Epoch  386 | Train Loss: 0.001497 | Test Loss: 0.004971 | Best Test: 0.003901 | Patience: 37/50 | LR: 1.00e-04 | Pred_std: 0.1612\n",
      "Epoch  387 | Train Loss: 0.001961 | Test Loss: 0.004980 | Best Test: 0.003901 | Patience: 38/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  388 | Train Loss: 0.001890 | Test Loss: 0.004633 | Best Test: 0.003901 | Patience: 39/50 | LR: 1.00e-04 | Pred_std: 0.1601\n",
      "Epoch  389 | Train Loss: 0.001575 | Test Loss: 0.004388 | Best Test: 0.003901 | Patience: 40/50 | LR: 1.00e-04 | Pred_std: 0.1609\n",
      "Epoch  390 | Train Loss: 0.002041 | Test Loss: 0.004379 | Best Test: 0.003901 | Patience: 41/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  391 | Train Loss: 0.001560 | Test Loss: 0.004547 | Best Test: 0.003901 | Patience: 42/50 | LR: 1.00e-04 | Pred_std: 0.1609\n",
      "Epoch  392 | Train Loss: 0.001441 | Test Loss: 0.004373 | Best Test: 0.003901 | Patience: 43/50 | LR: 1.00e-04 | Pred_std: 0.1614\n",
      "Epoch  393 | Train Loss: 0.001257 | Test Loss: 0.004125 | Best Test: 0.003901 | Patience: 44/50 | LR: 1.00e-04 | Pred_std: 0.1620\n",
      "Epoch  394 | Train Loss: 0.001642 | Test Loss: 0.004862 | Best Test: 0.003901 | Patience: 45/50 | LR: 1.00e-04 | Pred_std: 0.1607\n",
      "Epoch  395 | Train Loss: 0.001534 | Test Loss: 0.004716 | Best Test: 0.003901 | Patience: 46/50 | LR: 1.00e-04 | Pred_std: 0.1612\n",
      "Epoch  396 | Train Loss: 0.001451 | Test Loss: 0.004546 | Best Test: 0.003901 | Patience: 47/50 | LR: 1.00e-04 | Pred_std: 0.1612\n",
      "Epoch  397 | Train Loss: 0.001653 | Test Loss: 0.004465 | Best Test: 0.003901 | Patience: 48/50 | LR: 1.00e-04 | Pred_std: 0.1606\n",
      "Epoch  398 | Train Loss: 0.001631 | Test Loss: 0.004482 | Best Test: 0.003901 | Patience: 49/50 | LR: 1.00e-04 | Pred_std: 0.1610\n",
      "  Early stopping at epoch 399 (test_loss=0.004586)\n",
      "\n",
      "Evaluating on test set...\n",
      "Final train loss: 0.001377\n",
      "Test loss: 0.004586\n",
      "Generalization gap: 0.003209\n",
      "\n",
      " Config 'mpnn_layer_2' completed in 9645.1s\n",
      "\n",
      "############################################################\n",
      "# Running config 3/3: mpnn_layer_3\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "TRAINING CONFIG: mpnn_layer_3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Epoch    1 | Train Loss: 0.023055 | Test Loss: 0.016077 | Best Test: 0.016077 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.0772\n",
      "Epoch    2 | Train Loss: 0.013816 | Test Loss: 0.011851 | Best Test: 0.011851 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1206\n",
      "Epoch    3 | Train Loss: 0.012633 | Test Loss: 0.013203 | Best Test: 0.011851 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1237\n",
      "Epoch    4 | Train Loss: 0.012034 | Test Loss: 0.010757 | Best Test: 0.010757 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1261\n",
      "Epoch    5 | Train Loss: 0.011508 | Test Loss: 0.011989 | Best Test: 0.010757 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1279\n",
      "Epoch    6 | Train Loss: 0.011406 | Test Loss: 0.010368 | Best Test: 0.010368 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1285\n",
      "Epoch    7 | Train Loss: 0.011173 | Test Loss: 0.010450 | Best Test: 0.010368 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1291\n",
      "Epoch    8 | Train Loss: 0.010945 | Test Loss: 0.011973 | Best Test: 0.010368 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1298\n",
      "Epoch    9 | Train Loss: 0.010637 | Test Loss: 0.009939 | Best Test: 0.009939 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1311\n",
      "Epoch   10 | Train Loss: 0.010534 | Test Loss: 0.010218 | Best Test: 0.009939 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1314\n",
      "Epoch   11 | Train Loss: 0.010486 | Test Loss: 0.009206 | Best Test: 0.009206 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1319\n",
      "Epoch   12 | Train Loss: 0.010222 | Test Loss: 0.009528 | Best Test: 0.009206 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1327\n",
      "Epoch   13 | Train Loss: 0.010074 | Test Loss: 0.008929 | Best Test: 0.008929 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1332\n",
      "Epoch   14 | Train Loss: 0.009870 | Test Loss: 0.010501 | Best Test: 0.008929 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1341\n",
      "Epoch   15 | Train Loss: 0.009742 | Test Loss: 0.008916 | Best Test: 0.008916 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1340\n",
      "Epoch   16 | Train Loss: 0.009574 | Test Loss: 0.009448 | Best Test: 0.008916 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1352\n",
      "Epoch   17 | Train Loss: 0.009564 | Test Loss: 0.009746 | Best Test: 0.008916 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1352\n",
      "Epoch   18 | Train Loss: 0.009262 | Test Loss: 0.008521 | Best Test: 0.008521 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1361\n",
      "Epoch   19 | Train Loss: 0.009146 | Test Loss: 0.009072 | Best Test: 0.008521 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1363\n",
      "Epoch   20 | Train Loss: 0.009064 | Test Loss: 0.008618 | Best Test: 0.008521 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1367\n",
      "Epoch   21 | Train Loss: 0.009071 | Test Loss: 0.008739 | Best Test: 0.008521 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1368\n",
      "Epoch   22 | Train Loss: 0.009040 | Test Loss: 0.008845 | Best Test: 0.008521 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1368\n",
      "Epoch   23 | Train Loss: 0.008901 | Test Loss: 0.008546 | Best Test: 0.008521 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1372\n",
      "Epoch   24 | Train Loss: 0.008752 | Test Loss: 0.008702 | Best Test: 0.008521 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1378\n",
      "Epoch   25 | Train Loss: 0.008653 | Test Loss: 0.008402 | Best Test: 0.008402 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1379\n",
      "Epoch   26 | Train Loss: 0.008792 | Test Loss: 0.008500 | Best Test: 0.008402 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1375\n",
      "Epoch   27 | Train Loss: 0.008768 | Test Loss: 0.008525 | Best Test: 0.008402 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1379\n",
      "Epoch   28 | Train Loss: 0.008640 | Test Loss: 0.009103 | Best Test: 0.008402 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1381\n",
      "Epoch   29 | Train Loss: 0.008503 | Test Loss: 0.007900 | Best Test: 0.007900 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1387\n",
      "Epoch   30 | Train Loss: 0.008604 | Test Loss: 0.008039 | Best Test: 0.007900 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1382\n",
      "Epoch   31 | Train Loss: 0.008371 | Test Loss: 0.008361 | Best Test: 0.007900 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1387\n",
      "Epoch   32 | Train Loss: 0.008395 | Test Loss: 0.008221 | Best Test: 0.007900 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1393\n",
      "Epoch   33 | Train Loss: 0.008299 | Test Loss: 0.007643 | Best Test: 0.007643 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1391\n",
      "Epoch   34 | Train Loss: 0.008452 | Test Loss: 0.007505 | Best Test: 0.007505 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1386\n",
      "Epoch   35 | Train Loss: 0.008311 | Test Loss: 0.007907 | Best Test: 0.007505 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1392\n",
      "Epoch   36 | Train Loss: 0.008221 | Test Loss: 0.008127 | Best Test: 0.007505 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1394\n",
      "Epoch   37 | Train Loss: 0.008150 | Test Loss: 0.007925 | Best Test: 0.007505 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1397\n",
      "Epoch   38 | Train Loss: 0.008185 | Test Loss: 0.008057 | Best Test: 0.007505 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1395\n",
      "Epoch   39 | Train Loss: 0.007983 | Test Loss: 0.007944 | Best Test: 0.007505 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1399\n",
      "Epoch   40 | Train Loss: 0.008189 | Test Loss: 0.007490 | Best Test: 0.007490 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1396\n",
      "Epoch   41 | Train Loss: 0.007980 | Test Loss: 0.008234 | Best Test: 0.007490 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1400\n",
      "Epoch   42 | Train Loss: 0.008184 | Test Loss: 0.007331 | Best Test: 0.007331 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1394\n",
      "Epoch   43 | Train Loss: 0.007797 | Test Loss: 0.007697 | Best Test: 0.007331 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1408\n",
      "Epoch   44 | Train Loss: 0.007835 | Test Loss: 0.008633 | Best Test: 0.007331 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1408\n",
      "Epoch   45 | Train Loss: 0.008058 | Test Loss: 0.007632 | Best Test: 0.007331 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1399\n",
      "Epoch   46 | Train Loss: 0.007876 | Test Loss: 0.007511 | Best Test: 0.007331 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1406\n",
      "Epoch   47 | Train Loss: 0.007939 | Test Loss: 0.007349 | Best Test: 0.007331 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1403\n",
      "Epoch   48 | Train Loss: 0.007721 | Test Loss: 0.007806 | Best Test: 0.007331 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1412\n",
      "Epoch   49 | Train Loss: 0.007640 | Test Loss: 0.008101 | Best Test: 0.007331 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1411\n",
      "Epoch   50 | Train Loss: 0.007755 | Test Loss: 0.007658 | Best Test: 0.007331 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1408\n",
      "Epoch   51 | Train Loss: 0.007629 | Test Loss: 0.007536 | Best Test: 0.007331 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1415\n",
      "Epoch   52 | Train Loss: 0.007844 | Test Loss: 0.007717 | Best Test: 0.007331 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1405\n",
      "Epoch   53 | Train Loss: 0.007614 | Test Loss: 0.007460 | Best Test: 0.007331 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1414\n",
      "Epoch   54 | Train Loss: 0.007522 | Test Loss: 0.008813 | Best Test: 0.007331 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1416\n",
      "Epoch   55 | Train Loss: 0.007648 | Test Loss: 0.007149 | Best Test: 0.007149 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1412\n",
      "Epoch   56 | Train Loss: 0.007612 | Test Loss: 0.007604 | Best Test: 0.007149 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1415\n",
      "Epoch   57 | Train Loss: 0.007372 | Test Loss: 0.007549 | Best Test: 0.007149 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1422\n",
      "Epoch   58 | Train Loss: 0.007361 | Test Loss: 0.007197 | Best Test: 0.007149 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1417\n",
      "Epoch   59 | Train Loss: 0.007467 | Test Loss: 0.007877 | Best Test: 0.007149 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1421\n",
      "Epoch   60 | Train Loss: 0.007506 | Test Loss: 0.007112 | Best Test: 0.007112 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1416\n",
      "Epoch   61 | Train Loss: 0.007338 | Test Loss: 0.007651 | Best Test: 0.007112 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1423\n",
      "Epoch   62 | Train Loss: 0.007275 | Test Loss: 0.007450 | Best Test: 0.007112 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1425\n",
      "Epoch   63 | Train Loss: 0.007418 | Test Loss: 0.007684 | Best Test: 0.007112 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1422\n",
      "Epoch   64 | Train Loss: 0.007279 | Test Loss: 0.007133 | Best Test: 0.007112 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1420\n",
      "Epoch   65 | Train Loss: 0.007215 | Test Loss: 0.006979 | Best Test: 0.006979 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1427\n",
      "Epoch   66 | Train Loss: 0.007164 | Test Loss: 0.006773 | Best Test: 0.006773 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1428\n",
      "Epoch   67 | Train Loss: 0.006996 | Test Loss: 0.007247 | Best Test: 0.006773 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1432\n",
      "Epoch   68 | Train Loss: 0.007344 | Test Loss: 0.007444 | Best Test: 0.006773 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1424\n",
      "Epoch   69 | Train Loss: 0.007047 | Test Loss: 0.007005 | Best Test: 0.006773 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1431\n",
      "Epoch   70 | Train Loss: 0.007100 | Test Loss: 0.007678 | Best Test: 0.006773 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1430\n",
      "Epoch   71 | Train Loss: 0.007029 | Test Loss: 0.006780 | Best Test: 0.006773 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1435\n",
      "Epoch   72 | Train Loss: 0.006971 | Test Loss: 0.006844 | Best Test: 0.006773 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1432\n",
      "Epoch   73 | Train Loss: 0.006895 | Test Loss: 0.006928 | Best Test: 0.006773 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1438\n",
      "Epoch   74 | Train Loss: 0.006794 | Test Loss: 0.007438 | Best Test: 0.006773 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1440\n",
      "Epoch   75 | Train Loss: 0.006878 | Test Loss: 0.007825 | Best Test: 0.006773 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1437\n",
      "Epoch   76 | Train Loss: 0.006887 | Test Loss: 0.007180 | Best Test: 0.006773 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1437\n",
      "Epoch   77 | Train Loss: 0.006923 | Test Loss: 0.007094 | Best Test: 0.006773 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1437\n",
      "Epoch   78 | Train Loss: 0.006953 | Test Loss: 0.006905 | Best Test: 0.006773 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1432\n",
      "Epoch   79 | Train Loss: 0.006717 | Test Loss: 0.006854 | Best Test: 0.006773 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1444\n",
      "Epoch   80 | Train Loss: 0.006601 | Test Loss: 0.006761 | Best Test: 0.006761 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1448\n",
      "Epoch   81 | Train Loss: 0.006801 | Test Loss: 0.007362 | Best Test: 0.006761 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1443\n",
      "Epoch   82 | Train Loss: 0.006536 | Test Loss: 0.006614 | Best Test: 0.006614 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1447\n",
      "Epoch   83 | Train Loss: 0.006570 | Test Loss: 0.006920 | Best Test: 0.006614 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1447\n",
      "Epoch   84 | Train Loss: 0.006485 | Test Loss: 0.007117 | Best Test: 0.006614 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1452\n",
      "Epoch   85 | Train Loss: 0.006442 | Test Loss: 0.006481 | Best Test: 0.006481 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1451\n",
      "Epoch   86 | Train Loss: 0.006448 | Test Loss: 0.008536 | Best Test: 0.006481 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1454\n",
      "Epoch   87 | Train Loss: 0.006453 | Test Loss: 0.006559 | Best Test: 0.006481 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1450\n",
      "Epoch   88 | Train Loss: 0.006314 | Test Loss: 0.006318 | Best Test: 0.006318 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1455\n",
      "Epoch   89 | Train Loss: 0.006253 | Test Loss: 0.006510 | Best Test: 0.006318 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1458\n",
      "Epoch   90 | Train Loss: 0.006381 | Test Loss: 0.006674 | Best Test: 0.006318 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1455\n",
      "Epoch   91 | Train Loss: 0.006153 | Test Loss: 0.006201 | Best Test: 0.006201 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1465\n",
      "Epoch   92 | Train Loss: 0.006126 | Test Loss: 0.006523 | Best Test: 0.006201 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1460\n",
      "Epoch   93 | Train Loss: 0.006086 | Test Loss: 0.007035 | Best Test: 0.006201 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1468\n",
      "Epoch   94 | Train Loss: 0.006086 | Test Loss: 0.008173 | Best Test: 0.006201 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1463\n",
      "Epoch   95 | Train Loss: 0.006163 | Test Loss: 0.005905 | Best Test: 0.005905 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1461\n",
      "Epoch   96 | Train Loss: 0.005987 | Test Loss: 0.006258 | Best Test: 0.005905 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1466\n",
      "Epoch   97 | Train Loss: 0.005930 | Test Loss: 0.006823 | Best Test: 0.005905 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1470\n",
      "Epoch   98 | Train Loss: 0.006081 | Test Loss: 0.006680 | Best Test: 0.005905 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1463\n",
      "Epoch   99 | Train Loss: 0.005825 | Test Loss: 0.006718 | Best Test: 0.005905 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1473\n",
      "Epoch  100 | Train Loss: 0.005695 | Test Loss: 0.006183 | Best Test: 0.005905 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1478\n",
      "Epoch  101 | Train Loss: 0.005902 | Test Loss: 0.006167 | Best Test: 0.005905 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1470\n",
      "Epoch  102 | Train Loss: 0.005820 | Test Loss: 0.006343 | Best Test: 0.005905 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1473\n",
      "Epoch  103 | Train Loss: 0.005855 | Test Loss: 0.006057 | Best Test: 0.005905 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1473\n",
      "Epoch  104 | Train Loss: 0.005792 | Test Loss: 0.006456 | Best Test: 0.005905 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1474\n",
      "Epoch  105 | Train Loss: 0.005883 | Test Loss: 0.006090 | Best Test: 0.005905 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1472\n",
      "Epoch  106 | Train Loss: 0.005591 | Test Loss: 0.005885 | Best Test: 0.005885 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1478\n",
      "Epoch  107 | Train Loss: 0.005614 | Test Loss: 0.006630 | Best Test: 0.005885 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1481\n",
      "Epoch  108 | Train Loss: 0.005505 | Test Loss: 0.005967 | Best Test: 0.005885 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1485\n",
      "Epoch  109 | Train Loss: 0.005438 | Test Loss: 0.005888 | Best Test: 0.005885 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1486\n",
      "Epoch  110 | Train Loss: 0.005628 | Test Loss: 0.005962 | Best Test: 0.005885 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1480\n",
      "Epoch  111 | Train Loss: 0.005586 | Test Loss: 0.006169 | Best Test: 0.005885 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1480\n",
      "Epoch  112 | Train Loss: 0.005293 | Test Loss: 0.005821 | Best Test: 0.005821 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1490\n",
      "Epoch  113 | Train Loss: 0.005368 | Test Loss: 0.006464 | Best Test: 0.005821 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1489\n",
      "Epoch  114 | Train Loss: 0.005447 | Test Loss: 0.005819 | Best Test: 0.005821 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1486\n",
      "Epoch  115 | Train Loss: 0.005054 | Test Loss: 0.005759 | Best Test: 0.005759 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1501\n",
      "Epoch  116 | Train Loss: 0.005450 | Test Loss: 0.006466 | Best Test: 0.005759 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1483\n",
      "Epoch  117 | Train Loss: 0.005598 | Test Loss: 0.006154 | Best Test: 0.005759 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1482\n",
      "Epoch  118 | Train Loss: 0.005177 | Test Loss: 0.006631 | Best Test: 0.005759 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1495\n",
      "Epoch  119 | Train Loss: 0.005054 | Test Loss: 0.005748 | Best Test: 0.005748 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1497\n",
      "Epoch  120 | Train Loss: 0.005028 | Test Loss: 0.006112 | Best Test: 0.005748 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1497\n",
      "Epoch  121 | Train Loss: 0.005216 | Test Loss: 0.006723 | Best Test: 0.005748 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1497\n",
      "Epoch  122 | Train Loss: 0.005262 | Test Loss: 0.006212 | Best Test: 0.005748 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1490\n",
      "Epoch  123 | Train Loss: 0.005039 | Test Loss: 0.007253 | Best Test: 0.005748 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1503\n",
      "Epoch  124 | Train Loss: 0.005119 | Test Loss: 0.005315 | Best Test: 0.005315 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1496\n",
      "Epoch  125 | Train Loss: 0.004998 | Test Loss: 0.005721 | Best Test: 0.005315 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1497\n",
      "Epoch  126 | Train Loss: 0.005112 | Test Loss: 0.006048 | Best Test: 0.005315 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1496\n",
      "Epoch  127 | Train Loss: 0.004851 | Test Loss: 0.006235 | Best Test: 0.005315 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1507\n",
      "Epoch  128 | Train Loss: 0.004962 | Test Loss: 0.005600 | Best Test: 0.005315 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1499\n",
      "Epoch  129 | Train Loss: 0.004802 | Test Loss: 0.005959 | Best Test: 0.005315 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1508\n",
      "Epoch  130 | Train Loss: 0.004591 | Test Loss: 0.005335 | Best Test: 0.005315 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1513\n",
      "Epoch  131 | Train Loss: 0.004632 | Test Loss: 0.006309 | Best Test: 0.005315 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1513\n",
      "Epoch  132 | Train Loss: 0.004962 | Test Loss: 0.006023 | Best Test: 0.005315 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1503\n",
      "Epoch  133 | Train Loss: 0.004977 | Test Loss: 0.005883 | Best Test: 0.005315 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1502\n",
      "Epoch  134 | Train Loss: 0.004823 | Test Loss: 0.005878 | Best Test: 0.005315 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1507\n",
      "Epoch  135 | Train Loss: 0.004687 | Test Loss: 0.005462 | Best Test: 0.005315 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1512\n",
      "Epoch  136 | Train Loss: 0.004590 | Test Loss: 0.005597 | Best Test: 0.005315 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1513\n",
      "Epoch  137 | Train Loss: 0.004436 | Test Loss: 0.006050 | Best Test: 0.005315 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1517\n",
      "Epoch  138 | Train Loss: 0.004452 | Test Loss: 0.005213 | Best Test: 0.005213 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1519\n",
      "Epoch  139 | Train Loss: 0.004871 | Test Loss: 0.005391 | Best Test: 0.005213 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1506\n",
      "Epoch  140 | Train Loss: 0.004692 | Test Loss: 0.005326 | Best Test: 0.005213 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1509\n",
      "Epoch  141 | Train Loss: 0.004593 | Test Loss: 0.005176 | Best Test: 0.005176 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1515\n",
      "Epoch  142 | Train Loss: 0.004236 | Test Loss: 0.005373 | Best Test: 0.005176 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1526\n",
      "Epoch  143 | Train Loss: 0.004592 | Test Loss: 0.005616 | Best Test: 0.005176 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1513\n",
      "Epoch  144 | Train Loss: 0.004883 | Test Loss: 0.005584 | Best Test: 0.005176 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1501\n",
      "Epoch  145 | Train Loss: 0.004403 | Test Loss: 0.005386 | Best Test: 0.005176 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1522\n",
      "Epoch  146 | Train Loss: 0.004324 | Test Loss: 0.005969 | Best Test: 0.005176 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1523\n",
      "Epoch  147 | Train Loss: 0.004365 | Test Loss: 0.005227 | Best Test: 0.005176 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1520\n",
      "Epoch  148 | Train Loss: 0.004122 | Test Loss: 0.006172 | Best Test: 0.005176 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1529\n",
      "Epoch  149 | Train Loss: 0.004596 | Test Loss: 0.005721 | Best Test: 0.005176 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1512\n",
      "Epoch  150 | Train Loss: 0.004229 | Test Loss: 0.006144 | Best Test: 0.005176 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1529\n",
      "Epoch  151 | Train Loss: 0.004249 | Test Loss: 0.005193 | Best Test: 0.005176 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1525\n",
      "Epoch  152 | Train Loss: 0.004274 | Test Loss: 0.005849 | Best Test: 0.005176 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1524\n",
      "Epoch  153 | Train Loss: 0.004065 | Test Loss: 0.005282 | Best Test: 0.005176 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1533\n",
      "Epoch  154 | Train Loss: 0.004158 | Test Loss: 0.005115 | Best Test: 0.005115 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1526\n",
      "Epoch  155 | Train Loss: 0.004239 | Test Loss: 0.005380 | Best Test: 0.005115 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1525\n",
      "Epoch  156 | Train Loss: 0.004217 | Test Loss: 0.005091 | Best Test: 0.005091 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1527\n",
      "Epoch  157 | Train Loss: 0.004015 | Test Loss: 0.005621 | Best Test: 0.005091 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1532\n",
      "Epoch  158 | Train Loss: 0.004172 | Test Loss: 0.005229 | Best Test: 0.005091 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1529\n",
      "Epoch  159 | Train Loss: 0.003737 | Test Loss: 0.004888 | Best Test: 0.004888 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1542\n",
      "Epoch  160 | Train Loss: 0.003767 | Test Loss: 0.004917 | Best Test: 0.004888 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1541\n",
      "Epoch  161 | Train Loss: 0.003876 | Test Loss: 0.005141 | Best Test: 0.004888 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1536\n",
      "Epoch  162 | Train Loss: 0.004023 | Test Loss: 0.004961 | Best Test: 0.004888 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1534\n",
      "Epoch  163 | Train Loss: 0.003843 | Test Loss: 0.004945 | Best Test: 0.004888 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1537\n",
      "Epoch  164 | Train Loss: 0.003893 | Test Loss: 0.004970 | Best Test: 0.004888 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1539\n",
      "Epoch  165 | Train Loss: 0.003905 | Test Loss: 0.005074 | Best Test: 0.004888 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1535\n",
      "Epoch  166 | Train Loss: 0.003730 | Test Loss: 0.005133 | Best Test: 0.004888 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1540\n",
      "Epoch  167 | Train Loss: 0.003650 | Test Loss: 0.004940 | Best Test: 0.004888 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1544\n",
      "Epoch  168 | Train Loss: 0.003874 | Test Loss: 0.004799 | Best Test: 0.004799 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1538\n",
      "Epoch  169 | Train Loss: 0.003774 | Test Loss: 0.005132 | Best Test: 0.004799 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1541\n",
      "Epoch  170 | Train Loss: 0.003662 | Test Loss: 0.005422 | Best Test: 0.004799 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1545\n",
      "Epoch  171 | Train Loss: 0.003462 | Test Loss: 0.005035 | Best Test: 0.004799 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1550\n",
      "Epoch  172 | Train Loss: 0.003587 | Test Loss: 0.004881 | Best Test: 0.004799 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1547\n",
      "Epoch  173 | Train Loss: 0.003768 | Test Loss: 0.004771 | Best Test: 0.004771 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1539\n",
      "Epoch  174 | Train Loss: 0.003864 | Test Loss: 0.005152 | Best Test: 0.004771 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1538\n",
      "Epoch  175 | Train Loss: 0.003750 | Test Loss: 0.004824 | Best Test: 0.004771 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1541\n",
      "Epoch  176 | Train Loss: 0.003596 | Test Loss: 0.004969 | Best Test: 0.004771 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1545\n",
      "Epoch  177 | Train Loss: 0.003457 | Test Loss: 0.004938 | Best Test: 0.004771 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  178 | Train Loss: 0.003306 | Test Loss: 0.004857 | Best Test: 0.004771 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1556\n",
      "Epoch  179 | Train Loss: 0.003593 | Test Loss: 0.004843 | Best Test: 0.004771 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1544\n",
      "Epoch  180 | Train Loss: 0.003624 | Test Loss: 0.005235 | Best Test: 0.004771 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1546\n",
      "Epoch  181 | Train Loss: 0.003709 | Test Loss: 0.005292 | Best Test: 0.004771 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1544\n",
      "Epoch  182 | Train Loss: 0.003295 | Test Loss: 0.004753 | Best Test: 0.004753 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  183 | Train Loss: 0.003369 | Test Loss: 0.004860 | Best Test: 0.004753 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  184 | Train Loss: 0.003638 | Test Loss: 0.004754 | Best Test: 0.004753 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1547\n",
      "Epoch  185 | Train Loss: 0.003625 | Test Loss: 0.005088 | Best Test: 0.004753 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1548\n",
      "Epoch  186 | Train Loss: 0.003545 | Test Loss: 0.005501 | Best Test: 0.004753 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1547\n",
      "Epoch  187 | Train Loss: 0.003664 | Test Loss: 0.004802 | Best Test: 0.004753 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1542\n",
      "Epoch  188 | Train Loss: 0.003294 | Test Loss: 0.004695 | Best Test: 0.004695 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1558\n",
      "Epoch  189 | Train Loss: 0.003181 | Test Loss: 0.004547 | Best Test: 0.004547 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1559\n",
      "Epoch  190 | Train Loss: 0.003330 | Test Loss: 0.004660 | Best Test: 0.004547 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1554\n",
      "Epoch  191 | Train Loss: 0.003293 | Test Loss: 0.005088 | Best Test: 0.004547 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1558\n",
      "Epoch  192 | Train Loss: 0.003016 | Test Loss: 0.004581 | Best Test: 0.004547 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1565\n",
      "Epoch  193 | Train Loss: 0.003003 | Test Loss: 0.005123 | Best Test: 0.004547 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1566\n",
      "Epoch  194 | Train Loss: 0.003296 | Test Loss: 0.004507 | Best Test: 0.004507 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1556\n",
      "Epoch  195 | Train Loss: 0.003322 | Test Loss: 0.004619 | Best Test: 0.004507 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1556\n",
      "Epoch  196 | Train Loss: 0.003142 | Test Loss: 0.004662 | Best Test: 0.004507 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1560\n",
      "Epoch  197 | Train Loss: 0.003052 | Test Loss: 0.005132 | Best Test: 0.004507 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1564\n",
      "Epoch  198 | Train Loss: 0.003225 | Test Loss: 0.005004 | Best Test: 0.004507 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1560\n",
      "Epoch  199 | Train Loss: 0.003406 | Test Loss: 0.004278 | Best Test: 0.004278 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1551\n",
      "Epoch  200 | Train Loss: 0.003498 | Test Loss: 0.005561 | Best Test: 0.004278 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1553\n",
      "Epoch  201 | Train Loss: 0.003134 | Test Loss: 0.004792 | Best Test: 0.004278 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1558\n",
      "Epoch  202 | Train Loss: 0.002858 | Test Loss: 0.004779 | Best Test: 0.004278 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1569\n",
      "Epoch  203 | Train Loss: 0.002770 | Test Loss: 0.004438 | Best Test: 0.004278 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1572\n",
      "Epoch  204 | Train Loss: 0.003038 | Test Loss: 0.005111 | Best Test: 0.004278 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1565\n",
      "Epoch  205 | Train Loss: 0.002922 | Test Loss: 0.004595 | Best Test: 0.004278 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1567\n",
      "Epoch  206 | Train Loss: 0.002926 | Test Loss: 0.004278 | Best Test: 0.004278 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1568\n",
      "Epoch  207 | Train Loss: 0.003286 | Test Loss: 0.004706 | Best Test: 0.004278 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1556\n",
      "Epoch  208 | Train Loss: 0.003082 | Test Loss: 0.004842 | Best Test: 0.004278 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1562\n",
      "Epoch  209 | Train Loss: 0.003498 | Test Loss: 0.004966 | Best Test: 0.004278 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1550\n",
      "Epoch  210 | Train Loss: 0.002990 | Test Loss: 0.004919 | Best Test: 0.004278 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1566\n",
      "Epoch  211 | Train Loss: 0.002801 | Test Loss: 0.004496 | Best Test: 0.004278 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1571\n",
      "Epoch  212 | Train Loss: 0.002911 | Test Loss: 0.004428 | Best Test: 0.004278 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1568\n",
      "Epoch  213 | Train Loss: 0.002781 | Test Loss: 0.004781 | Best Test: 0.004278 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1573\n",
      "Epoch  214 | Train Loss: 0.002868 | Test Loss: 0.004769 | Best Test: 0.004278 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1571\n",
      "Epoch  215 | Train Loss: 0.002959 | Test Loss: 0.004702 | Best Test: 0.004278 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1565\n",
      "Epoch  216 | Train Loss: 0.002807 | Test Loss: 0.004650 | Best Test: 0.004278 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1572\n",
      "Epoch  217 | Train Loss: 0.002851 | Test Loss: 0.004623 | Best Test: 0.004278 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1570\n",
      "Epoch  218 | Train Loss: 0.002635 | Test Loss: 0.004860 | Best Test: 0.004278 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1577\n",
      "Epoch  219 | Train Loss: 0.002802 | Test Loss: 0.004392 | Best Test: 0.004278 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1571\n",
      "Epoch  220 | Train Loss: 0.003070 | Test Loss: 0.005130 | Best Test: 0.004278 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1564\n",
      "Epoch  221 | Train Loss: 0.002727 | Test Loss: 0.004280 | Best Test: 0.004278 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1572\n",
      "Epoch  222 | Train Loss: 0.002443 | Test Loss: 0.004204 | Best Test: 0.004204 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1582\n",
      "Epoch  223 | Train Loss: 0.002876 | Test Loss: 0.004757 | Best Test: 0.004204 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1570\n",
      "Epoch  224 | Train Loss: 0.002757 | Test Loss: 0.004623 | Best Test: 0.004204 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1572\n",
      "Epoch  225 | Train Loss: 0.002563 | Test Loss: 0.004187 | Best Test: 0.004187 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  226 | Train Loss: 0.002592 | Test Loss: 0.004782 | Best Test: 0.004187 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1578\n",
      "Epoch  227 | Train Loss: 0.002786 | Test Loss: 0.004964 | Best Test: 0.004187 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1570\n",
      "Epoch  228 | Train Loss: 0.002927 | Test Loss: 0.004754 | Best Test: 0.004187 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1570\n",
      "Epoch  229 | Train Loss: 0.002734 | Test Loss: 0.005526 | Best Test: 0.004187 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1574\n",
      "Epoch  230 | Train Loss: 0.002941 | Test Loss: 0.004557 | Best Test: 0.004187 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1568\n",
      "Epoch  231 | Train Loss: 0.002407 | Test Loss: 0.004049 | Best Test: 0.004049 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1583\n",
      "Epoch  232 | Train Loss: 0.002659 | Test Loss: 0.004659 | Best Test: 0.004049 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1579\n",
      "Epoch  233 | Train Loss: 0.002620 | Test Loss: 0.004391 | Best Test: 0.004049 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1573\n",
      "Epoch  234 | Train Loss: 0.002463 | Test Loss: 0.004065 | Best Test: 0.004049 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  235 | Train Loss: 0.002472 | Test Loss: 0.004437 | Best Test: 0.004049 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  236 | Train Loss: 0.002404 | Test Loss: 0.005229 | Best Test: 0.004049 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  237 | Train Loss: 0.002689 | Test Loss: 0.005339 | Best Test: 0.004049 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1576\n",
      "Epoch  238 | Train Loss: 0.002518 | Test Loss: 0.004782 | Best Test: 0.004049 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1579\n",
      "Epoch  239 | Train Loss: 0.002832 | Test Loss: 0.004725 | Best Test: 0.004049 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1571\n",
      "Epoch  240 | Train Loss: 0.002459 | Test Loss: 0.004276 | Best Test: 0.004049 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  241 | Train Loss: 0.002384 | Test Loss: 0.004626 | Best Test: 0.004049 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  242 | Train Loss: 0.002273 | Test Loss: 0.004535 | Best Test: 0.004049 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1589\n",
      "Epoch  243 | Train Loss: 0.002528 | Test Loss: 0.004363 | Best Test: 0.004049 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  244 | Train Loss: 0.002318 | Test Loss: 0.004415 | Best Test: 0.004049 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1587\n",
      "Epoch  245 | Train Loss: 0.002418 | Test Loss: 0.004244 | Best Test: 0.004049 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  246 | Train Loss: 0.002301 | Test Loss: 0.004382 | Best Test: 0.004049 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1587\n",
      "Epoch  247 | Train Loss: 0.002412 | Test Loss: 0.004284 | Best Test: 0.004049 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1584\n",
      "Epoch  248 | Train Loss: 0.002525 | Test Loss: 0.004320 | Best Test: 0.004049 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1578\n",
      "Epoch  249 | Train Loss: 0.002319 | Test Loss: 0.004757 | Best Test: 0.004049 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  250 | Train Loss: 0.002677 | Test Loss: 0.005004 | Best Test: 0.004049 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1575\n",
      "Epoch  251 | Train Loss: 0.002561 | Test Loss: 0.004459 | Best Test: 0.004049 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  252 | Train Loss: 0.002315 | Test Loss: 0.004699 | Best Test: 0.004049 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  253 | Train Loss: 0.002550 | Test Loss: 0.004311 | Best Test: 0.004049 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  254 | Train Loss: 0.002400 | Test Loss: 0.004282 | Best Test: 0.004049 | Patience: 23/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  255 | Train Loss: 0.002270 | Test Loss: 0.004252 | Best Test: 0.004049 | Patience: 24/50 | LR: 1.00e-04 | Pred_std: 0.1590\n",
      "Epoch  256 | Train Loss: 0.002514 | Test Loss: 0.004560 | Best Test: 0.004049 | Patience: 25/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  257 | Train Loss: 0.002104 | Test Loss: 0.004198 | Best Test: 0.004049 | Patience: 26/50 | LR: 1.00e-04 | Pred_std: 0.1593\n",
      "Epoch  258 | Train Loss: 0.002387 | Test Loss: 0.004231 | Best Test: 0.004049 | Patience: 27/50 | LR: 1.00e-04 | Pred_std: 0.1585\n",
      "Epoch  259 | Train Loss: 0.002261 | Test Loss: 0.004695 | Best Test: 0.004049 | Patience: 28/50 | LR: 1.00e-04 | Pred_std: 0.1589\n",
      "Epoch  260 | Train Loss: 0.002173 | Test Loss: 0.004094 | Best Test: 0.004049 | Patience: 29/50 | LR: 1.00e-04 | Pred_std: 0.1592\n",
      "Epoch  261 | Train Loss: 0.002003 | Test Loss: 0.004496 | Best Test: 0.004049 | Patience: 30/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  262 | Train Loss: 0.002243 | Test Loss: 0.004624 | Best Test: 0.004049 | Patience: 31/50 | LR: 1.00e-04 | Pred_std: 0.1589\n",
      "Epoch  263 | Train Loss: 0.002330 | Test Loss: 0.004498 | Best Test: 0.004049 | Patience: 32/50 | LR: 1.00e-04 | Pred_std: 0.1584\n",
      "Epoch  264 | Train Loss: 0.002148 | Test Loss: 0.004202 | Best Test: 0.004049 | Patience: 33/50 | LR: 1.00e-04 | Pred_std: 0.1593\n",
      "Epoch  265 | Train Loss: 0.002168 | Test Loss: 0.004124 | Best Test: 0.004049 | Patience: 34/50 | LR: 1.00e-04 | Pred_std: 0.1593\n",
      "Epoch  266 | Train Loss: 0.002032 | Test Loss: 0.004880 | Best Test: 0.004049 | Patience: 35/50 | LR: 1.00e-04 | Pred_std: 0.1596\n",
      "Epoch  267 | Train Loss: 0.001990 | Test Loss: 0.004813 | Best Test: 0.004049 | Patience: 36/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  268 | Train Loss: 0.002496 | Test Loss: 0.004882 | Best Test: 0.004049 | Patience: 37/50 | LR: 1.00e-04 | Pred_std: 0.1581\n",
      "Epoch  269 | Train Loss: 0.002458 | Test Loss: 0.005502 | Best Test: 0.004049 | Patience: 38/50 | LR: 1.00e-04 | Pred_std: 0.1583\n",
      "Epoch  270 | Train Loss: 0.002391 | Test Loss: 0.003996 | Best Test: 0.003996 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1584\n",
      "Epoch  271 | Train Loss: 0.002244 | Test Loss: 0.004167 | Best Test: 0.003996 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1588\n",
      "Epoch  272 | Train Loss: 0.002172 | Test Loss: 0.004166 | Best Test: 0.003996 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1592\n",
      "Epoch  273 | Train Loss: 0.001921 | Test Loss: 0.004007 | Best Test: 0.003996 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  274 | Train Loss: 0.002144 | Test Loss: 0.004522 | Best Test: 0.003996 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1593\n",
      "Epoch  275 | Train Loss: 0.002081 | Test Loss: 0.004715 | Best Test: 0.003996 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1594\n",
      "Epoch  276 | Train Loss: 0.002514 | Test Loss: 0.004311 | Best Test: 0.003996 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1580\n",
      "Epoch  277 | Train Loss: 0.002168 | Test Loss: 0.004170 | Best Test: 0.003996 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1591\n",
      "Epoch  278 | Train Loss: 0.001864 | Test Loss: 0.004280 | Best Test: 0.003996 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1601\n",
      "Epoch  279 | Train Loss: 0.001926 | Test Loss: 0.003704 | Best Test: 0.003704 | Patience: 0/50 | LR: 1.00e-04 | Pred_std: 0.1599\n",
      "Epoch  280 | Train Loss: 0.002046 | Test Loss: 0.004528 | Best Test: 0.003704 | Patience: 1/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  281 | Train Loss: 0.001812 | Test Loss: 0.004225 | Best Test: 0.003704 | Patience: 2/50 | LR: 1.00e-04 | Pred_std: 0.1603\n",
      "Epoch  282 | Train Loss: 0.002083 | Test Loss: 0.004450 | Best Test: 0.003704 | Patience: 3/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  283 | Train Loss: 0.002293 | Test Loss: 0.004041 | Best Test: 0.003704 | Patience: 4/50 | LR: 1.00e-04 | Pred_std: 0.1586\n",
      "Epoch  284 | Train Loss: 0.001884 | Test Loss: 0.004215 | Best Test: 0.003704 | Patience: 5/50 | LR: 1.00e-04 | Pred_std: 0.1601\n",
      "Epoch  285 | Train Loss: 0.001895 | Test Loss: 0.004256 | Best Test: 0.003704 | Patience: 6/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  286 | Train Loss: 0.001952 | Test Loss: 0.004276 | Best Test: 0.003704 | Patience: 7/50 | LR: 1.00e-04 | Pred_std: 0.1600\n",
      "Epoch  287 | Train Loss: 0.002493 | Test Loss: 0.004865 | Best Test: 0.003704 | Patience: 8/50 | LR: 1.00e-04 | Pred_std: 0.1579\n",
      "Epoch  288 | Train Loss: 0.002185 | Test Loss: 0.004539 | Best Test: 0.003704 | Patience: 9/50 | LR: 1.00e-04 | Pred_std: 0.1592\n",
      "Epoch  289 | Train Loss: 0.002097 | Test Loss: 0.003887 | Best Test: 0.003704 | Patience: 10/50 | LR: 1.00e-04 | Pred_std: 0.1593\n",
      "Epoch  290 | Train Loss: 0.001978 | Test Loss: 0.004122 | Best Test: 0.003704 | Patience: 11/50 | LR: 1.00e-04 | Pred_std: 0.1599\n",
      "Epoch  291 | Train Loss: 0.001960 | Test Loss: 0.003882 | Best Test: 0.003704 | Patience: 12/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  292 | Train Loss: 0.002036 | Test Loss: 0.004830 | Best Test: 0.003704 | Patience: 13/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  293 | Train Loss: 0.002101 | Test Loss: 0.004168 | Best Test: 0.003704 | Patience: 14/50 | LR: 1.00e-04 | Pred_std: 0.1596\n",
      "Epoch  294 | Train Loss: 0.001985 | Test Loss: 0.003966 | Best Test: 0.003704 | Patience: 15/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  295 | Train Loss: 0.002066 | Test Loss: 0.003897 | Best Test: 0.003704 | Patience: 16/50 | LR: 1.00e-04 | Pred_std: 0.1594\n",
      "Epoch  296 | Train Loss: 0.001795 | Test Loss: 0.003895 | Best Test: 0.003704 | Patience: 17/50 | LR: 1.00e-04 | Pred_std: 0.1603\n",
      "Epoch  297 | Train Loss: 0.001763 | Test Loss: 0.004063 | Best Test: 0.003704 | Patience: 18/50 | LR: 1.00e-04 | Pred_std: 0.1604\n",
      "Epoch  298 | Train Loss: 0.002035 | Test Loss: 0.004025 | Best Test: 0.003704 | Patience: 19/50 | LR: 1.00e-04 | Pred_std: 0.1596\n",
      "Epoch  299 | Train Loss: 0.001702 | Test Loss: 0.003829 | Best Test: 0.003704 | Patience: 20/50 | LR: 1.00e-04 | Pred_std: 0.1605\n",
      "Epoch  300 | Train Loss: 0.001760 | Test Loss: 0.004045 | Best Test: 0.003704 | Patience: 21/50 | LR: 1.00e-04 | Pred_std: 0.1605\n",
      "Epoch  301 | Train Loss: 0.001985 | Test Loss: 0.004496 | Best Test: 0.003704 | Patience: 22/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  302 | Train Loss: 0.002000 | Test Loss: 0.004168 | Best Test: 0.003704 | Patience: 23/50 | LR: 1.00e-04 | Pred_std: 0.1597\n",
      "Epoch  303 | Train Loss: 0.001822 | Test Loss: 0.004138 | Best Test: 0.003704 | Patience: 24/50 | LR: 1.00e-04 | Pred_std: 0.1602\n",
      "Epoch  304 | Train Loss: 0.001792 | Test Loss: 0.004281 | Best Test: 0.003704 | Patience: 25/50 | LR: 1.00e-04 | Pred_std: 0.1603\n",
      "Epoch  305 | Train Loss: 0.002271 | Test Loss: 0.004805 | Best Test: 0.003704 | Patience: 26/50 | LR: 1.00e-04 | Pred_std: 0.1589\n",
      "Epoch  306 | Train Loss: 0.001915 | Test Loss: 0.004337 | Best Test: 0.003704 | Patience: 27/50 | LR: 1.00e-04 | Pred_std: 0.1598\n",
      "Epoch  307 | Train Loss: 0.001833 | Test Loss: 0.004395 | Best Test: 0.003704 | Patience: 28/50 | LR: 1.00e-04 | Pred_std: 0.1602\n",
      "Epoch  308 | Train Loss: 0.002306 | Test Loss: 0.004569 | Best Test: 0.003704 | Patience: 29/50 | LR: 1.00e-04 | Pred_std: 0.1587\n",
      "Epoch  309 | Train Loss: 0.001717 | Test Loss: 0.004044 | Best Test: 0.003704 | Patience: 30/50 | LR: 1.00e-04 | Pred_std: 0.1606\n",
      "Epoch  310 | Train Loss: 0.002038 | Test Loss: 0.003996 | Best Test: 0.003704 | Patience: 31/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  311 | Train Loss: 0.001800 | Test Loss: 0.003718 | Best Test: 0.003704 | Patience: 32/50 | LR: 1.00e-04 | Pred_std: 0.1604\n",
      "Epoch  312 | Train Loss: 0.001610 | Test Loss: 0.004200 | Best Test: 0.003704 | Patience: 33/50 | LR: 1.00e-04 | Pred_std: 0.1609\n",
      "Epoch  313 | Train Loss: 0.001773 | Test Loss: 0.004822 | Best Test: 0.003704 | Patience: 34/50 | LR: 1.00e-04 | Pred_std: 0.1602\n",
      "Epoch  314 | Train Loss: 0.002133 | Test Loss: 0.004611 | Best Test: 0.003704 | Patience: 35/50 | LR: 1.00e-04 | Pred_std: 0.1594\n",
      "Epoch  315 | Train Loss: 0.001828 | Test Loss: 0.004130 | Best Test: 0.003704 | Patience: 36/50 | LR: 1.00e-04 | Pred_std: 0.1601\n",
      "Epoch  316 | Train Loss: 0.001888 | Test Loss: 0.005108 | Best Test: 0.003704 | Patience: 37/50 | LR: 1.00e-04 | Pred_std: 0.1601\n",
      "Epoch  317 | Train Loss: 0.001750 | Test Loss: 0.003959 | Best Test: 0.003704 | Patience: 38/50 | LR: 1.00e-04 | Pred_std: 0.1603\n",
      "Epoch  318 | Train Loss: 0.001715 | Test Loss: 0.004481 | Best Test: 0.003704 | Patience: 39/50 | LR: 1.00e-04 | Pred_std: 0.1606\n",
      "Epoch  319 | Train Loss: 0.001588 | Test Loss: 0.004290 | Best Test: 0.003704 | Patience: 40/50 | LR: 1.00e-04 | Pred_std: 0.1610\n",
      "Epoch  320 | Train Loss: 0.001844 | Test Loss: 0.004484 | Best Test: 0.003704 | Patience: 41/50 | LR: 1.00e-04 | Pred_std: 0.1602\n",
      "Epoch  321 | Train Loss: 0.001819 | Test Loss: 0.003743 | Best Test: 0.003704 | Patience: 42/50 | LR: 1.00e-04 | Pred_std: 0.1603\n",
      "Epoch  322 | Train Loss: 0.001549 | Test Loss: 0.003743 | Best Test: 0.003704 | Patience: 43/50 | LR: 1.00e-04 | Pred_std: 0.1610\n",
      "Epoch  323 | Train Loss: 0.001615 | Test Loss: 0.003815 | Best Test: 0.003704 | Patience: 44/50 | LR: 1.00e-04 | Pred_std: 0.1608\n",
      "Epoch  324 | Train Loss: 0.001532 | Test Loss: 0.003951 | Best Test: 0.003704 | Patience: 45/50 | LR: 1.00e-04 | Pred_std: 0.1611\n",
      "Epoch  325 | Train Loss: 0.002037 | Test Loss: 0.003990 | Best Test: 0.003704 | Patience: 46/50 | LR: 1.00e-04 | Pred_std: 0.1595\n",
      "Epoch  326 | Train Loss: 0.002078 | Test Loss: 0.003942 | Best Test: 0.003704 | Patience: 47/50 | LR: 1.00e-04 | Pred_std: 0.1596\n",
      "Epoch  327 | Train Loss: 0.001735 | Test Loss: 0.004079 | Best Test: 0.003704 | Patience: 48/50 | LR: 1.00e-04 | Pred_std: 0.1605\n",
      "Epoch  328 | Train Loss: 0.001505 | Test Loss: 0.004197 | Best Test: 0.003704 | Patience: 49/50 | LR: 1.00e-04 | Pred_std: 0.1610\n",
      "  Early stopping at epoch 329 (test_loss=0.004230)\n",
      "\n",
      "Evaluating on test set...\n",
      "Final train loss: 0.001688\n",
      "Test loss: 0.004230\n",
      "Generalization gap: 0.002542\n",
      "\n",
      " Config 'mpnn_layer_3' completed in 8153.6s\n",
      "\n",
      "============================================================\n",
      "ALL CONFIGS COMPLETED\n",
      "Total time: 438.5 minutes\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- seed, device and data path setup ---\n",
    "SEED = 0\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "h5_path = \"../../simulated_data/b_train_10k.h5\"\n",
    "test_path = \"../../simulated_data/b_test_1k.h5\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Total configs to run: {len(configs)}\\n\")\n",
    "\n",
    "# --- Run all configs ---\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, config in enumerate(configs):\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"# Running config {idx+1}/{len(configs)}: {config['name']}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    config_start = time.time()\n",
    "    \n",
    "    # --- Train and evaluate the model for this config ---\n",
    "    try:\n",
    "        results = train_single_config(\n",
    "            config=config,\n",
    "            device=device,\n",
    "            h5_path=h5_path,\n",
    "            test_path=test_path,\n",
    "            seed=SEED\n",
    "        )\n",
    "        \n",
    "        # --- Add timestamp and runtime to results ---\n",
    "        results['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        results['runtime_seconds'] = time.time() - config_start\n",
    "        \n",
    "        all_results.append(results)\n",
    "        \n",
    "        print(f\"\\n Config '{config['name']}' completed in {results['runtime_seconds']:.1f}s\")\n",
    "        \n",
    "    # --- Still record the failed configs ---\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Config '{config['name']}' FAILED with error:\")\n",
    "        print(f\"  {type(e).__name__}: {e}\")\n",
    "        \n",
    "        all_results.append({\n",
    "            'config_name': config['name'],\n",
    "            'status': 'FAILED',\n",
    "            'error': str(e),\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'config': config.copy()\n",
    "        })\n",
    "        \n",
    "        # --- Continue with next config instead of crashing ---\n",
    "        continue\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL CONFIGS COMPLETED\")\n",
    "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cd569",
   "metadata": {},
   "source": [
    "<h2> Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18292354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "      Config  Train Loss  Test Loss  Gen Gap  Epochs  Runtime (min)  Status\n",
      "mpnn_layer_3    0.001688   0.004230 0.002542   329.0     135.893819 SUCCESS\n",
      "mpnn_layer_2    0.001377   0.004586 0.003209   399.0     160.752163 SUCCESS\n",
      "mpnn_layer_1    0.002130   0.005467 0.003338   368.0     141.892487 SUCCESS\n",
      "mpnn_layer_1         NaN        NaN      NaN     NaN            NaN  FAILED\n",
      "mpnn_layer_2         NaN        NaN      NaN     NaN            NaN  FAILED\n",
      "mpnn_layer_3         NaN        NaN      NaN     NaN            NaN  FAILED\n",
      "================================================================================\n",
      "\n",
      " Best config: mpnn_layer_3\n",
      "   Test loss: 0.004230\n"
     ]
    }
   ],
   "source": [
    "# --- Convert to DataFrame for easy analysis and summary ---\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Config': r['config_name'],\n",
    "        'Train Loss': r.get('final_train_loss', np.nan),\n",
    "        'Test Loss': r.get('test_loss', np.nan),\n",
    "        'Gen Gap': r.get('generalization_gap', np.nan),\n",
    "        'Epochs': r.get('epochs_trained', np.nan),\n",
    "        'Runtime (min)': r.get('runtime_seconds', np.nan) / 60,\n",
    "        'Status': r.get('status', 'SUCCESS')\n",
    "    }\n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "# --- Sort by test loss (best first) ---\n",
    "results_df = results_df.sort_values('Test Loss')\n",
    "\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Find best config that succeeded ---\n",
    "if len(results_df[results_df['Status'] == 'SUCCESS']) > 0:\n",
    "    best_idx = results_df[results_df['Status'] == 'SUCCESS']['Test Loss'].idxmin()\n",
    "    best_config = results_df.loc[best_idx, 'Config']\n",
    "    print(f\"\\n Best config: {best_config}\")\n",
    "    print(f\"   Test loss: {results_df.loc[best_idx, 'Test Loss']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd647d91",
   "metadata": {},
   "source": [
    "<h2> Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d383e9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# --- Only plot successful runs ---\n",
    "successful_results = [r for r in all_results if 'loss_history' in r]\n",
    "\n",
    "for result in successful_results:\n",
    "    # --- Plot 1: Loss curves ---\n",
    "    axes[0].plot(result['loss_history'], label=result['config_name'], alpha=0.7)\n",
    "    \n",
    "    \n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Curves')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19cd4a1",
   "metadata": {},
   "source": [
    "<h2> Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d21e42",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/results/hyperparameter_search_20260212_104606.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m results_file = results_dir / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhyperparameter_search_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pkl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# --- Save everything ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     13\u001b[39m     pickle.dump({\n\u001b[32m     14\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mall_results\u001b[39m\u001b[33m'\u001b[39m: all_results,\n\u001b[32m     15\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mconfigs\u001b[39m\u001b[33m'\u001b[39m: configs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m         }\n\u001b[32m     23\u001b[39m     }, f)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResults saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/results/hyperparameter_search_20260212_104606.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# --- Create results directory if it doesn't exist ---\n",
    "results_dir = Path(\"/results\")\n",
    "#results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# --- Generate filename with timestamp ---\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = results_dir / f\"hyperparameter_search_{timestamp}.pkl\"\n",
    "\n",
    "# --- Save everything ---\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'all_results': all_results,\n",
    "        'configs': configs,\n",
    "        'results_df': results_df,\n",
    "        'metadata': {\n",
    "            'seed': SEED,\n",
    "            'device': str(device),\n",
    "            'h5_path': h5_path,\n",
    "            'test_path': test_path,\n",
    "        }\n",
    "    }, f)\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")\n",
    "\n",
    "# --- Also save CSV for spreadsheet analysis ---\n",
    "csv_file = results_dir / f\"hyperparameter_search_{timestamp}.csv\"\n",
    "results_df.to_csv(csv_file, index=False)\n",
    "print(f\"CSV saved to: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4258e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<precomputed_dataset.precomputedDataset at 0x7f5134158d40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a72145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
