{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fe7fd3",
   "metadata": {},
   "source": [
    "<h1> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e70bf09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/acoupipe_customFeatures/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data \n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003c21b",
   "metadata": {},
   "source": [
    "<h1> Data Set Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b69b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class h5Dataset(Dataset):\n",
    "    def __init__(self, h5_path):\n",
    "        self.h5_path = h5_path\n",
    "\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            self.keys = list(f.keys())\n",
    "\n",
    "        self._edge_cache = {} # cache for fully connected edges to improve performance\n",
    "        self._f = None  # open file once \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # --- open file ---\n",
    "        f = self._get_file()\n",
    "\n",
    "        # --- load sample ---\n",
    "        sample = f[self.keys[index]]\n",
    "\n",
    "        # --- load and cast raw features from sample ---\n",
    "        csm = torch.from_numpy(sample[\"csm\"][:]).squeeze().to(torch.complex64) # (N, N), complex64\n",
    "        eigmode = torch.from_numpy(sample[\"eigmode\"][:]).to(torch.complex64) # (N, N), complex64\n",
    "        eigmode = torch.view_as_real(eigmode).to(torch.float32)  \n",
    "        coords = torch.from_numpy(sample[\"cartesian_coordinates\"][:]).T.to(torch.float32) # (N, 3), float32 \n",
    "        loc = torch.from_numpy(sample[\"loc\"][:]).to(torch.float32) # (3, nsources), float32\n",
    "        source_strength = torch.from_numpy(sample[\"source_strength_analytic\"][:]).squeeze(0).to(torch.float32) # (nsources,), float32\n",
    "\n",
    "\n",
    "        # --- normalize raw features ---\n",
    "        #TODO: check alternative approach normalize autopower by trace and cross spectra by coherence\n",
    "        csm = csm / torch.trace(csm).real\n",
    "        source_strength = source_strength / source_strength.sum()\n",
    "\n",
    "        # --- define node features ---        \n",
    "        theta = torch.atan2(coords[:, 1], coords[:, 0])\n",
    "        cos_theta = torch.cos(theta) # (N,), float32\n",
    "        sin_theta = torch.sin(theta) # (N,), float32\n",
    "\n",
    "        r = torch.sqrt(coords[:, 0]**2 + coords[:, 1]**2) # (N,), float32\n",
    "        r = r / (r.max() + 1e-8) # normalize radius  \n",
    "        \n",
    "        autopower = torch.diagonal(csm) # (N,), complex64\n",
    "        autopower_real = autopower.real # (N,), float32\n",
    "        autopower_imag = autopower.imag # (N,), float32\n",
    "\n",
    "        #TODO: implement positional encoding (Min-Sang Baek, Joon-Hyuk Chang, and Israel Cohen) \n",
    " \n",
    "        # --- define adjacency--- \n",
    "        N = coords.size(0)\n",
    "        edge_index = self.get_fully_connected_edges(N)   # (2, E), cached, no self-loops\n",
    "\n",
    "        src, dst = edge_index  # (E,), (E,)\n",
    "\n",
    "        # --- define edge features ---\n",
    "        cross_spectra = csm[src, dst]  # (E, 1), complex64\n",
    "        cross_spectra_real = cross_spectra.real # (E, 1), float32\n",
    "        cross_spectra_imag = cross_spectra.imag # (E, 1), float32\n",
    "\n",
    "        dx = (coords[dst, 0] - coords[src, 0])\n",
    "        dy = (coords[dst, 1] - coords[src, 1])   \n",
    "        dist = torch.sqrt(dx**2 + dy**2 + 1e-8) # (E, 1), float32\n",
    "        \n",
    "        unit_direction_x = dx / dist # (E, 1), float32 \n",
    "        unit_direction_y = dy / dist # (E, 1), float32\n",
    "\n",
    "        cos_sim = (cos_theta[src] * cos_theta[dst] + sin_theta[src] * sin_theta[dst]) # (E, 1), float32, computed with trigonometric identity\n",
    "\n",
    "        #TODO: implement directional features (Jingjie Fan, Rongzhi Gu, Yi Luo, and Cong Pang)\n",
    "\n",
    "\n",
    "        # --- build feature vectors ---\n",
    "        node_feat = self.build_feature(coords, r, cos_theta, sin_theta, autopower_real, autopower_imag, dim=1) # (N, F_node)\n",
    "        edge_attr = self.build_feature(cross_spectra_real,cross_spectra_imag, dist, unit_direction_x, unit_direction_y, cos_sim, dim=1)  # (E, F_edge)\n",
    "\n",
    "        # ---  define eigmode tokens analog to Kujawaski et. al---\n",
    "        eigmode = torch.cat([torch.cat([eigmode[..., 0], -eigmode[..., 1]], dim=-1), torch.cat([eigmode[..., 1],  eigmode[..., 0]], dim=-1),],dim=-2,)\n",
    "\n",
    "        # --- labels ---\n",
    "        loc_strongest_source = loc[:,torch.argmax(source_strength)]\n",
    "        loc_strongest_source = loc_strongest_source[:2].unsqueeze(0) #x and y coordinates only\n",
    "\n",
    "        strength_strongest_source = source_strength[torch.argmax(source_strength)] \n",
    "\n",
    "        # --- build PyG Data ---\n",
    "        data = Data(\n",
    "            x=node_feat,                 # (N, F_node)\n",
    "            edge_index=edge_index,       # (2, E)\n",
    "            edge_attr=edge_attr,         # (E, F_edge)\n",
    "            #TODO: Change to multiple sources and strengths later on\n",
    "            y=loc_strongest_source,      # label used by training loop\n",
    "        )\n",
    "\n",
    "        data.eigmode = eigmode\n",
    "\n",
    "        return data\n",
    "    \n",
    "\n",
    "    #--- utility functions ---\n",
    "    @staticmethod\n",
    "    def build_feature(*feats, dim=-1):\n",
    "        \"\"\"\n",
    "        Utility function to construct a feature tensor from multiple inputs.\n",
    "\n",
    "        If a tensor is 1D (shape: [N]), it is automatically expanded to\n",
    "        shape [N, 1] so that it can be concatenated with higher-dimensional\n",
    "        feature tensors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        *feats : torch.Tensor\n",
    "            Feature tensors to be combined. Must be broadcast-compatible\n",
    "            except for the concatenation dimension.\n",
    "        dim : int, optional\n",
    "            Dimension along which to concatenate the features (default: -1).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Concatenated feature tensor.\n",
    "        \"\"\"\n",
    "        feats = [feature.unsqueeze(-1) if feature.dim() == 1 else feature for feature in feats]\n",
    "        return torch.cat(feats, dim=dim)\n",
    "\n",
    "    def _get_file(self):\n",
    "        \"\"\"\n",
    "        Lazily opens the HDF5 file and keeps it open for reuse\n",
    "        to avoids repeatedly opening and closing the HDF5 file on every\n",
    "        __getitem__ call. Reduces I/O overhead.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._f is None:\n",
    "            self._f = h5py.File(self.h5_path, \"r\")\n",
    "        return self._f\n",
    "\n",
    "    def get_fully_connected_edges(self, N):\n",
    "        \"\"\"\n",
    "        Returns the edge_index of a fully connected directed graph with N nodes,\n",
    "        excluding self-loops and caches the result for performance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        N : int\n",
    "            Number of nodes in the graph.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        edge_index : torch.Tensor\n",
    "            Edge index tensor \n",
    "        \"\"\"\n",
    "        if N not in self._edge_cache:\n",
    "            adj = torch.ones(N, N, dtype=torch.bool)\n",
    "            adj.fill_diagonal_(False)\n",
    "            self._edge_cache[N] = dense_to_sparse(adj)[0]\n",
    "\n",
    "        return self._edge_cache[N]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323b358",
   "metadata": {},
   "source": [
    "<h1> Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8bc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNNLayer(MessagePassing):\n",
    "    \"\"\"\n",
    "    One message-passing block with edge features, mean aggregation,\n",
    "    residual connection, and LayerNorm to reduce oversmoothing in fully-connected graphs.\n",
    "\n",
    "    Input/Output node dim stays constant: hidden_dim -> hidden_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, edge_dim: int, dropout: float = 0.0):\n",
    "        \n",
    "        super().__init__(aggr=\"mean\") # fully connected graph, so messages dont blow up with \"add\" aggregation\n",
    "\n",
    "        self.msg_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim + edge_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.upd_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(hidden_dim) # helps with oversmoothing\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        # propagate: message passing + aggregation + update\n",
    "        out = self.propagate(edge_index=edge_index, x=x, edge_attr=edge_attr)\n",
    "        # residual helps with oversmoothing\n",
    "        return self.norm(x + out)\n",
    "        #return x + out\n",
    "\n",
    "    def message(self, x_i: torch.Tensor, x_j: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        # x_i: target node features [E, H]\n",
    "        # x_j: source node features [E, H]\n",
    "        # edge_attr:             [E, E_dim]\n",
    "        msg_in = torch.cat([x_i, x_j, edge_attr], dim=-1)  # [E, 2H + E_dim]\n",
    "        return self.msg_mlp(msg_in)                        # [E, H]\n",
    "\n",
    "    def update(self, aggr_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        # aggr_out: [N, H], x: [N, H]\n",
    "        upd_in = torch.cat([x, aggr_out], dim=-1)          # [N, 2H]\n",
    "        return self.upd_mlp(upd_in)                        # [N, H]\n",
    "\n",
    "class MPNNTokenizer(nn.Module):\n",
    "    \"\"\"\n",
    "    MPNN-based tokenizer to convert graphs into token embeddings for attention mechanism:\n",
    "      node_in -> hidden_dim -> (L x MPNNLayer) -> out_dim\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_dim: int,\n",
    "        edge_in_dim: int,\n",
    "        hidden_dim: int = 128,\n",
    "        out_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- project node features to hidden dim\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(node_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        # --- multiple MPNN layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            MPNNLayer(hidden_dim=hidden_dim, edge_dim=edge_in_dim, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # --- final projection to the token dim you want for cross-attention\n",
    "        self.node_head = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x:         [N, node_in_dim]\n",
    "        edge_index:[2, E]\n",
    "        edge_attr: [E, edge_in_dim]\n",
    "        returns:   [N, out_dim] microphone embeddings (tokens)\n",
    "        \"\"\"\n",
    "        h = self.node_encoder(x)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, edge_index, edge_attr)\n",
    "        return self.node_head(h)\n",
    "\n",
    "class SelfAttentionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder following ViT-Base architecture:\n",
    "    - 12 layers of multi-head self-attention (8 heads, D=128)\n",
    "    - Takes microphone embeddings and outputs pooled vector for MLP head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 128,\n",
    "        num_heads: int = 8, # embed_dim must be divisible by num_heads \n",
    "        num_layers: int = 12,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # --- learnable CLS token (1 token, D) ---\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # --- Multihead self-attention layer ---\n",
    "        multihead_self_attention_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,  # typical 4x expansion in transformer FFN\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True,  # pre-norm for better training stability\n",
    "        )\n",
    "        \n",
    "        # -- Transformer encoder with multiple multihead self-attention layers ---\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            multihead_self_attention_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        \n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        tokens: [N, embed_dim] or [B, N, embed_dim] microphone embeddings from MPNNTokenizer\n",
    "        returns: [embed_dim] or [B, embed_dim] encoded features after global pooling\n",
    "        \"\"\"\n",
    "        # Adds batch dimension if needed: [N, D] -> [1, N, D]\n",
    "        squeeze_output = False\n",
    "        if tokens.dim() == 2:\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "            squeeze_output = True\n",
    "        \n",
    "        #B = tokens.size(0)\n",
    "        # prepend CLS token: [B, 1, D] + [B, N, D] -> [B, 1+N, D]\n",
    "        #cls = self.cls_token.expand(B, -1, -1)\n",
    "        #tokens = torch.cat([cls, tokens], dim=1)\n",
    "\n",
    "        # --- Multihead self-attention layers ---\n",
    "        encoded = self.transformer_encoder(tokens)  # [B, N, D]\n",
    "        \n",
    "        # --- Global pooling (mean over all microphone tokens) ---\n",
    "        # TODO: try other pooling mechanism f.e. CLI Token\n",
    "        #pooled = encoded.mean(dim=1)  # [B, D]\n",
    "        # --- CLS pooling ---\n",
    "        pooled = encoded[:, 0, :]  # [B, D]\n",
    "        \n",
    "        # Remove batch dimension if input was unbatched\n",
    "        if squeeze_output:\n",
    "            pooled = pooled.squeeze(0)  # [D]\n",
    "        \n",
    "        return pooled\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer MLP (512 neurons each) that outputs source locations and strengths.\n",
    "    - Location head: outputs I source locations (x, y coordinates)\n",
    "    - Strength head: outputs normalized strengths via Softmax\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 128,\n",
    "        mlp_hidden_dim: int = 512,\n",
    "        num_output_sources: int = 1,  # I = 1 source component\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_output_sources = num_output_sources\n",
    "        \n",
    "        # --- Two-layer MLP with 512 neurons each ---\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # --- Source location head (2D coordinates per source) ---\n",
    "        self.location_head = nn.Linear(mlp_hidden_dim, num_output_sources * 2)\n",
    "        \n",
    "        # --- Source strength head (normalized via softmax) ---\n",
    "        #self.strength_head = nn.Linear(mlp_hidden_dim, num_output_sources)\n",
    "        \n",
    "    def forward(self, encoded_features: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        encoded_features: [embed_dim] or [B, embed_dim] from TransformerEncoder\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        locations: [num_output_sources, 2] or [B, num_output_sources, 2] \n",
    "                   predicted source locations (x, y)\n",
    "        strengths: [num_output_sources] or [B, num_output_sources]\n",
    "                   normalized source strengths (sum to 1)\n",
    "        \"\"\"\n",
    "        # Handle both batched and unbatched input\n",
    "        squeeze_output = False\n",
    "        if encoded_features.dim() == 1:\n",
    "            encoded_features = encoded_features.unsqueeze(0)\n",
    "            squeeze_output = True\n",
    "        \n",
    "        # --- MLP processing ---\n",
    "        features = self.mlp(encoded_features)  # [B, mlp_hidden_dim]\n",
    "        \n",
    "        # --- LOCATION HEAD OUTPUT ---\n",
    "        # Raw output is [B, num_output_sources * 2]\n",
    "        locations = self.location_head(features)  # [B, I * 2]\n",
    "        # Reshape to [B, I, 2] where each source has (x, y) coordinates\n",
    "        locations = locations.view(-1, self.num_output_sources, 2)  # [B, I, 2]\n",
    "        \n",
    "        # --- STRENGTH HEAD OUTPUT ---\n",
    "        # Raw output is [B, num_output_sources]\n",
    "        #strengths = self.strength_head(features)  # [B, I]\n",
    "        # Apply softmax to normalize strengths (they sum to 1)\n",
    "        #strengths = torch.softmax(strengths, dim=-1)  # [B, I]\n",
    "        \n",
    "        # Remove batch dimension if input was unbatched\n",
    "        if squeeze_output:\n",
    "            locations = locations.squeeze(0)  # [I, 2]\n",
    "            #strengths = strengths.squeeze(0)  # [I]\n",
    "        \n",
    "        return locations# , strengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a59c4c",
   "metadata": {},
   "source": [
    "<h1> Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba30782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNNTransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "      data (f.e. h5Dataset) -> MPNNTokenizer -> SelfAttentionEncoder -> PredictionHead\n",
    "\n",
    "    Expected inputs (PyGeometric style):\n",
    "      x:         [N, node_in_dim]\n",
    "      edge_index:[2, E]\n",
    "      edge_attr: [E, edge_in_dim]\n",
    "\n",
    "    Output:\n",
    "      locations: [I, 2] (unbatched) or [B, I, 2] if you pass batched tokens later\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # --- tokenizer params --- #\n",
    "        node_in_dim: int,\n",
    "        edge_in_dim: int,\n",
    "        mpnn_hidden_dim: int = 128,\n",
    "        token_dim: int = 128,\n",
    "        mpnn_num_layers: int = 1,\n",
    "        mpnn_dropout: float = 0.0,\n",
    "        # --- self-attention encoder params --- #\n",
    "        attn_num_heads: int = 8, # token dim must be divisible by attn_num_heads\n",
    "        attn_num_layers: int = 2, #12,\n",
    "        attn_dropout: float = 0.0, #0.1,\n",
    "        # --- prediction head params --- #\n",
    "        head_mlp_hidden_dim: int = 512,\n",
    "        num_output_sources: int = 1,\n",
    "        head_dropout: float = 0.0 #0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- tokenizer (graph -> mic tokens) ---\n",
    "        self.tokenizer = MPNNTokenizer(\n",
    "            node_in_dim=node_in_dim,\n",
    "            edge_in_dim=edge_in_dim,\n",
    "            hidden_dim=mpnn_hidden_dim,\n",
    "            out_dim=token_dim,\n",
    "            num_layers=mpnn_num_layers,\n",
    "            dropout=mpnn_dropout,\n",
    "        )\n",
    "\n",
    "        # --- self-attention encoder (tokens -> pooled embedding) ---\n",
    "        self.encoder = SelfAttentionEncoder(\n",
    "            embed_dim=token_dim,\n",
    "            num_heads=attn_num_heads,\n",
    "            num_layers=attn_num_layers,\n",
    "            dropout=attn_dropout,\n",
    "        )\n",
    "\n",
    "        # --- prediction head (pooled embedding -> outputs) ---\n",
    "        self.head = PredictionHead(\n",
    "            embed_dim=token_dim,\n",
    "            mlp_hidden_dim=head_mlp_hidden_dim,\n",
    "            num_output_sources=num_output_sources,\n",
    "            dropout=head_dropout,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        # 1) graph -> mic tokens: [N, D]\n",
    "        tokens = self.tokenizer(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        # 2) tokens -> pooled: [D] (or [B, D] if tokens were batched)\n",
    "        #encoded = self.encoder(tokens)\n",
    "\n",
    "        # 3) pooled -> locations: [I, 2] (or [B, I, 2])\n",
    "        #locations = self.head(encoded)\n",
    "        return tokens\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        return self.forward(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    def forward_from_data(self, data) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convenience for PyG Data/Batch objects that expose .x, .edge_index, .edge_attr\n",
    "        \"\"\"\n",
    "        return self.forward(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2dd61f",
   "metadata": {},
   "source": [
    "<h1>Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/acoupipe_customFeatures/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Dataset\n",
    "h5_path = \"10samples.h5\"  # <- CHANGE THIS\n",
    "ds = h5Dataset(h5_path)\n",
    "\n",
    "loader = DataLoader(ds, batch_size=10)\n",
    "\n",
    "# take feature dims from one sample\n",
    "sample0 = ds[0]\n",
    "node_in_dim = sample0.x.shape[-1]\n",
    "edge_in_dim = sample0.edge_attr.shape[-1]\n",
    "\n",
    "# Build model\n",
    "model = MPNNTransformerModel(\n",
    "    node_in_dim=node_in_dim,\n",
    "    edge_in_dim=edge_in_dim,\n",
    "    num_output_sources=1,  \n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-2, weight_decay=0.0)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=50,   # every 10 epochs\n",
    "    gamma=0.5       # multiply LR by 0.5\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "#for data in loader:\n",
    "    #data = data.to(device)\n",
    "    #pred = model.forward_from_data(data)  \n",
    "    #pred.shape\n",
    "\n",
    "for batch in loader:\n",
    "    #print(batch.batch)    \n",
    "    #print(batch.batch) \n",
    "    #print(batch.batch.shape) \n",
    "    p\n",
    "\n",
    "#Shape (Microphone Count * Batch Size, Token Dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b940f0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m lengths = torch.tensor([\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mlengths\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03a371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acoupipe_customFeatures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
