{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fe7fd3",
   "metadata": {},
   "source": [
    "<h1> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70bf09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data \n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003c21b",
   "metadata": {},
   "source": [
    "<h1> Data Set Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3b69b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class h5Dataset(Dataset):\n",
    "    def __init__(self, h5_path):\n",
    "        self.h5_path = h5_path\n",
    "\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            self.keys = list(f.keys())\n",
    "\n",
    "        self._edge_cache = {} # cache for fully connected edges to improve performance\n",
    "        self._f = None  # open file once \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # --- open file ---\n",
    "        f = self._get_file()\n",
    "\n",
    "        # --- load sample ---\n",
    "        sample = f[self.keys[index]]\n",
    "\n",
    "        # --- load and cast raw features from sample ---\n",
    "        csm = torch.from_numpy(sample[\"csm\"][:]).squeeze().to(torch.complex64) # (N, N), complex64\n",
    "        eigmode = torch.from_numpy(sample[\"eigmode\"][:]).to(torch.complex64) # (N, N), complex64\n",
    "        eigmode = torch.view_as_real(eigmode).to(torch.float32)  \n",
    "        coords = torch.from_numpy(sample[\"cartesian_coordinates\"][:]).T.to(torch.float32) # (N, 3), float32 \n",
    "        loc = torch.from_numpy(sample[\"loc\"][:]).to(torch.float32) # (3, nsources), float32\n",
    "        source_strength = torch.from_numpy(sample[\"source_strength_analytic\"][:]).squeeze(0).to(torch.float32) # (nsources,), float32\n",
    "\n",
    "\n",
    "        # --- normalize raw features ---\n",
    "        #TODO: check alternative approach normalize autopower by trace and cross spectra by coherence\n",
    "        csm = csm / torch.trace(csm).real\n",
    "        source_strength = source_strength / source_strength.sum()\n",
    "\n",
    "        # --- define node features ---        \n",
    "        theta = torch.atan2(coords[:, 1], coords[:, 0])\n",
    "        cos_theta = torch.cos(theta) # (N,), float32\n",
    "        sin_theta = torch.sin(theta) # (N,), float32\n",
    "\n",
    "        r = torch.sqrt(coords[:, 0]**2 + coords[:, 1]**2) # (N,), float32\n",
    "        r = r / (r.max() + 1e-8) # normalize radius  \n",
    "        \n",
    "        autopower = torch.diagonal(csm) # (N,), complex64\n",
    "        autopower_real = autopower.real # (N,), float32\n",
    "        autopower_imag = autopower.imag # (N,), float32\n",
    "\n",
    "        #TODO: implement positional encoding (Min-Sang Baek, Joon-Hyuk Chang, and Israel Cohen) \n",
    " \n",
    "        # --- define adjacency--- \n",
    "        N = coords.size(0)\n",
    "        edge_index = self.get_fully_connected_edges(N)   # (2, E), cached, no self-loops\n",
    "\n",
    "        src, dst = edge_index  # (E,), (E,)\n",
    "\n",
    "        # --- define edge features ---\n",
    "        cross_spectra = csm[src, dst]  # (E, 1), complex64\n",
    "        cross_spectra_real = cross_spectra.real # (E, 1), float32\n",
    "        cross_spectra_imag = cross_spectra.imag # (E, 1), float32\n",
    "\n",
    "        dx = (coords[dst, 0] - coords[src, 0])\n",
    "        dy = (coords[dst, 1] - coords[src, 1])   \n",
    "        dist = torch.sqrt(dx**2 + dy**2 + 1e-8) # (E, 1), float32\n",
    "        \n",
    "        unit_direction_x = dx / dist # (E, 1), float32 \n",
    "        unit_direction_y = dy / dist # (E, 1), float32\n",
    "\n",
    "        cos_sim = (cos_theta[src] * cos_theta[dst] + sin_theta[src] * sin_theta[dst]) # (E, 1), float32, computed with trigonometric identity\n",
    "\n",
    "        #TODO: implement directional features (Jingjie Fan, Rongzhi Gu, Yi Luo, and Cong Pang)\n",
    "\n",
    "\n",
    "        # --- build feature vectors ---\n",
    "        node_feat = self.build_feature(coords, r, cos_theta, sin_theta, autopower_real, autopower_imag, dim=1) # (N, F_node)\n",
    "        edge_attr = self.build_feature(cross_spectra_real,cross_spectra_imag, dist, unit_direction_x, unit_direction_y, cos_sim, dim=1)  # (E, F_edge)\n",
    "\n",
    "        # ---  define eigmode tokens analog to Kujawaski et. al---\n",
    "        eigmode = torch.cat([torch.cat([eigmode[..., 0], -eigmode[..., 1]], dim=-1), torch.cat([eigmode[..., 1],  eigmode[..., 0]], dim=-1),],dim=-2,)\n",
    "\n",
    "        # --- labels ---\n",
    "        loc_strongest_source = loc[:,torch.argmax(source_strength)]\n",
    "        loc_strongest_source = loc_strongest_source[:2].unsqueeze(0).unsqueeze(0) #x and y coordinates only\n",
    "\n",
    "        strength_strongest_source = source_strength[torch.argmax(source_strength)] \n",
    "\n",
    "        # --- build PyG Data ---\n",
    "        data = Data(\n",
    "            x=node_feat,                 # (N, F_node)\n",
    "            edge_index=edge_index,       # (2, E)\n",
    "            edge_attr=edge_attr,         # (E, F_edge)\n",
    "            #TODO: Change to multiple sources and strengths later on\n",
    "            y=loc_strongest_source,      # label used by training loop\n",
    "        )\n",
    "\n",
    "        data.eigmode = eigmode\n",
    "\n",
    "        return data\n",
    "    \n",
    "\n",
    "    #--- utility functions ---\n",
    "    @staticmethod\n",
    "    def build_feature(*feats, dim=-1):\n",
    "        \"\"\"\n",
    "        Utility function to construct a feature tensor from multiple inputs.\n",
    "\n",
    "        If a tensor is 1D (shape: [N]), it is automatically expanded to\n",
    "        shape [N, 1] so that it can be concatenated with higher-dimensional\n",
    "        feature tensors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        *feats : torch.Tensor\n",
    "            Feature tensors to be combined. Must be broadcast-compatible\n",
    "            except for the concatenation dimension.\n",
    "        dim : int, optional\n",
    "            Dimension along which to concatenate the features (default: -1).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Concatenated feature tensor.\n",
    "        \"\"\"\n",
    "        feats = [feature.unsqueeze(-1) if feature.dim() == 1 else feature for feature in feats]\n",
    "        return torch.cat(feats, dim=dim)\n",
    "\n",
    "    def _get_file(self):\n",
    "        \"\"\"\n",
    "        Lazily opens the HDF5 file and keeps it open for reuse\n",
    "        to avoids repeatedly opening and closing the HDF5 file on every\n",
    "        __getitem__ call. Reduces I/O overhead.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._f is None:\n",
    "            self._f = h5py.File(self.h5_path, \"r\")\n",
    "        return self._f\n",
    "\n",
    "    def get_fully_connected_edges(self, N):\n",
    "        \"\"\"\n",
    "        Returns the edge_index of a fully connected directed graph with N nodes,\n",
    "        excluding self-loops and caches the result for performance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        N : int\n",
    "            Number of nodes in the graph.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        edge_index : torch.Tensor\n",
    "            Edge index tensor \n",
    "        \"\"\"\n",
    "        if N not in self._edge_cache:\n",
    "            adj = torch.ones(N, N, dtype=torch.bool)\n",
    "            adj.fill_diagonal_(False)\n",
    "            self._edge_cache[N] = dense_to_sparse(adj)[0]\n",
    "\n",
    "        return self._edge_cache[N]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323b358",
   "metadata": {},
   "source": [
    "<h1> Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b8bc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNNLayer(MessagePassing):\n",
    "    \"\"\"\n",
    "    One message-passing block with edge features, mean aggregation,\n",
    "    residual connection, and LayerNorm to reduce oversmoothing in fully-connected graphs.\n",
    "\n",
    "    Input/Output node dim stays constant: hidden_dim -> hidden_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, edge_dim: int, dropout: float = 0.0):\n",
    "        \n",
    "        super().__init__(aggr=\"mean\") # fully connected graph, so messages dont blow up with \"add\" aggregation\n",
    "\n",
    "        self.msg_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim + edge_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.upd_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(hidden_dim) # helps with oversmoothing\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        # propagate: message passing + aggregation + update\n",
    "        out = self.propagate(edge_index=edge_index, x=x, edge_attr=edge_attr)\n",
    "        # residual helps with oversmoothing\n",
    "        #return self.norm(x + out)\n",
    "        return x + out\n",
    "\n",
    "    def message(self, x_i: torch.Tensor, x_j: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        # x_i: target node features [E, H]\n",
    "        # x_j: source node features [E, H]\n",
    "        # edge_attr:             [E, E_dim]\n",
    "        msg_in = torch.cat([x_i, x_j, edge_attr], dim=-1)  # [E, 2H + E_dim]\n",
    "        return self.msg_mlp(msg_in)                        # [E, H]\n",
    "\n",
    "    def update(self, aggr_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        # aggr_out: [N, H], x: [N, H]\n",
    "        upd_in = torch.cat([x, aggr_out], dim=-1)          # [N, 2H]\n",
    "        return self.upd_mlp(upd_in)                        # [N, H]\n",
    "\n",
    "class MPNNTokenizer(nn.Module):\n",
    "    \"\"\"\n",
    "    MPNN-based tokenizer to convert graphs into token embeddings for attention mechanism:\n",
    "      node_in -> hidden_dim -> (L x MPNNLayer) -> out_dim\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_dim: int,\n",
    "        edge_in_dim: int,\n",
    "        hidden_dim: int = 128,\n",
    "        out_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- project node features to hidden dim\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(node_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        # --- multiple MPNN layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            MPNNLayer(hidden_dim=hidden_dim, edge_dim=edge_in_dim, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # --- final projection to the token dim you want for cross-attention\n",
    "        self.node_head = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x:         [N, node_in_dim]\n",
    "        edge_index:[2, E]\n",
    "        edge_attr: [E, edge_in_dim]\n",
    "        returns:   [N, out_dim] microphone embeddings (tokens)\n",
    "        \"\"\"\n",
    "        h = self.node_encoder(x)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, edge_index, edge_attr)\n",
    "        return self.node_head(h)\n",
    "\n",
    "class SelfAttentionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder following ViT-Base architecture:\n",
    "    - 12 layers of multi-head self-attention (8 heads, D=128)\n",
    "    - Takes microphone embeddings and outputs pooled vector for MLP head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 128,\n",
    "        num_heads: int = 8, # embed_dim must be divisible by num_heads \n",
    "        num_layers: int = 12,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # --- learnable CLS token (1 token, D) ---\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # --- Multihead self-attention layer ---\n",
    "        multihead_self_attention_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,  # typical 4x expansion in transformer FFN\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True,  # pre-norm for better training stability\n",
    "        )\n",
    "        \n",
    "        # -- Transformer encoder with multiple multihead self-attention layers ---\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            multihead_self_attention_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        \n",
    "    def forward(self, tokens: torch.Tensor, src_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        tokens: [N, embed_dim] or [B, N, embed_dim] microphone embeddings from MPNNTokenizer\n",
    "        returns: [embed_dim] or [B, embed_dim] encoded features after global pooling\n",
    "        \"\"\"\n",
    "        # Adds batch dimension if needed: [N, D] -> [1, N, D]\n",
    "        squeeze_output = False\n",
    "        if tokens.dim() == 2:\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "            squeeze_output = True\n",
    "        \n",
    "        #B = tokens.size(0)\n",
    "        # prepend CLS token: [B, 1, D] + [B, N, D] -> [B, 1+N, D]\n",
    "        #cls = self.cls_token.expand(B, -1, -1)\n",
    "        #tokens = torch.cat([cls, tokens], dim=1)\n",
    "\n",
    "        # --- Multihead self-attention layers ---\n",
    "        encoded = self.transformer_encoder(tokens, src_key_padding_mask=src_key_padding_mask)  # [B, N, D]\n",
    "        \n",
    "        # --- Global pooling (mean over all microphone tokens) ---\n",
    "        # TODO: try other pooling mechanism f.e. CLI Token\n",
    "        #pooled = encoded.mean(dim=1)  # [B, D]\n",
    "        # --- CLS pooling ---\n",
    "        pooled = encoded[:, 0, :]  # [B, D]\n",
    "        \n",
    "        # Remove batch dimension if input was unbatched\n",
    "        if squeeze_output:\n",
    "            pooled = pooled.squeeze(0)  # [D]\n",
    "        \n",
    "        return pooled\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer MLP (512 neurons each) that outputs source locations and strengths.\n",
    "    - Location head: outputs I source locations (x, y coordinates)\n",
    "    - Strength head: outputs normalized strengths via Softmax\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 128,\n",
    "        mlp_hidden_dim: int = 512,\n",
    "        num_output_sources: int = 1,  # I = 1 source component\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_output_sources = num_output_sources\n",
    "        \n",
    "        # --- Two-layer MLP with 512 neurons each ---\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # --- Source location head (2D coordinates per source) ---\n",
    "        self.location_head = nn.Linear(mlp_hidden_dim, num_output_sources * 2)\n",
    "        \n",
    "        # --- Source strength head (normalized via softmax) ---\n",
    "        #self.strength_head = nn.Linear(mlp_hidden_dim, num_output_sources)\n",
    "        \n",
    "    def forward(self, encoded_features: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        encoded_features: [embed_dim] or [B, embed_dim] from TransformerEncoder\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        locations: [num_output_sources, 2] or [B, num_output_sources, 2] \n",
    "                   predicted source locations (x, y)\n",
    "        strengths: [num_output_sources] or [B, num_output_sources]\n",
    "                   normalized source strengths (sum to 1)\n",
    "        \"\"\"\n",
    "        # Handle both batched and unbatched input\n",
    "        squeeze_output = False\n",
    "        if encoded_features.dim() == 1:\n",
    "            encoded_features = encoded_features.unsqueeze(0)\n",
    "            squeeze_output = True\n",
    "        \n",
    "        # --- MLP processing ---\n",
    "        features = self.mlp(encoded_features)  # [B, mlp_hidden_dim]\n",
    "        \n",
    "        # --- LOCATION HEAD OUTPUT ---\n",
    "        # Raw output is [B, num_output_sources * 2]\n",
    "        locations = self.location_head(features)  # [B, I * 2]\n",
    "        # Reshape to [B, I, 2] where each source has (x, y) coordinates\n",
    "        locations = locations.view(-1, self.num_output_sources, 2)  # [B, I, 2]\n",
    "        \n",
    "        # --- STRENGTH HEAD OUTPUT ---\n",
    "        # Raw output is [B, num_output_sources]\n",
    "        #strengths = self.strength_head(features)  # [B, I]\n",
    "        # Apply softmax to normalize strengths (they sum to 1)\n",
    "        #strengths = torch.softmax(strengths, dim=-1)  # [B, I]\n",
    "        \n",
    "        # Remove batch dimension if input was unbatched\n",
    "        if squeeze_output:\n",
    "            locations = locations.squeeze(0)  # [I, 2]\n",
    "            #strengths = strengths.squeeze(0)  # [I]\n",
    "        \n",
    "        return locations# , strengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a59c4c",
   "metadata": {},
   "source": [
    "<h1> Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dba30782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNNTransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "      data (f.e. h5Dataset) -> MPNNTokenizer -> SelfAttentionEncoder -> PredictionHead\n",
    "\n",
    "    Expected inputs (PyGeometric style):\n",
    "      x:         [N, node_in_dim]\n",
    "      edge_index:[2, E]\n",
    "      edge_attr: [E, edge_in_dim]\n",
    "\n",
    "    Output:\n",
    "      locations: [I, 2] (unbatched) or [B, I, 2] if you pass batched tokens later\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # --- tokenizer params --- #\n",
    "        node_in_dim: int,\n",
    "        edge_in_dim: int,\n",
    "        mpnn_hidden_dim: int = 128,\n",
    "        token_dim: int = 128,\n",
    "        mpnn_num_layers: int = 1,\n",
    "        mpnn_dropout: float = 0.0,\n",
    "        # --- self-attention encoder params --- #\n",
    "        attn_num_heads: int = 8, # token dim must be divisible by attn_num_heads\n",
    "        attn_num_layers: int = 2, #12,\n",
    "        attn_dropout: float = 0.0, #0.1,\n",
    "        # --- prediction head params --- #\n",
    "        head_mlp_hidden_dim: int = 512,\n",
    "        num_output_sources: int = 1,\n",
    "        head_dropout: float = 0.0 #0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- tokenizer (graph -> mic tokens) ---\n",
    "        self.tokenizer = MPNNTokenizer(\n",
    "            node_in_dim=node_in_dim,\n",
    "            edge_in_dim=edge_in_dim,\n",
    "            hidden_dim=mpnn_hidden_dim,\n",
    "            out_dim=token_dim,\n",
    "            num_layers=mpnn_num_layers,\n",
    "            dropout=mpnn_dropout,\n",
    "        )\n",
    "\n",
    "        # --- self-attention encoder (tokens -> pooled embedding) ---\n",
    "        self.encoder = SelfAttentionEncoder(\n",
    "            embed_dim=token_dim,\n",
    "            num_heads=attn_num_heads,\n",
    "            num_layers=attn_num_layers,\n",
    "            dropout=attn_dropout,\n",
    "        )\n",
    "\n",
    "        # --- prediction head (pooled embedding -> outputs) ---\n",
    "        self.head = PredictionHead(\n",
    "            embed_dim=token_dim,\n",
    "            mlp_hidden_dim=head_mlp_hidden_dim,\n",
    "            num_output_sources=num_output_sources,\n",
    "            dropout=head_dropout,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "        batch: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        # 1) graph -> mic tokens: [N, D]\n",
    "        tokens = self.tokenizer(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        # 2) tokens -> contextualized token: [D] (or [B, D] if tokens were batched)\n",
    "        if batch is None:\n",
    "            pooled = self.encoder(tokens)\n",
    "        # batch nodes for transformer\n",
    "        else:    \n",
    "            batched_tokens, src_key_padding_mask = self.batch_nodes_for_transformer(tokens, batch_vector=batch)\n",
    "            pooled = self.encoder(batched_tokens, src_key_padding_mask=src_key_padding_mask)  \n",
    "\n",
    "        # 3) pooled -> locations: [I, 2] (or [B, I, 2])\n",
    "        locations = self.head(pooled)\n",
    "        return locations\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        return self.forward(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    def forward_from_data(self, data) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convenience for PyG Data/Batch objects that expose .x, .edge_index, .edge_attr\n",
    "        \"\"\"\n",
    "        batch = getattr(data, 'batch', None)\n",
    "        return self.forward(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, batch = batch)\n",
    "    \n",
    "\n",
    "    @staticmethod \n",
    "    def batch_nodes_for_transformer(node_embeddings, batch_vector):\n",
    "        \"\"\"\n",
    "        Convert PyG flat batched node embeddings to dense 3D tensor for Transformer.\n",
    "        - PyG batches graphs by concatenating all nodes into a flat [N, D] tensor\n",
    "        - Transformers require dense [B, L, D] tensors where L (seq_len) is uniform\n",
    "    \n",
    "        Expected inputs (PyTorch Geometric style):\n",
    "        node_embeddings: [N, hidden_dim]  — flat tensor with all nodes from all graphs\n",
    "        batch_vector:    [N]              — graph assignment index (e.g. [0,0,1,1,1,2,...])\n",
    "        \n",
    "        Output:\n",
    "        padded:                  [B, max_nodes, hidden_dim] — dense tensor, zero-padded\n",
    "        src_key_padding_mask:    [B, max_nodes]             — True where padded (ignore in attention)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Split the flat node tensor into a list of per-graph tensors\n",
    "        node_lists = []\n",
    "        for i in range(batch_vector.max().item() + 1):\n",
    "            mask = (batch_vector == i)\n",
    "            node_lists.append(node_embeddings[mask])  # (num_nodes_graph_i, hidden_dim)\n",
    "        \n",
    "        # Pad all graphs to max_nodes in dimension 1 (sequence length)\n",
    "        padded = pad_sequence(node_lists, batch_first=True, padding_value=0.0)  # (batch_size, max_num_nodes, hidden_dim)\n",
    "        \n",
    "        # Create attention mask: True where PADDED (TransformerEncoder ignores True positions)\n",
    "        lengths = torch.tensor([n.size(0) for n in node_lists])\n",
    "        max_len = padded.size(1)\n",
    "        attention_mask = torch.arange(max_len).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "        src_key_padding_mask = ~attention_mask  # [batch_size, max_num_nodes]\n",
    "        \n",
    "        return padded, src_key_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2dd61f",
   "metadata": {},
   "source": [
    "<h1>Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d24303a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 | loss = 0.058631\n",
      "Epoch   25 | loss = 0.017997\n",
      "Epoch   50 | loss = 0.012107\n",
      "Epoch   75 | loss = 0.010738\n",
      "Epoch  100 | loss = 0.011438\n",
      "Epoch  125 | loss = 0.009703\n",
      "Epoch  150 | loss = 0.007607\n",
      "Epoch  175 | loss = 0.009943\n",
      "Epoch  200 | loss = 0.007357\n",
      "Epoch  225 | loss = 0.006673\n",
      "Epoch  250 | loss = 0.007001\n",
      "Epoch  275 | loss = 0.004928\n",
      "Epoch  300 | loss = 0.005787\n",
      "\n",
      "Predictions vs targets (10 samples):\n",
      "[00] pred=[0.0787011  0.17103283]  target=[[0.0637504  0.27527907]]\n",
      "[01] pred=[ 0.16053125 -0.06300885]  target=[[ 0.03151155 -0.06910084]]\n",
      "[02] pred=[ 0.23529062 -0.2678623 ]  target=[[ 0.26129934 -0.24150708]]\n",
      "[03] pred=[-0.05240301  0.09999402]  target=[[-0.01454815  0.1408742 ]]\n",
      "[04] pred=[-0.1810579  0.1320733]  target=[[-0.28629878  0.15405327]]\n",
      "[05] pred=[-0.28952152  0.10936275]  target=[[-0.28557602  0.19455917]]\n",
      "[06] pred=[ 0.16298714 -0.00927548]  target=[[0.21992949 0.13799658]]\n",
      "[07] pred=[-0.25413927  0.18242784]  target=[[-0.22682664  0.30383795]]\n",
      "[08] pred=[0.04686727 0.07505259]  target=[[0.00428969 0.11796549]]\n",
      "[09] pred=[-0.00358979  0.01130019]  target=[[ 0.00596436 -0.0097244 ]]\n",
      "[10] pred=[-0.07272584 -0.14086513]  target=[[-0.08260332 -0.15429649]]\n",
      "[11] pred=[0.00908767 0.08774924]  target=[[-0.04077311  0.09625879]]\n",
      "[12] pred=[ 0.10068659 -0.03750596]  target=[[ 0.17163995 -0.020958  ]]\n",
      "[13] pred=[-0.1543482  -0.34037003]  target=[[-0.14536427 -0.3341658 ]]\n",
      "[14] pred=[ 0.10985821 -0.1318924 ]  target=[[ 0.1739218  -0.07540922]]\n",
      "[15] pred=[ 0.09527137 -0.32785147]  target=[[ 0.12125795 -0.31386778]]\n",
      "[16] pred=[0.25081363 0.00517355]  target=[[0.27394906 0.05149376]]\n",
      "[17] pred=[ 0.06696414 -0.1155128 ]  target=[[ 0.05893723 -0.15768556]]\n",
      "[18] pred=[-0.05705756  0.09118274]  target=[[-0.08002059  0.08013394]]\n",
      "[19] pred=[-0.1613285  -0.28929284]  target=[[-0.23182662 -0.31611535]]\n",
      "[20] pred=[-0.01520824  0.02837963]  target=[[ 0.04314064 -0.0091474 ]]\n",
      "[21] pred=[-0.2141825   0.12275335]  target=[[-0.21718974  0.10007179]]\n",
      "[22] pred=[-0.08365519  0.00918956]  target=[[-0.15660311 -0.07054469]]\n",
      "[23] pred=[0.13049576 0.07421848]  target=[[0.18256158 0.13644063]]\n",
      "[24] pred=[ 0.06520631 -0.21854849]  target=[[ 0.04266528 -0.21712722]]\n",
      "[25] pred=[-0.24072492  0.16579603]  target=[[-0.16967599 -0.05030137]]\n",
      "[26] pred=[0.1989187  0.01915284]  target=[[0.138599   0.15153645]]\n",
      "[27] pred=[-0.05446115  0.01732604]  target=[[-0.16798405  0.08486   ]]\n",
      "[28] pred=[-0.07809073  0.06756328]  target=[[-0.07661296  0.02519329]]\n",
      "[29] pred=[-0.20872581 -0.24484359]  target=[[-0.18983716 -0.21881983]]\n",
      "[30] pred=[-0.01274581  0.4448802 ]  target=[[-0.01784112  0.43952525]]\n",
      "[31] pred=[-0.03869346 -0.11919136]  target=[[-0.11629149 -0.19621392]]\n",
      "[32] pred=[ 0.12841111 -0.12276319]  target=[[ 0.15872175 -0.12587605]]\n",
      "[33] pred=[-0.04083911 -0.00852121]  target=[[-0.13029328 -0.05797008]]\n",
      "[34] pred=[-0.21469322 -0.07375193]  target=[[-0.15185934 -0.07274883]]\n",
      "[35] pred=[ 0.11551573 -0.16803955]  target=[[ 0.05060507 -0.16078793]]\n",
      "[36] pred=[0.20457065 0.03869135]  target=[[ 0.10228128 -0.02938183]]\n",
      "[37] pred=[ 0.01411509 -0.03288421]  target=[[-0.10582602 -0.03594827]]\n",
      "[38] pred=[ 0.01704147 -0.08661121]  target=[[-0.00948752 -0.15915945]]\n",
      "[39] pred=[ 0.26790056 -0.03021837]  target=[[ 0.23836572 -0.05372366]]\n",
      "[40] pred=[-0.13472122 -0.18548435]  target=[[-0.22652367 -0.26074058]]\n",
      "[41] pred=[ 0.02082415 -0.05045326]  target=[[ 0.00788777 -0.00656841]]\n",
      "[42] pred=[-0.3105871  -0.01652781]  target=[[-0.33254567 -0.02873346]]\n",
      "[43] pred=[-0.03474508 -0.31081668]  target=[[ 0.00974224 -0.32789254]]\n",
      "[44] pred=[0.28791225 0.10962246]  target=[[0.30097574 0.09136516]]\n",
      "[45] pred=[ 0.02344918 -0.03001018]  target=[[-0.09683662 -0.14698893]]\n",
      "[46] pred=[ 0.15007377 -0.04993998]  target=[[0.26075915 0.00605029]]\n",
      "[47] pred=[0.19221282 0.03001804]  target=[[0.24610285 0.0037387 ]]\n",
      "[48] pred=[-0.19118273  0.08544907]  target=[[-0.35851353  0.10047796]]\n",
      "[49] pred=[-0.0922486  -0.13153623]  target=[[-0.1402203  -0.10573576]]\n",
      "[50] pred=[-0.07101864  0.05423884]  target=[[-0.05494808  0.05431987]]\n",
      "[51] pred=[-0.04781548  0.06479831]  target=[[-0.1057008  -0.13006473]]\n",
      "[52] pred=[-0.20061463  0.05941809]  target=[[-0.21054526  0.02883739]]\n",
      "[53] pred=[ 0.01225869 -0.00740319]  target=[[-0.05882196  0.00926739]]\n",
      "[54] pred=[-0.05051184 -0.2033133 ]  target=[[-0.0537852 -0.1996091]]\n",
      "[55] pred=[0.10976296 0.13879757]  target=[[0.05554394 0.20329913]]\n",
      "[56] pred=[-0.20658147 -0.10308939]  target=[[-0.27471852 -0.07617891]]\n",
      "[57] pred=[0.15311146 0.07413392]  target=[[0.15920873 0.09965578]]\n",
      "[58] pred=[ 0.28421056 -0.07702559]  target=[[ 0.35644   -0.0182947]]\n",
      "[59] pred=[0.15867406 0.00617315]  target=[[0.11457159 0.07998385]]\n",
      "[60] pred=[-0.29891255  0.22970824]  target=[[-0.31290305  0.27256173]]\n",
      "[61] pred=[0.24745232 0.4360884 ]  target=[[0.23226716 0.42289513]]\n",
      "[62] pred=[-0.35255742  0.03866469]  target=[[-0.33978346  0.03254929]]\n",
      "[63] pred=[-0.27704078  0.21717356]  target=[[-0.23702303  0.22607505]]\n",
      "[64] pred=[-0.01196458  0.00684522]  target=[[-0.04943635 -0.03167411]]\n",
      "[65] pred=[-0.12742037  0.11124882]  target=[[-0.27964678  0.369026  ]]\n",
      "[66] pred=[0.18013096 0.01386857]  target=[[0.1279788  0.05650916]]\n",
      "[67] pred=[ 0.25257957 -0.05770947]  target=[[ 0.20876288 -0.14567009]]\n",
      "[68] pred=[0.15842694 0.07319525]  target=[[0.10427347 0.09768349]]\n",
      "[69] pred=[-0.08870839  0.10010938]  target=[[-0.09669659  0.08472237]]\n",
      "[70] pred=[0.08483521 0.18271719]  target=[[0.09137259 0.19545048]]\n",
      "[71] pred=[0.21074522 0.08070175]  target=[[0.25652644 0.07209346]]\n",
      "[72] pred=[-0.03173522 -0.00593973]  target=[[-0.06139472 -0.10327242]]\n",
      "[73] pred=[ 0.05026273 -0.24218006]  target=[[ 0.09401625 -0.22012693]]\n",
      "[74] pred=[0.21194297 0.00658967]  target=[[ 0.26697907 -0.05852067]]\n",
      "[75] pred=[-0.00664525  0.10677072]  target=[[-0.06155634  0.08791418]]\n",
      "[76] pred=[ 0.1799835  -0.09843862]  target=[[ 0.17732109 -0.04633634]]\n",
      "[77] pred=[ 0.28612375 -0.00867104]  target=[[ 0.3246447  -0.06130649]]\n",
      "[78] pred=[ 0.24610958 -0.01097564]  target=[[0.11889087 0.07745685]]\n",
      "[79] pred=[-0.05258631 -0.38898095]  target=[[-0.04993698 -0.3859418 ]]\n",
      "[80] pred=[ 0.02486137 -0.06503763]  target=[[-0.00852012 -0.00258362]]\n",
      "[81] pred=[ 0.14152583 -0.01530368]  target=[[ 0.15045789 -0.02046215]]\n",
      "[82] pred=[ 0.06094924 -0.02537929]  target=[[ 0.00948533 -0.04759878]]\n",
      "[83] pred=[-0.04561707  0.09985118]  target=[[-0.17678314  0.07976846]]\n",
      "[84] pred=[-0.3711666  0.2747773]  target=[[-0.36964634  0.26344153]]\n",
      "[85] pred=[-0.06993632  0.01034911]  target=[[-0.00943615 -0.02905401]]\n",
      "[86] pred=[-0.11283221 -0.10176751]  target=[[-0.13727775 -0.1496013 ]]\n",
      "[87] pred=[-0.05378117  0.02170545]  target=[[-0.05939069 -0.01895204]]\n",
      "[88] pred=[0.1532608  0.07563752]  target=[[0.03104303 0.1355039 ]]\n",
      "[89] pred=[ 0.02110703 -0.0193598 ]  target=[[-0.01546489 -0.01928374]]\n",
      "[90] pred=[-0.09372273 -0.03261073]  target=[[-0.10697766 -0.18521619]]\n",
      "[91] pred=[ 0.04024079 -0.02775095]  target=[[0.05985807 0.02128263]]\n",
      "[92] pred=[ 0.01019485 -0.06261744]  target=[[-0.22184056 -0.14354381]]\n",
      "[93] pred=[-0.10503779  0.09778824]  target=[[ 0.09025484 -0.16440605]]\n",
      "[94] pred=[0.14142022 0.07791521]  target=[[0.13933782 0.05220385]]\n",
      "[95] pred=[-0.0428169  0.0529361]  target=[[-0.07065611  0.07613002]]\n",
      "[96] pred=[0.17918023 0.17525618]  target=[[0.23305096 0.1633327 ]]\n",
      "[97] pred=[0.23615411 0.07941958]  target=[[0.22735275 0.15119112]]\n",
      "[98] pred=[-0.22766218 -0.16086696]  target=[[-0.30355534 -0.20852834]]\n",
      "[99] pred=[0.28704453 0.12409568]  target=[[0.32768074 0.16117927]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "   \n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load Dataset\n",
    "    h5_path = \"100samples.h5\"  # <- CHANGE THIS\n",
    "    ds = h5Dataset(h5_path)\n",
    "\n",
    "    loader = DataLoader(ds, batch_size=20)\n",
    "\n",
    "    # take feature dims from one sample\n",
    "    \n",
    "    sample0 = ds[0]\n",
    "    node_in_dim = sample0.x.shape[-1]\n",
    "    edge_in_dim = sample0.edge_attr.shape[-1]\n",
    "\n",
    "    # Build model\n",
    " \n",
    "    model = MPNNTransformerModel(\n",
    "        node_in_dim=node_in_dim,\n",
    "        edge_in_dim=edge_in_dim,\n",
    "        num_output_sources=1,  \n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer, loss, scheduler\n",
    "   \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.0)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=100,   # every 10 epochs\n",
    "        gamma=0.5      # multiply LR by 0.5\n",
    "    )\n",
    "\n",
    "    # 6) Train loop (overfit)\n",
    "    num_epochs = 300  # usually enough to see memorization\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            pred = model.forward_from_data(data)  # expected shape: [I, 2] with I=1\n",
    "            \n",
    "            target = data.y\n",
    "\n",
    "            loss = F.mse_loss(pred, target)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            \n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(loader)\n",
    "        \n",
    "        # print occasionally\n",
    "        if epoch % 25 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:4d} | loss = {avg_loss:.6f}\")\n",
    "\n",
    "        if avg_loss < 1e-6:\n",
    "            print(f\"Early stopping at epoch {epoch} with loss {avg_loss:.6f}\")\n",
    "            break\n",
    "\n",
    "    # Quick evaluation on the same 10 samples\n",
    "\n",
    "    model.eval()\n",
    "    print(\"\\nPredictions vs targets (10 samples):\")\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(ds):\n",
    "            data = data.to(device)\n",
    "            pred = model.forward_from_data(data)  # [1,2]\n",
    "            target = data.y\n",
    "            if target.dim() == 1:\n",
    "                target = target.unsqueeze(0)\n",
    "            if target.shape[-1] == 3:\n",
    "                target = target[:, :2]\n",
    "\n",
    "            print(f\"[{i:02d}] pred={pred.squeeze(0).cpu().numpy()}  target={target.squeeze(0).cpu().numpy()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b940f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be7fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acoupipe_customFeatures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
