{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "095ea246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and config\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from precomputed_dataset import precomputedDataset\n",
    "from model import MPNNTransformerModel\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "config = {\n",
    "    # Tier 1\n",
    "    \"lr\": 9.89e-4,\n",
    "    \"mpnn_hidden_dim\": 64,\n",
    "    \"mpnn_num_layers\": 4,\n",
    "    \"attn_num_heads\": 8,\n",
    "    \"attn_num_layers\": 6,\n",
    "    \"token_dim\": 64,\n",
    "    \"pooling_strategy\": \"mean_pooling\",\n",
    "    # Tier 2\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"mp_layer_norm\": False,\n",
    "    # Fixed\n",
    "    \"train_batch_size\": 128,\n",
    "    \"val_batch_size\": 256,\n",
    "    \"scheduler_min_lr\": 5e-6,\n",
    "    \"head_mlp_hidden_dim\": 256,\n",
    "    \"gradient_clip_max_norm\": 1.0,\n",
    "    \"num_output_sources\": 1,\n",
    "    \"node_in_dim\": 6,\n",
    "    \"edge_in_dim\": 6,\n",
    "    # Training\n",
    "    \"epochs\": 120,\n",
    "    \"seed\": 0,\n",
    "    \"device\": \"cuda:2\",\n",
    "    \"train_path\": \"/mnt/data/zaid/projects/simulated_data/step_2_3.h5\",\n",
    "    \"val_path\": \"/mnt/data/zaid/projects/simulated_data/validation.h5\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53a9ce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 554,563\n",
      "Train samples: 62500, Val samples: 62500\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.cuda.manual_seed_all(config[\"seed\"])\n",
    "\n",
    "device = config[\"device\"]\n",
    "\n",
    "train_ds = precomputedDataset(config[\"train_path\"])\n",
    "val_ds = precomputedDataset(config[\"val_path\"])\n",
    "\n",
    "train_loader = PyGDataLoader(train_ds, batch_size=config[\"train_batch_size\"], shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n",
    "val_loader = PyGDataLoader(val_ds, batch_size=config[\"val_batch_size\"], shuffle=False, num_workers=4, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "model = MPNNTransformerModel(\n",
    "    node_in_dim=config[\"node_in_dim\"],\n",
    "    edge_in_dim=config[\"edge_in_dim\"],\n",
    "    num_output_sources=config[\"num_output_sources\"],\n",
    "    mpnn_hidden_dim=config[\"mpnn_hidden_dim\"],\n",
    "    mpnn_num_layers=config[\"mpnn_num_layers\"],\n",
    "    token_dim=config[\"token_dim\"],\n",
    "    attn_num_heads=config[\"attn_num_heads\"],\n",
    "    attn_num_layers=config[\"attn_num_layers\"],\n",
    "    pooling_strategy=config[\"pooling_strategy\"],\n",
    "    head_mlp_hidden_dim=config[\"head_mlp_hidden_dim\"],\n",
    "    mp_layer_norm=config[\"mp_layer_norm\"],\n",
    "    mpnn_dropout=config[\"dropout\"],\n",
    "    attn_dropout=config[\"dropout\"],\n",
    "    head_dropout=config[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"epochs\"], eta_min=config[\"scheduler_min_lr\"])\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b112abf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 | Train: 0.044760 (loc=0.015317 str=0.029443) | Val: 0.032814 (loc=0.022147 str=0.010668) | Best: 0.032814 @ ep1 | LR: 9.89e-04\n",
      "Epoch    2 | Train: 0.013926 (loc=0.008797 str=0.005129) | Val: 0.017408 (loc=0.013892 str=0.003516) | Best: 0.017408 @ ep2 | LR: 9.89e-04\n",
      "Epoch    3 | Train: 0.010240 (loc=0.007006 str=0.003234) | Val: 0.022177 (loc=0.019353 str=0.002824) | Best: 0.017408 @ ep2 | LR: 9.89e-04\n",
      "Epoch    4 | Train: 0.009069 (loc=0.006110 str=0.002959) | Val: 0.013049 (loc=0.010234 str=0.002815) | Best: 0.013049 @ ep4 | LR: 9.89e-04\n",
      "Epoch    5 | Train: 0.008516 (loc=0.005714 str=0.002802) | Val: 0.013046 (loc=0.010748 str=0.002297) | Best: 0.013046 @ ep5 | LR: 9.89e-04\n",
      "Epoch    6 | Train: 0.007921 (loc=0.005229 str=0.002692) | Val: 0.014040 (loc=0.009779 str=0.004262) | Best: 0.013046 @ ep5 | LR: 9.89e-04\n",
      "Epoch    7 | Train: 0.007468 (loc=0.004946 str=0.002522) | Val: 0.010860 (loc=0.008538 str=0.002322) | Best: 0.010860 @ ep7 | LR: 9.89e-04\n",
      "Epoch    8 | Train: 0.007149 (loc=0.004731 str=0.002418) | Val: 0.010529 (loc=0.008130 str=0.002400) | Best: 0.010529 @ ep8 | LR: 9.89e-04\n",
      "Epoch    9 | Train: 0.006917 (loc=0.004533 str=0.002384) | Val: 0.010814 (loc=0.008176 str=0.002638) | Best: 0.010529 @ ep8 | LR: 9.89e-04\n",
      "Epoch   10 | Train: 0.006763 (loc=0.004418 str=0.002345) | Val: 0.012158 (loc=0.009038 str=0.003120) | Best: 0.010529 @ ep8 | LR: 9.89e-04\n",
      "Epoch   11 | Train: 0.006646 (loc=0.004353 str=0.002293) | Val: 0.010847 (loc=0.008576 str=0.002271) | Best: 0.010529 @ ep8 | LR: 9.89e-04\n",
      "Epoch   12 | Train: 0.006320 (loc=0.004117 str=0.002203) | Val: 0.008586 (loc=0.006648 str=0.001938) | Best: 0.008586 @ ep12 | LR: 9.89e-04\n",
      "Epoch   13 | Train: 0.006220 (loc=0.004035 str=0.002186) | Val: 0.009342 (loc=0.007319 str=0.002023) | Best: 0.008586 @ ep12 | LR: 9.89e-04\n",
      "Epoch   14 | Train: 0.006067 (loc=0.003912 str=0.002155) | Val: 0.010819 (loc=0.008382 str=0.002437) | Best: 0.008586 @ ep12 | LR: 9.89e-04\n",
      "Epoch   15 | Train: 0.005895 (loc=0.003806 str=0.002089) | Val: 0.009625 (loc=0.007608 str=0.002017) | Best: 0.008586 @ ep12 | LR: 9.89e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython._zmq.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/_zmq.py\", line 179, in zmq.backend.cython._zmq._check_rc\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: 'zmq.backend.cython._zmq.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/_zmq.py\", line 179, in zmq.backend.cython._zmq._check_rc\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: 'zmq.backend.cython._zmq.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/_zmq.py\", line 179, in zmq.backend.cython._zmq._check_rc\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     13\u001b[39m     data = data.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     pred_loc, pred_str = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_from_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     loss_loc = F.mse_loss(pred_loc, data.y)\n\u001b[32m     16\u001b[39m     loss_str = F.mse_loss(pred_str, data.strength)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/data/zaid/projects/geometry-aware-ssl/hyperparameter_tuning/../src/model.py:107\u001b[39m, in \u001b[36mMPNNTransformerModel.forward_from_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03mConvenience for PyG Data/Batch objects that expose .x, .edge_index, .edge_attr\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    106\u001b[39m batch = \u001b[38;5;28mgetattr\u001b[39m(data, \u001b[33m'\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/data/zaid/projects/geometry-aware-ssl/hyperparameter_tuning/../src/model.py:86\u001b[39m, in \u001b[36mMPNNTransformerModel.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_attr, batch)\u001b[39m\n\u001b[32m     83\u001b[39m     pooled = \u001b[38;5;28mself\u001b[39m.encoder(tokens)\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# batch nodes for transformer\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:    \n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     batched_tokens, src_key_padding_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_nodes_for_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_vector\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     pooled = \u001b[38;5;28mself\u001b[39m.encoder(batched_tokens, src_key_padding_mask=src_key_padding_mask) \n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# 3) pooled -> locations: [I, 2] (or [B, I, 2])\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/data/zaid/projects/geometry-aware-ssl/hyperparameter_tuning/../src/model.py:130\u001b[39m, in \u001b[36mMPNNTransformerModel.batch_nodes_for_transformer\u001b[39m\u001b[34m(node_embeddings, batch_vector)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Split the flat node tensor into a list of per-graph tensors\u001b[39;00m\n\u001b[32m    129\u001b[39m node_lists = []\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mbatch_vector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m + \u001b[32m1\u001b[39m):\n\u001b[32m    131\u001b[39m     mask = (batch_vector == i)\n\u001b[32m    132\u001b[39m     node_lists.append(node_embeddings[mask])  \u001b[38;5;66;03m# (num_nodes_graph_i, hidden_dim)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "history = {\"train_loss\": [], \"train_loc\": [], \"train_str\": [], \"val_loss\": [], \"val_loc\": [], \"val_str\": [], \"lr\": []}\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    epoch_loss = epoch_loc = epoch_str = 0.0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        pred_loc, pred_str = model.forward_from_data(data)\n",
    "        loss_loc = F.mse_loss(pred_loc, data.y)\n",
    "        loss_str = F.mse_loss(pred_str, data.strength)\n",
    "        loss = loss_loc + loss_str\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"gradient_clip_max_norm\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_loc += loss_loc.item()\n",
    "        epoch_str += loss_str.item()\n",
    "\n",
    "    n = len(train_loader)\n",
    "    epoch_loss /= n; epoch_loc /= n; epoch_str /= n\n",
    "\n",
    "    # --- Validate ---\n",
    "    model.eval()\n",
    "    val_loss_total = val_loc_total = val_str_total = 0.0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            pred_loc, pred_str = model.forward_from_data(data)\n",
    "            val_loc_total += F.mse_loss(pred_loc, data.y, reduction=\"sum\").item()\n",
    "            val_str_total += F.mse_loss(pred_str, data.strength, reduction=\"sum\").item()\n",
    "            num_samples += pred_loc.size(0)\n",
    "\n",
    "    val_loss = (val_loc_total + val_str_total) / num_samples\n",
    "    val_loc = val_loc_total / num_samples\n",
    "    val_str = val_str_total / num_samples\n",
    "\n",
    "    # --- Track ---\n",
    "    history[\"train_loss\"].append(epoch_loss)\n",
    "    history[\"train_loc\"].append(epoch_loc)\n",
    "    history[\"train_str\"].append(epoch_str)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_loc\"].append(val_loc)\n",
    "    history[\"val_str\"].append(val_str)\n",
    "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "\n",
    "    # --- Log ---\n",
    "    print(\n",
    "        f\"Epoch {epoch:4d} | \"\n",
    "        f\"Train: {epoch_loss:.6f} (loc={epoch_loc:.6f} str={epoch_str:.6f}) | \"\n",
    "        f\"Val: {val_loss:.6f} (loc={val_loc:.6f} str={val_str:.6f}) | \"\n",
    "        f\"Best: {best_val_loss:.6f} @ ep{best_epoch} | \"\n",
    "        f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "    )\n",
    "    \n",
    "    #scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938152ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m fig, axes = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m axes[\u001b[32m0\u001b[39m].plot(\u001b[43mhistory\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m], label=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m axes[\u001b[32m0\u001b[39m].plot(history[\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m], label=\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m axes[\u001b[32m0\u001b[39m].axvline(best_epoch - \u001b[32m1\u001b[39m, color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m, linestyle=\u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m, alpha=\u001b[32m0.5\u001b[39m, label=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbest @ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history[\"train_loss\"], label=\"train\")\n",
    "axes[0].plot(history[\"val_loss\"], label=\"val\")\n",
    "axes[0].axvline(best_epoch - 1, color=\"red\", linestyle=\"--\", alpha=0.5, label=f\"best @ {best_epoch}\")\n",
    "axes[0].set_title(\"Total Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history[\"train_loc\"], label=\"train\")\n",
    "axes[1].plot(history[\"val_loc\"], label=\"val\")\n",
    "axes[1].set_title(\"Location Loss\")\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(history[\"train_str\"], label=\"train\")\n",
    "axes[2].plot(history[\"val_str\"], label=\"val\")\n",
    "axes[2].set_title(\"Strength Loss\")\n",
    "axes[2].legend()\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"MSE\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest val_loss: {best_val_loss:.6f} at epoch {best_epoch}\")\n",
    "print(f\"Epochs after best: {len(history['val_loss']) - best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e839547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
