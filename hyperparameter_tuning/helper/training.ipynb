{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095ea246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and config\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from precomputed_dataset import precomputedDataset\n",
    "from model import MPNNTransformerModel\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "config = {\n",
    "    # Tier 1\n",
    "    \"lr\": 1e-4,\n",
    "    \"mpnn_hidden_dim\": 128,\n",
    "    \"mpnn_num_layers\": 3,\n",
    "    \"attn_num_heads\": 4,\n",
    "    \"attn_num_layers\": 4,\n",
    "    \"token_dim\": 128,\n",
    "    \"pooling_strategy\": \"cls_token\",\n",
    "    # Tier 2\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"mp_layer_norm\": False,\n",
    "    # Fixed\n",
    "    \"train_batch_size\": 128,\n",
    "    \"val_batch_size\": 256,\n",
    "    \"scheduler_min_lr\": 5e-6,\n",
    "    \"head_mlp_hidden_dim\": 256,\n",
    "    \"gradient_clip_max_norm\": 1.0,\n",
    "    \"num_output_sources\": 1,\n",
    "    \"node_in_dim\": 6,\n",
    "    \"edge_in_dim\": 6,\n",
    "    # Training\n",
    "    \"epochs\": 200,\n",
    "    \"seed\": 0,\n",
    "    \"device\": \"cuda:2\",\n",
    "    \"train_path\": \"/mnt/data/zaid/projects/simulated_data/step1.h5\",\n",
    "    \"val_path\": \"/mnt/data/zaid/projects/simulated_data/validation.h5\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53a9ce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 1,275,011\n",
      "Train samples: 10000, Val samples: 62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/zaid/projects/geometry-aware-ssl/hyperparameter_tuning/../src/modules.py:130: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  self.transformer_encoder = nn.TransformerEncoder(\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.cuda.manual_seed_all(config[\"seed\"])\n",
    "\n",
    "device = config[\"device\"]\n",
    "\n",
    "train_ds = precomputedDataset(config[\"train_path\"])\n",
    "val_ds = precomputedDataset(config[\"val_path\"])\n",
    "\n",
    "train_loader = PyGDataLoader(train_ds, batch_size=config[\"train_batch_size\"], shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n",
    "val_loader = PyGDataLoader(val_ds, batch_size=config[\"val_batch_size\"], shuffle=False, num_workers=4, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "model = MPNNTransformerModel(\n",
    "    node_in_dim=config[\"node_in_dim\"],\n",
    "    edge_in_dim=config[\"edge_in_dim\"],\n",
    "    num_output_sources=config[\"num_output_sources\"],\n",
    "    mpnn_hidden_dim=config[\"mpnn_hidden_dim\"],\n",
    "    mpnn_num_layers=config[\"mpnn_num_layers\"],\n",
    "    token_dim=config[\"token_dim\"],\n",
    "    attn_num_heads=config[\"attn_num_heads\"],\n",
    "    attn_num_layers=config[\"attn_num_layers\"],\n",
    "    pooling_strategy=config[\"pooling_strategy\"],\n",
    "    head_mlp_hidden_dim=config[\"head_mlp_hidden_dim\"],\n",
    "    mp_layer_norm=config[\"mp_layer_norm\"],\n",
    "    mpnn_dropout=config[\"dropout\"],\n",
    "    attn_dropout=config[\"dropout\"],\n",
    "    head_dropout=config[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"epochs\"], eta_min=config[\"scheduler_min_lr\"])\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b112abf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 | Train: 0.059508 (loc=0.024997 str=0.034512) | Val: 0.080358 (loc=0.042460 str=0.037898) | Best: 0.080358 @ ep1 | LR: 5.00e-04\n",
      "Epoch   10 | Train: 0.015311 (loc=0.008996 str=0.006315) | Val: 0.025778 (loc=0.020644 str=0.005134) | Best: 0.025648 @ ep8 | LR: 4.98e-04\n",
      "Epoch   20 | Train: 0.010233 (loc=0.006731 str=0.003502) | Val: 0.015889 (loc=0.012903 str=0.002987) | Best: 0.015889 @ ep20 | LR: 4.89e-04\n",
      "Epoch   30 | Train: 0.008884 (loc=0.005976 str=0.002908) | Val: 0.014447 (loc=0.010811 str=0.003635) | Best: 0.013257 @ ep28 | LR: 4.75e-04\n",
      "Epoch   40 | Train: 0.008151 (loc=0.005367 str=0.002783) | Val: 0.015225 (loc=0.012484 str=0.002741) | Best: 0.011662 @ ep39 | LR: 4.55e-04\n",
      "Epoch   50 | Train: 0.007111 (loc=0.004616 str=0.002495) | Val: 0.016418 (loc=0.013779 str=0.002639) | Best: 0.010761 @ ep49 | LR: 4.30e-04\n",
      "Epoch   60 | Train: 0.006265 (loc=0.003792 str=0.002472) | Val: 0.010511 (loc=0.008412 str=0.002099) | Best: 0.009980 @ ep55 | LR: 4.01e-04\n",
      "Epoch   70 | Train: 0.005864 (loc=0.003511 str=0.002353) | Val: 0.010794 (loc=0.007997 str=0.002797) | Best: 0.009754 @ ep68 | LR: 3.68e-04\n",
      "Epoch   80 | Train: 0.005017 (loc=0.003110 str=0.001908) | Val: 0.011770 (loc=0.009140 str=0.002630) | Best: 0.009528 @ ep72 | LR: 3.33e-04\n",
      "Epoch   90 | Train: 0.004437 (loc=0.002579 str=0.001858) | Val: 0.009775 (loc=0.007784 str=0.001991) | Best: 0.009528 @ ep72 | LR: 2.95e-04\n",
      "Epoch  100 | Train: 0.004119 (loc=0.002512 str=0.001607) | Val: 0.012536 (loc=0.010548 str=0.001988) | Best: 0.009360 @ ep95 | LR: 2.56e-04\n",
      "Epoch  110 | Train: 0.003828 (loc=0.002237 str=0.001591) | Val: 0.009544 (loc=0.007550 str=0.001994) | Best: 0.008834 @ ep102 | LR: 2.18e-04\n",
      "Epoch  120 | Train: 0.003457 (loc=0.001872 str=0.001585) | Val: 0.009315 (loc=0.007426 str=0.001889) | Best: 0.008834 @ ep102 | LR: 1.80e-04\n",
      "Epoch  130 | Train: 0.002764 (loc=0.001510 str=0.001254) | Val: 0.008738 (loc=0.006771 str=0.001967) | Best: 0.008738 @ ep130 | LR: 1.44e-04\n",
      "Epoch  140 | Train: 0.002398 (loc=0.001259 str=0.001139) | Val: 0.008664 (loc=0.006889 str=0.001775) | Best: 0.008619 @ ep138 | LR: 1.10e-04\n",
      "Epoch  150 | Train: 0.002233 (loc=0.001150 str=0.001083) | Val: 0.009444 (loc=0.007462 str=0.001982) | Best: 0.008619 @ ep138 | LR: 8.03e-05\n",
      "Epoch  160 | Train: 0.001939 (loc=0.000984 str=0.000955) | Val: 0.008976 (loc=0.007175 str=0.001801) | Best: 0.008603 @ ep157 | LR: 5.46e-05\n",
      "Epoch  170 | Train: 0.001797 (loc=0.000890 str=0.000907) | Val: 0.008717 (loc=0.006937 str=0.001780) | Best: 0.008583 @ ep168 | LR: 3.38e-05\n",
      "Epoch  180 | Train: 0.001672 (loc=0.000849 str=0.000823) | Val: 0.008624 (loc=0.006859 str=0.001765) | Best: 0.008583 @ ep168 | LR: 1.83e-05\n",
      "Epoch  190 | Train: 0.001595 (loc=0.000810 str=0.000785) | Val: 0.008641 (loc=0.006871 str=0.001770) | Best: 0.008570 @ ep184 | LR: 8.69e-06\n",
      "Epoch  200 | Train: 0.001586 (loc=0.000790 str=0.000797) | Val: 0.008628 (loc=0.006883 str=0.001745) | Best: 0.008570 @ ep184 | LR: 5.03e-06\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "history = {\"train_loss\": [], \"train_loc\": [], \"train_str\": [], \"val_loss\": [], \"val_loc\": [], \"val_str\": [], \"lr\": []}\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    epoch_loss = epoch_loc = epoch_str = 0.0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        pred_loc, pred_str = model.forward_from_data(data)\n",
    "        loss_loc = F.mse_loss(pred_loc, data.y)\n",
    "        loss_str = F.mse_loss(pred_str, data.strength)\n",
    "        loss = loss_loc + loss_str\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"gradient_clip_max_norm\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_loc += loss_loc.item()\n",
    "        epoch_str += loss_str.item()\n",
    "\n",
    "    n = len(train_loader)\n",
    "    epoch_loss /= n; epoch_loc /= n; epoch_str /= n\n",
    "\n",
    "    # --- Validate ---\n",
    "    model.eval()\n",
    "    val_loss_total = val_loc_total = val_str_total = 0.0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            pred_loc, pred_str = model.forward_from_data(data)\n",
    "            val_loc_total += F.mse_loss(pred_loc, data.y, reduction=\"sum\").item()\n",
    "            val_str_total += F.mse_loss(pred_str, data.strength, reduction=\"sum\").item()\n",
    "            num_samples += pred_loc.size(0)\n",
    "\n",
    "    val_loss = (val_loc_total + val_str_total) / num_samples\n",
    "    val_loc = val_loc_total / num_samples\n",
    "    val_str = val_str_total / num_samples\n",
    "\n",
    "    # --- Track ---\n",
    "    history[\"train_loss\"].append(epoch_loss)\n",
    "    history[\"train_loc\"].append(epoch_loc)\n",
    "    history[\"train_str\"].append(epoch_str)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_loc\"].append(val_loc)\n",
    "    history[\"val_str\"].append(val_str)\n",
    "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch:4d} | \"\n",
    "            f\"Train: {epoch_loss:.6f} (loc={epoch_loc:.6f} str={epoch_str:.6f}) | \"\n",
    "            f\"Val: {val_loss:.6f} (loc={val_loc:.6f} str={val_str:.6f}) | \"\n",
    "            f\"Best: {best_val_loss:.6f} @ ep{best_epoch} | \"\n",
    "            f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        )\n",
    "    \n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "938152ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m fig, axes = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m axes[\u001b[32m0\u001b[39m].plot(\u001b[43mhistory\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m], label=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m axes[\u001b[32m0\u001b[39m].plot(history[\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m], label=\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m axes[\u001b[32m0\u001b[39m].axvline(best_epoch - \u001b[32m1\u001b[39m, color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m, linestyle=\u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m, alpha=\u001b[32m0.5\u001b[39m, label=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbest @ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history[\"train_loss\"], label=\"train\")\n",
    "axes[0].plot(history[\"val_loss\"], label=\"val\")\n",
    "axes[0].axvline(best_epoch - 1, color=\"red\", linestyle=\"--\", alpha=0.5, label=f\"best @ {best_epoch}\")\n",
    "axes[0].set_title(\"Total Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history[\"train_loc\"], label=\"train\")\n",
    "axes[1].plot(history[\"val_loc\"], label=\"val\")\n",
    "axes[1].set_title(\"Location Loss\")\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(history[\"train_str\"], label=\"train\")\n",
    "axes[2].plot(history[\"val_str\"], label=\"val\")\n",
    "axes[2].set_title(\"Strength Loss\")\n",
    "axes[2].legend()\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"MSE\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest val_loss: {best_val_loss:.6f} at epoch {best_epoch}\")\n",
    "print(f\"Epochs after best: {len(history['val_loss']) - best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e839547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
